% document class
\documentclass[12pt]{article}

% use package
\usepackage{scribe}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx,times}
\usepackage{mathtools}
\usepackage{url}
%\usepackage{setspacing}
\usepackage{fullpage}
\usepackage{palatino}
\usepackage{mathpazo}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xifthen} % Allows us to put in optional arguments to questions
\usepackage{color}
\usepackage{manfnt}
\usepackage{ifthen}
\usepackage{listings}
\usepackage{nicefrac,mathtools}
\usepackage{float}
\usepackage{showexpl}
\usepackage{xcolor}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{chngcntr}
\usepackage{xparse}
\usepackage{etoolbox}
\usepackage{blkarray}
\usepackage{cprotect}
\usepackage{authblk}

\newenvironment{commentline}[1]{
\noindent{\color{red} \rule{\linewidth}{0.5mm}}
\textbf{Comment}: #1

\noindent{\color{red} \rule{\linewidth}{0.5mm}}
}

\title{Post-Shock Transition Estimation}
\author{Jilei Lin\thanks{jileil2@ilinois.edu} }
\author{Daniel J. Eck\thanks{dje13@illinois.edu}}
\affil{Department of Statistics, University of Illinois at Urbana-Champaign}


% start document
\begin{document}


\maketitle

\section{Introduction}

Suppose that an individual, given time series $\{(\mathbf{x}_{1, t},y_{1,t})\colon t=1, \ldots, T_1\}$, is interested in predicting $y_{1, T_1+k}$ for some $k \in \naturals$. However,  he knows that the time series is about experience a shock at $T_1+k$, whose signal is not embedded in the data. In this case,   statistical methodologies that rely on capturing signal from training data to predict are not expected to perform well. 

\commentline{Do not use `he', use something like `the researcher'.}

Based on the synthetic control theory \cite{abadie2010synthetic}, \citet{lin2020minimizing} proposes a post-shock prediction method that borrows information from other similar  time series experiencing analogous shocks by constructing a donor pool $\{(\mathbf{x}_{i, t},y_{i,t})\colon i = 2, \ldots, n +1, t = 1, \ldots, T_i\}$, and uses synthetic control method to weight the shock effects $\hat{\alpha}_i$ estimated from the donor pool for $i = 2, \ldots, n+1$ to provide a scalar adjustment to the naive time series forecast $\hat{y}_{1, T_1+k}$. \citet{lin2020minimizing} provides conditions when  optimality of minimizing mean squared forecast error is attained.

\commentline{Our method is not synthetic control, it's a weighted-adjustment estimator that is inspired by synthetic control.}

However, the work of \citet{lin2020minimizing} is limited to the nowcasting problem. Moreover, it provides no information about how the shock will affect the time series afterwards, let alone whether it will die out.  In practice, it is natural to think of applications where we are interested in the dynamics of the shock, whether and how the shock will die out. For example, when a company knows it is going to experience a disastrous shock, managers will be interested in  whether the shock will be persistent, how it affects future sales for a number of months, and thus how to adjust cost and production to combat such crisis. 

In this paper, we are interested in extending the work of \citet{lin2020minimizing} to estimate the shock transition after the time series of interest experience a shock, i.e., $\{\hat{y}_{1, T_1+k+j}\colon j = 1, \ldots, d\}$, extend its optimality propositions of nowcasting to prediction for a period, and develop a hypothesis testing procedure to judge whether the shock is permanent or transitory.

\section{Methods}

Suppose at $t=T_i^*$, one knows the shock is about to occur at $t=T_i^*+1$. Consider the following model
\begin{align*}
  \mc{M} \colon 
  \begin{array}{l}
    y_{i,t} = \eta_i +  \alpha_{i,t}I(t > T_i^*)+ \phi_i y_{i,t-1} + \mathbf{x}_{i,t}\bs{\theta}_i + \varepsilon_{i,t}\\[.3cm]
    \alpha_{i,t} = \mu_{\alpha} + \mbf{x}_{i, t}\boldsymbol{\delta}_i + \tilde{\varepsilon}_i
  \end{array},
\end{align*}
where $D_{i,t}= I(t= T_i^* +t)$, $I(\cdot)$ is an indicator function, and $\mbf{x}_{i,t}, \boldsymbol{\theta}_{i}\in \reals^p$ with $p\geq 1$. The random effects structure is organized as below.
\begin{align*}
  \tilde{\varepsilon}_i &\simiid \mc{F}_{\tilde{\varepsilon}} \text{ with } \E_{\mc{F}_{\tilde{\varepsilon}}} = 0, \Var_{\mc{F}_{\tilde{\varepsilon}}}(\tilde{\varepsilon}_i)= \sigma^2_{\alpha}  >0, \\
  \eta_i & \indep \alpha_{i,t} \indep \phi_i \indep \boldsymbol{\theta}_i \indep \varepsilon_{i,t} \indep \tilde{\varepsilon}_i.
\end{align*}
The candidate forecasts are
\begin{align*}
  &\text{Forecast 1}: \hat{y}_{1,T_1^*+k + j}^{(1)} = \hat\eta_1 
    + \hat\phi_1 \hat{y}^{(1)}_{1,T_1^*+k+j-1} + \mbf{x}_{1,T_1^*+k+j-1} \hat{\bs{\theta}}_1
    , \\
  &\text{Forecast 2}: \hat{y}_{1,T_1^*+k+j}^{(2)} = \hat\eta_1 
    + \hat\phi_1 \hat{y}^{(2)}_{1,T_1^*+k+j-1} + \mbf{x}_{1,T_1^*+k+j-1} \hat{\bs{\theta}}_1
    + \hat{\alpha}^{(j)},
\end{align*}
where $\hat{y}_{1, T_1^*}^{(1)}=\hat{y}_{1, T_1^*}^{(2)}=y_{1,T_1^*}$, $\hat{\alpha}^{(j)}$ is an estimate of the shock effect occurring in the model of $y_{1,T_1^*+k+j}$, and other parameters are estimated in an unbiased fashion, e.g., OLS. 

\commentline{There is also the h-ahead forecast model, see: \url{https://www.ssc.wisc.edu/~bhansen/papers/hstep.pdf} Perhaps their framework is more appropriate. See a more authoritative presentation in \cite{clements1996multi}.}

Define
\begin{align}
  \begin{array}{lll}
     \Theta &= &\{(\eta_i, \phi_i, \bs{\theta}_i, \alpha_{i,t}, \mbf{x}_{i,t}, y_{i,t-1}, \bs{\delta}_i)\colon t= 1, \ldots, T_i, i = 2, \ldots, n +1\},\\
    \Theta_1 &= &\{(\eta_i, \phi_i, \bs{\theta}_i, \alpha_{i,t}, \mbf{x}_{i,t}, y_{i,t-1}, \bs{\delta}_i)\colon t= 1, \ldots, T_i, i = 1\},\label{parameter}
  \end{array}
\end{align}


For simplicity, assume $k=0$. Given the post-shock estimated transition, $\{\hat{y}_{1, t}^2\colon t= T_1 +1, \ldots, T_1+d\}$, to gauge the forecast performance, we consider the forecast risk in the form of MSE as below
\begin{align*}
  R_k 
  &= \frac{1}{d} \sum_{j=1}^d \E [(\hat{y}_{1,T_1^*+j}^{(k)}-y_{1, T_1+j})^2] ,
  \quad k = 1, 2.
\end{align*}

\begin{proposition}
  Suppose that $\hat{\alpha}^{(j)}$ estimates $\alpha_{1, T_1^*+j}$ unbiasedly and that  $\hat{\alpha}^{(j)}$ is independent of $\Theta_1$. Then, $R_1 > R_2$ when
\end{proposition}
\begin{proof}
  Define
\begin{align*}
  C(\Theta_1, j) = \hat{\eta}_1 + \hat{\phi}_1 \hat{y}_{1, T_1^*+j-1}^{(2)} + \mbf{x}_{1, T_1^*+j}\hat{\bs{\theta}}_1 - (\eta_1 + \phi_1 y_{1, T_1^* + j-1} + \mbf{x}_{1, T_1^*+j} \bs{\theta}_1 +\varepsilon_{1, T_1^*+j}) .
\end{align*}
We first consider a simple case when $d=2$. By Lemma 1 in \cite{lin2020minimizing},
\begin{align*}
  & \E [(\hat{y}_{1,T_1+1}^{(1)}-y_{1, T_1+1})^2] -  \E [(\hat{y}_{1,T_1+1}^{(2)}-y_{1, T_1+1})^2]\\
= \quad   & \E(\alpha_{1, T_1^*+1}^2) - \E[(\hat{\alpha}^{(1)}-\alpha_{1, T_1^*+1})^2] \\
= \quad & - \E[ (\hat{\alpha}^{(1)})^2]+2 \E(\hat{\alpha}^{(1)} \alpha_{1, T_1^*+1}) \\
= \quad &  - \E[ (\hat{\alpha}^{(1)})^2]+2 \E[\alpha_{1, T_1^*+1} \cdot \E(\hat{\alpha}^{(1)} \mid  \alpha_{1, T_1^*+1})] \\
= \quad & 2 \E[ \alpha_{1, T_1^*+2}^2]- \E[(\hat{\alpha}^{(1)})^2] \\
= \quad & \E[ \alpha_{1, T_1^*+2}^2]- \Var(\hat{\alpha}^{(1)}).
\end{align*}
Next,
\begin{align*}
 & \E [(\hat{y}_{1,T_1+2}^{(1)}-y_{1, T_1+2})^2] -  \E [(\hat{y}_{1,T_1+2}^{(2)}-y_{1, T_1+2})^2]  \\
= \quad  & \E \Big\{ (C(\Theta_1, 2) - \hat{\phi}_1\hat{\alpha}^{(2)}-\alpha_{1, T_1^*+2})^2\Big\}- \E \Big\{ (C(\Theta_1, 2) -\alpha_{1, T_1^*+2})^2\Big\} \\
= \quad & -\E (\alpha_{1, T_1^*+2}^2)  + 2 \E (C(\Theta_1, 2)\alpha_{1, T_1^*+2}) - 2 \E [C(\Theta_1, 2) (\hat{\phi}_1\hat{\alpha}^{(2)}+\alpha_{1, T_1^*+2})] + \E[(\hat{\phi}_1\hat{\alpha}^{(2)}+\alpha_{1, T_1^*+2})^2] \\
= \quad & \E[(\hat{\phi}_1\hat{\alpha}^{(2)})^2] + 2 \E [\hat{\phi}_1 \hat{\alpha}^{(2)} \alpha_{1, T_1^*+2}] -2 \E[C(\Theta_1, 2) \hat{\phi}_{1}\hat{\alpha}^{(2)}]
\end{align*}
By assumption, $\hat{\alpha}^{(2)}$ is independent  of $\Theta_1$.  Moreover, $\alpha_{1, T_1^*+2}$ is also independent  of $\Theta_1$ by construction. Since OLS estimators are unbiased and $\hat{\alpha}^{(2)}$ estimates $\alpha_{1, T_1^*+2}$ unbiasedly, 
\begin{align*}
  \E [\hat{\phi}_1 \hat{\alpha}^{(2)} \alpha_{1, T_1^*+2}] &= \phi_1 [\E (\hat{\alpha}^{(2)})]^2\\
  \E [C(\Theta_1, 2)\hat{\phi}_1 \hat{\alpha}^{(2)}] &=  \E[C(\Theta_1, 2) \hat{\phi}_1 ] \cdot \E(\hat{\alpha}^{(2)})
\end{align*}
Thus,
\begin{align*}
 & \E [(\hat{y}_{1,T_1+2}^{(1)}-y_{1, T_1+2})^2] -  \E [(\hat{y}_{1,T_1+2}^{(2)}-y_{1, T_1+2})^2] \\
= \quad & \E[(\hat{\phi}_1\hat{\alpha}^{(2)})^2] + 2 \phi_1 \cdot [\E (\hat{\alpha}^{(2)})]^2 -2 \E[C(\Theta_1, 2) \hat{\phi}_1 ] \cdot \E(\hat{\alpha}^{(2)}) \\
= \quad & \E[(\hat{\phi}_1\hat{\alpha}^{(2)})^2]  + 2\cdot \E(\hat{\alpha}^{(2)}) \cdot  \Big\{\phi_1 \E (\hat{\alpha}^{(2)}) - \E[C(\Theta_1, 2) \hat{\phi}_1] \Big\}
\end{align*}
\textcolor{red}{Challenges: How to evaluate $\E[C(\Theta_1, 2) \hat{\phi}_1]$.}
\end{proof}




\subsection{Challenges}

\begin{challenges}
  How to define a proper null hypothesis that captures whether the shock transition is  transitory or permanent?   Define $ \hat{\boldsymbol{\alpha}} = (\hat{\alpha}^{(j)})_{j=1}^d$ and $\bs{\alpha}_1= (\alpha_{T_1^*+j})_{j=1}^d$. One possible hypothesis is
  \begin{align*}
    H_0 \colon \bs{\alpha}_1=\bs{0} 
    \quad \text{ versus } \quad 
    H_1 \colon \bs{\alpha}_1 \neq \bs{0} 
  \end{align*}
  In any cases, it is important to find an unbiased estimator of $\bs{\alpha}_1$. $\hat{\alpha}_{\rm wadj}$ in \citep{lin2020minimizing} might be possible. 
\end{challenges}

\commentline{We could consider an $h$-ahead forecasting procedure at each value over some window $h = 1,\ldots,H$. We can then consider the adjusted and unadjusted responses at every $h$, and then develop a test of the differences between these two quantities for each $h$. Alternatively, we could consider if the shock effect washes out in each series in the donor pool for each $h$. Both of these procedures would require a multiple testing framework. All of this may depend on the forecast horizon $H$. }

\begin{challenges}
  How to generalize to $\hat{\alpha}_{\rm wadj}$? In \citep{lin2020minimizing}, the weighting is with respect to  $\mbf{x}_{i, T_i^*+1}$ for $i = 1, \ldots, n+1$. However, in this case, we have $\mbf{x}_{i, T_i^*+j}$ for $i = 1, \ldots, n+1$ and $j = 1, \ldots, d$. Perhaps, we may want to do $d$ times of weighting such that the unbiased  property still holds. However, a lot of forecast error for $\mbf{x}_{T_i^*+j}$ when $d$ is large may be of concern in constructing such estimator.
\end{challenges}

\commentline{The $h$ ahead framework does not involve $x$ values beyond the range of data collection. We could do something with forecasting, but that seems shaky.}


\bibliographystyle{plainnat}
\bibliography{synthetic-prediction-notes}

\end{document}

