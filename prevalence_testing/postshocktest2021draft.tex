\documentclass[11pt]{article}

% use packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{url}

\usepackage{graphicx}
%\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{bm}
\usepackage[usenames]{color}
\usepackage{hyperref}

\usepackage{caption}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{tikz}
\usepackage{multirow}
\usepackage[linesnumbered, ruled,vlined]{algorithm2e}
\usepackage{pdflscape}

% margin setup
\usepackage{geometry}
\geometry{margin=0.8in}


% function definition
\newcommand{\R}{\mathbb{R}}
\newcommand{\w}{\textbf{w}}
\newcommand{\x}{\textbf{x}}
\newcommand{\dbf}{\textbf{d}}
\newcommand{\y}{\textbf{y}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\L}{\textbf{L}}
\newcommand{\Hist}{\mathcal{H}}
\newcommand{\Prob}{\mathbb{P}}
\def\mbf#1{\mathbf{#1}} % bold but not italic
\def\ind#1{\mathrm{1}(#1)} % indicator function
\newcommand{\simiid}{\stackrel{iid}{\sim}} %[] IID 
\def\where{\text{ where }} % where
\newcommand{\indep}{\perp \!\!\! \perp } % independent symbols
\def\cov#1#2{\mathrm{Cov}(#1, #2)} % covariance 
\def\mrm#1{\mathrm{#1}} % remove math
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\def\t#1{\tilde{#1}} % tilde
\def\normal#1#2{\mathcal{N}(#1,#2)} % normal
\def\mbi#1{\boldsymbol{#1}} % Bold and italic (math bold italic)
\def\v#1{\mbi{#1}} % Vector notation
\def\mc#1{\mathcal{#1}} % mathical
\DeclareMathOperator*{\argmax}{arg\,max} % arg max
\DeclareMathOperator*{\argmin}{arg\,min} % arg min
\def\E{\mathbb{E}} % Expectation symbol
\def\mc#1{\mathcal{#1}}
\def\var#1{\mathrm{Var}(#1)} % Variance symbol
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} % checkmark
\newcommand\red[1]{{\color{red}#1}}
\def\bs#1{\boldsymbol{#1}}
\def\P{\mathbb{P}}

\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % A norm with 1 argument
\DeclareMathOperator{\Var}{Var} % Variance symbol

\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\hypersetup{
  linkcolor  = blue,
  citecolor  = blue,
  urlcolor   = blue,
  colorlinks = true,
} % color setup

% proof to proposition 
\newenvironment{proof-of-proposition}[1][{}]{\noindent{\bf
    Proof of Proposition {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}
% general proof of corollary
  \newenvironment{proof-of-corollary}[1][{}]{\noindent{\bf
    Proof of Corollary {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}
% general proof of lemma
  \newenvironment{proof-of-lemma}[1][{}]{\noindent{\bf
    Proof of Lemma {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}

\allowdisplaybreaks

\title{Prospective testing for the prevalence or transience of a shock effect before it occurs}
\author{}
\date{}

\begin{document}

\maketitle

%We develop a forecasting methodology for providing credible forecasts for time series that have recently undergone a shock. We achieve this by borrowing knowledge from other time series that have undergone similar shocks for which post-shock outcomes are observed. Three shock effect estimators are motivated with the aim of minimizing average forecast risk. We propose risk-reduction propositions that provide conditions that establish when our methodology works. Bootstrap and leave-one-out cross validation procedures are provided to prospectively assess the performance of our methodology. Several simulated data examples, and two real data examples of forecasting Conoco Phillips and Apple stock price are provided for verification and illustration

\begin{abstract}
We develop a hypothesis testing procedure to prospectively test whether an anticipated shock is likely to be transient or permanent over a time horizon. We achieve this by borrowing knowledge from other time series that have undergone similar shocks for which post-shock outcomes are observed. These additional time series form a donor pool. 
For each of the time series in the donor pool we calculate a p-value corresponding to a hypothesis test on the relevance of the inclusion of shock-effect information in predicting the response over the time horizon.
These p-values are then combined to form an aggregated p-value which guides one decision in determining whether the shock effect for the time series under study is expected to be prevalent or transient. This p-value can be computed before the shock-effect is observed in the time series under study provided one can form a suitable donor pool. Several simulated data examples, and two real data examples of forecasting Conoco Phillips stock price and .... are provided for verification and illustration.
\end{abstract}


\section{Introduction}

%We provide forecasting adjustment techniques with the goal of lowering overall forecast error when the time series under study has undergone a structural shock.
We provide forecasting methodology for assessing the lingering effect of an anticipated structural shock to a time series under study. We focus on the setting in which a structural shock has occurred and one desires a prediction for the post-shock response over a set time horizon $H$. Specific interest is in determining whether the shock is expected to be permanent or transient over $H$. Standard forecasting methods may not yield any guidance on the post-shock trajectories \citep{baumeister2014real}. 
This is a general problem that has many real life applications. For example, one may acquire terrible or great news about a company and desire to determine whether that news is bound to impact the stock price of that company over a relevant time period.  Companies may be interested in forecasting the demand of their products after they were involved in a brand crisis, but they only have recent sales data from pre-crises times. %Policy makers, economists, and citizens alike may be interested in determining whether inflation will dissipate or persist for the foreseeable future
All is not lost in this forecasting setting, one may be able to supplement the present forecast with past data borrowed from other time series which contain post-shock trajectories arising from materially similar structural shocks.

The core idea of our methodology is to sensibly aggregate similar past realized shock effects which arose from other time series, and then incorporate the aggregated shock effect estimator into the present forecast.

Our testing method embraces ideas from 
forecast aggregation in the post-shock setting \citep{lin2021minimizing}, 
forecast comparison \citep{diebold1995comparing, quaedvlieg2021multi}, 
p-value combination, 
conditional forecasting \citep{baumeister2014real, kilian2017structural}, 
time series pooling using cross-sectional panel data \citep{ramaswamy1993empirical, pesaran1999pooled, hoogstrate2000pooling, baltagi2008forecasting, koop2012forecasting, liu2020forecasting},
forecasting with judgement and models \citep{svensson2005monetary, monti2008forecast}, 
synthetic control methodology \citep{abadie2010synthetic, agarwal2020two},  
expectation shocks \citep{croushore2006data, baumeister2014general, clements2019measuring}.




\section{Setting}

We will suppose that a researcher has multivariate time series data $\y_{i,t}$, $t = 1, \ldots,  T_i$ and $i = 1, \ldots, n+1$. We let $\y_{i,t} = (y_{i,t}$, $\x_{i,t}$) where $y_{i,t}$ is a scalar response and $\x_{i,t}$ is a vector of covariates that are revealed to the analyst prior to the observation of $y_{1,t}$.  Suppose that the analyst is interested in forecasting $y_{1,t}$, the first time series in the collection. 
We will suppose that each time series $\y_{i,t}$ undergoes a shock at time $T^*_i \leq T_i + 1$. To define an interesting setting, we will suppose that $T^*_1 = T_1 + 1$, and $1 < T^*_i < T_i + 1$ for $i \geq 2$. 
We will suppose that $\x_{i,t=T^*_i}$ is observed before the shock takes effect on $y_{i,t=T^*_i}$.

We are interested in point forecasts $y_{i,t}^h$ at multiple horizons, $h = 1, \ldots, H$ with the aim of determining whether the shock has an effect on $y_{i,t}^h$.
\cite{quaedvlieg2021multi} provided a methodology for comparing forecasts jointly across all horizons of a forecast path, $h = 1,\ldots, H$. In our post-shock setting, we want to compare the forecasts 
$$
  \hat y^{1,h}_{i,t} \; \text{and} \; \hat y^{2,h}_{i,t}
$$
where $y^{1,h}_{i,t}$ is the forecast for $y_{i,t}$ that accounts for the yet-to-be observed structural shock and is based on the information set $\mathcal{F}_{t-h}$, and $\hat y^{2,h}_{i,t}$ is defined similarly for the forecast that does not include any shock effect information. We will compare these forecasts in terms of their loss differential
$$
  \dbf_{i,t} = \L_{i,t,1} - \L_{i,t,2},
$$
where $\L_{i,t,j} \in \R^H$ has elements $L^h(y_{i,t}$, $\hat y_{i,t}^{j,h})$, $j = 1,2$, and $L$ is a loss function. Hypothesis tests in \cite{quaedvlieg2021multi} are with respect to $E(\dbf_{i,t}) = \mbf\mu_{i,t}$. Conditions for these tests require conditions of \cite{giacomini2006tests}. 

We will be interested in $\mathbf{\mu}_i = \lim_{T\to\infty}\frac{1}{T}\sum_{i=1}^T \mathbf{\mu}_{i,t}$.

\vspace*{0.5cm}\noindent\textbf{Note}: We need more formality for constructing $\hat y^{1,h}_{1,t}$. We could use the forecasts in \cite{lin2021minimizing} and then consider $h$-ahead methods after adjusting for the shock. Or we could consider aggregation approaches which average all post-shock responses of the series in the donor pool. 
\vspace*{0.5cm}

We will consider the average superior predictive ability (aSPA) to assess whether or not a shock is permanent or transitory. The aSPA investigates forecast comparisons based on their weighted average loss difference
$$
  \mu^{(\text{Avg})}_i = \textbf{w}_i'\mathbf{\mu}_i = \sum_{h=1}^H w_{i,h} \mu_i^h,
$$
with weights $\textbf{w}_i$ that sum to one. Note that aSPA requires the user to take a stand on the relative importance of under-performance at one horizon against out-performance at another, and note that it is likely that $\mu_i^h > 0$ for $h$ closer to 1 since the user expects that a structural shock will occur and the structural shock is taken into account by forecast 1. 

We consider a simple test for average SPA, based on the weighted-average loss differential. The associated null is
\begin{equation} \label{test}
	H_{i,\text{aSPA}}^0: \mu^{(\text{Avg})}_i \leq 0.
\end{equation}
A studentized test statistic corresponding to the null hypothesis \eqref{test} is of the form 
\begin{equation} \label{tstat}
	t_{i,\text{aSPA}} = \frac{\sqrt{T}_i\bar{d}_{i}}{\hat{\zeta}_{i}},
\end{equation}
where $\bar{d}_{i} = \textbf{w}_i'\textbf{d}_{i}$ and we choose to estimate $\zeta_{i} = \sqrt{\textbf{w}_i'\Omega_{i}\textbf{w}_i}$ directly based on $\textbf{w}_i'\textbf{d}_{i,t}$ using the HAC estimator \citep{giacomini2006tests} where $\Omega_{i} = \text{avar}\left(\sqrt{T}_i(\bar{d}_{i} - \mbf{\mu}_{i})\right)$.





\subsection{Model setup}
\label{modelsetup}

\noindent\textbf{Note}: We need to update the modeling setup. We should present a general modeling class which includes both the decay model and the permanent shift model.

\vspace{0.5cm}\noindent\textbf{Possible modeling approach}

\vspace{0.5cm} We now describe the assumed autoregressive models with random effects for which post-shock aggregated estimators are provided. The model $\mc{M}$ is defined as
\begin{align}
\mc{M} \colon \begin{array}{l}
  y_{i,t} =\eta_i + \sum_{j=1}^{q_1}\phi_{i,j}y_{i, t-j} + \sum_{j=0}^{q_2-1}\theta_{i,j+1}'\mbf{x}_{i,t-j} + \alpha_i D_{i,t}f(t) + \varepsilon_{i,t},\\[.2cm]
  %\; f(\mc{F}_{i,t-1},\x_{i,t-1},\alpha_i) =  \\[.2cm] %+ \sum_{j=1}^{q_3}\t\phi_{i,j}y_{i, t-j} + \sum_{j=0}^{q_4-1}\t\theta_{i,j+1}'\mbf{x}_{i,t-j}, \\[.2cm]
  \; \alpha_i = \mu_{\alpha} + \delta_i'\mbf{x}_{i, T_i^*+1} + \t{\varepsilon}_{i},
  %\sum_{j=1}^{q_3}\delta_{i,j}'\mbf{x}_{i, T_i^*-j+1}+ \t{\varepsilon}_{i}, 
\end{array}\label{model}
\end{align}
where $D_{i,t} = I(t \geq T_i^* + 1)$, $I(\cdot)$ is the indicator function, $\x_{i,t} \in \R^{p}$ are fixed with $p \geq 1$, $f(t)$ is a bounded continuous function which does not cross zero and $\lim_{t\to\infty}g(t) = a \in \R$. Let 
$\mathbf{\phi}_i = (\phi_{i,1},\ldots,\phi_{i,q_1})'$, 
$\mathbf{\theta}_i = (\theta_{i,1},\ldots,\theta_{i,q_2})'$, 
%$\tilde{\mathbf{\phi}}_i = (\t\phi_{i,1},\ldots,\t\phi_{i,q_3})'$, 
%$\tilde{\mathbf{\theta}}_i = (\t\theta_{i,1},\ldots,\t\theta_{i,q_4})'$, 
$\mathbf{\delta}_i = (\delta_{i,1},\ldots,\delta_{i,q_3})'$, 
and suppose that the regression coefficients in \eqref{model} have the following random effects structure:
\begin{align*}
  \eta_i &\simiid \mc{F}_{\eta} \text{ with }  \; \mrm{E}_{\mc{F}_{\eta}}(\eta_i) = \mu_\eta, \mrm{Var}_{\mc{F}_{\eta}}(\eta_i)  = \sigma^2_{\eta}, \\
  \mathbf{\phi}_i &\simiid \mc{F}_{\mathbf{\phi}} \text{ where } |\phi_{i,j}| < 1, \\
  \mathbf{\theta}_i &\simiid \mc{F}_{\mathbf{\theta}} \text{ with }  \; \mrm{E}_{\mc{F}_{\mathbf{\theta}}}(\theta_i) = \mu_{\mathbf{\theta}}, \mrm{Var}_{\mc{F}_{\mathbf{\theta}}}(\mathbf{\theta}_i)  = \Sigma^2_{\mathbf{\theta}}, \\
  %\tilde{\mathbf{\phi}}_i &\simiid \mc{F}_{\tilde{\mathbf{\phi}}} \text{ where } |\t\phi_{i,j}| < 1, \\
  %\tilde{\mathbf{\theta}}_i &\simiid \mc{F}_{\tilde{\mathbf{\theta}}} \text{ with }  \; \mrm{E}_{\mc{F}_{\tilde{\mathbf{\theta}}}}(\theta_i) = \mu_{\tilde{\mathbf{\theta}}}, \mrm{Var}_{\mc{F}_{\tilde{\mathbf{\theta}}}}(\tilde{\mathbf{\theta}}_i)  = \Sigma^2_{\tilde{\mathbf{\theta}}}, \\  
  \mathbf{\delta}_i &\simiid  \mc{F}_{\mathbf{\delta}} \text{ with } \mrm{E}_{\mc{F}_{\mathbf{\delta}}}(\mathbf{\delta}_i)=\mu_{\mathbf{\delta}}, \mrm{Var}_{\mc{F}_{\mathbf{\delta}}}(\mathbf{\delta}_i)=\Sigma_{\mathbf{\delta}}, \\
\varepsilon_{i,t} & \simiid  \mc{F}_{\varepsilon_i} \text{ with }  \; \mrm{E}_{\mc{F}_{\varepsilon_i}}(\varepsilon_{i,t}) = 0, \mrm{Var}_{\mc{F}_{\varepsilon_i}}(\varepsilon_{i,t})  = \sigma^2_i,  \\
\t{\varepsilon}_{i} &\simiid  \mc{F}_{\t{\varepsilon}} \text{ with }\mrm{E}_{\mc{F}_{\t{\varepsilon}}}(\t{\varepsilon}_{i})=0, \mrm{Var}_{\mc{F}_{\t{\varepsilon}}}(\t{\varepsilon}_{i})=\sigma^2_{\alpha}, \\
&\eta_i \indep \mathbf{\phi}_i \indep \mathbf{\theta}_i \indep 
%\tilde{\mathbf{\phi}}_i \indep \tilde{\mathbf{\theta}}_i \indep 
\mathbf{\delta}_i \indep \varepsilon_{i,t} \indep \tilde\varepsilon_{i,t}.
\end{align*}
The model \eqref{model} with the above random effects structure is a generalization of both model formulations in \cite{lin2021minimizing}. \textbf{Need to carefully show}. In this formulation $f(t)$ represents either a permanent or transient structural change to the time series that results from the shock.

We further define the parameter sets
\begin{align}
\begin{array}{l}
 \;\, \Theta = \{(\eta_i, \mathbf{\phi}_i, \mathbf{\theta}_i, \mathbf{\delta}_i, \alpha_i, \mbf{x}_{i,t}, y_{i,t-1})\colon    t= 1, \ldots, T_i, i = 2, \ldots, n +1\} \\
   \Theta_1 = \{(\eta_i, \mathbf{\phi}_i, \mathbf{\theta}_i, \mathbf{\delta}_i, \alpha_i, \mbf{x}_{i,t}, y_{i,t-1})\colon  t= 1, \ldots, T_i, i = 1\}
\end{array}\label{parameter} 
\end{align}


\vspace*{0.5cm}\noindent\textbf{Alternative model formulation} 

\vspace*{0.5cm} We now describe the assumed autoregressive models with random effects for which post-shock aggregated estimators are provided. The model $\mc{M}$ is defined as
\begin{align}
\mc{M} \colon \begin{array}{l}
  y_{i,t} = \left(\eta_i + \sum_{j=1}^{q_1}\phi_{i,j}y_{i, t-j} + \sum_{j=0}^{q_2-1}\theta_{i,j+1}'\mbf{x}_{i,t-j}\right)(1 - D_{i,t}) + f(\mathcal{F}_{i,t},\alpha_i)D_{i,t} + \varepsilon_{i,t},\\[.2cm]
  \; f(\mc{F}_{i,t},\alpha_i) = \alpha_i +  \sum_{j=1}^{q_{1}}\t\phi_{i,j}y_{i, t-j} + \sum_{j=0}^{q_2-1}\t\theta_{i,j+1}'\mbf{x}_{i,t-j}, \\[.2cm]
  \; \alpha_i = \mu_{\alpha} + \delta_i'\mbf{x}_{i, T_i^*+1} + \varepsilon_{\alpha, i},
  %\sum_{j=1}^{q_3}\delta_{i,j}'\mbf{x}_{i, T_i^*-j+1}+ \t{\varepsilon}_{i}, 
\end{array}\label{model-gen}
\end{align}
where $g(t)$ is a known or estimable bounded continuous function which does not cross zero and $\lim_{t\to\infty}g(t) = a \in \R$. Let 
$\mathbf{\phi}_i = (\phi_{i,1},\ldots,\phi_{i,q_1})'$, 
$\mathbf{\theta}_i = (\theta_{i,1},\ldots,\theta_{i,q_2})'$, 
$\tilde{\mathbf{\phi}}_i = (\t\phi_{i,1},\ldots,\t\phi_{i,q_{1}})'$, 
$\tilde{\mathbf{\theta}}_i = (\t\theta_{i,1},\ldots,\t\theta_{i, q_{2}})'$, 
$\mathbf{\delta}_i = (\delta_{i,1},\ldots,\delta_{i,p})'$, 
and suppose that the regression coefficients in \eqref{model-gen} have the following hierarchical random effects structure: 
\begin{equation} \label{random-effects}
\begin{split}
  \eta_i &\simiid \mc{F}_{\eta} \text{ with }  \; \mrm{E}_{\mc{F}_{\eta}}(\eta_i) = \mu_\eta, \mrm{Var}_{\mc{F}_{\eta}}(\eta_i)  = \sigma^2_{\eta}, \\
  \mathbf{\phi}_{i,j} &\simiid \mc{F}_{\mathbf{\phi}_j} \text{ where } |\phi_{i,j}| < 1, \\
  \mathbf{\theta}_{i,j} &\simiid \mc{F}_{\mathbf{\theta}_j} \text{ with }  \; \mrm{E}_{\mc{F}_{\mathbf{\theta}_j}}(\theta_{i,j}) = \mu_{\mathbf{\theta}_j}, \mrm{Var}_{\mc{F}_{\mathbf{\theta}_j}}(\mathbf{\theta}_{i,j})  = \Sigma^2_{\mathbf{\theta}_j}, \\
  \mathbf{\delta}_i &\simiid  \mc{F}_{\mathbf{\delta}} \text{ with } \mrm{E}_{\mc{F}_{\mathbf{\delta}}}(\mathbf{\delta}_i)=\mu_{\mathbf{\delta}}, \mrm{Var}_{\mc{F}_{\mathbf{\delta}}}(\mathbf{\delta}_i)=\Sigma_{\mathbf{\delta}}, \\
\varepsilon_{i,t} & \simiid  \mc{F}_{\varepsilon_i} \text{ with }  \; \mrm{E}_{\mc{F}_{\varepsilon_i}}(\varepsilon_{i,t}) = 0, \mrm{Var}_{\mc{F}_{\varepsilon_i}}(\varepsilon_{i,t})  = \sigma^2_i,  \\
\varepsilon_{\alpha, i} &\simiid  \mc{F}_{\varepsilon_\alpha} \text{ with }\mrm{E}_{\mc{F}_{\varepsilon_\alpha}}(\varepsilon_{\alpha, i})=0, \mrm{Var}_{\mc{F}_{\varepsilon_{\alpha}}}(\varepsilon_{\alpha, i})=\sigma^2_{\alpha}, \\
  \tilde{\mathbf{\phi}}_{i,j} &\overset{ind}{\sim} \mathcal{F}_{\tilde{\mathbf{\phi}_j}}(\x_{i,T_i^*+1}) \; \text{where} \; |\tilde{\mathbf{\phi}}_{i,j}| < 1,  \\
  \tilde{\mathbf{\theta}}_{i,j} &= \theta_{i,j} + \gamma_{i,j}, j = 1,\ldots, q_{2}, \; \text{where} \; \gamma_{i,j} \simiid \mathcal{F}_{\gamma} \; \text{with} \; \mrm{E}_{\mathcal{F}_{\gamma}}(\gamma_{i,j}) = \beta_o + \beta\mbf{x}_{i,T_i^*+1}, \mrm{Var}_{\mathcal{F}_{\gamma}}(\gamma_{i,j}) = \Sigma^2_\gamma,  \\  
 % \t q_{i,k} &\overset{ind}{\sim} \mathcal{F}_k(\x_{i,T_i^*}), \; k = 1,2, \\
&\eta_i \indep \mathbf{\phi}_{i,j} \indep \mathbf{\theta}_{i,j} \indep 
\tilde{\mathbf{\phi}}_{i,j} \indep \lambda_{i,j} \indep \gamma_i \indep \mathbf{\delta}_i \indep \varepsilon_{i,t} \indep \tilde\varepsilon_{i}, 
\end{split}
\end{equation}
where the distribution $\mathcal{F}_{\tilde{\mathbf{\phi}}_j}(\x_{i,T_i^*+1})$ exists between -1 and 1 and $\mrm{E}(\mathcal{F}_{\tilde{\mathbf{\phi}}_j}(\x_{i,T_i^*+1})) - \mrm{E}(\mathcal{F}_{\tilde{\mathbf{\phi}}_j}(\x_{j,T_i^*+1})) \to 0$ as $\|\x_{i,T_i^*+1} - \x_{j,T_i^*+1}\| \to 0$.
%the distribution $\mathcal{F}_k(\x_{i,T_i^*})$ is a discrete distribution conditional on $\x_{i,T_i^*}$, $\theta_{i,j} = 0$ for any $q_1 < j \leq \t q_{i,1}$, and $\phi_{i,j} = 0$ for any $q_2 < j \leq \t q_{i,2}$. 
The model \eqref{model-gen} with the above random effects structure is a generalization of both model formulations in \cite{lin2021minimizing}. \textbf{Need to carefully show}. Note that for model \eqref{model-gen} to be of use for post-shock forecasting, the variation in $\mathcal{F}_{\tilde{\mathbf{\phi}}}$ needs to be small relative to the signal captured in $\x_{i,T_i^*}$, and $\sigma^2_\alpha$ and $\Sigma^2_\gamma$ needs to be small relative to $\mu_\alpha + \delta_i'\x_{i,T_i^*}$ and $\beta_o + \beta\mbf{x}_{i,T_i^*+1}$ respectively. 

We see that model \eqref{model-gen} with its accompanying random effects structure is flexible enough to capture changing structural dynamics as well as a mean-shift. These dynamic changes depend heavily on the value of $\mbf{x}_{i,T_i^*+1}$. Two series $i,j$ with small $\|\mbf{x}_{i,T_i^*+1} - \mbf{x}_{j,T_i^*+1}\|_2$ are expected to experience similar structural changes. This makes distance based weighting an attractive avenue.








\subsection{Forecasting and testing for shock persistence}
\label{forecast}

\noindent\textbf{Note}: Our forecast needs to be written with respect to our general model. Specifics can be given when we conduct our numerical examples.
\vspace*{0.5cm}

In our post-shock setting we consider the following candidate forecasts: 
\begin{align*}
  &\text{Forecast 1}: \\
  &\text{Forecast 2}: 
\end{align*}
We want to determine which forecast is appropriate over a horizon while the methods in \cite{lin2021minimizing} were only appropriate in the nowcasting setting in which prediction was only focused on the response immediately following the shock.


\vspace*{0.5cm}\noindent\textbf{Note}: We need to explain what the voting method is, possibly in algorithmic format. Also note that the p-values that we obtain are computed using a bootstrap procedure. Perhaps an additional proposition that states the performance of these bootstrap p-values would further guarantee reliability. 

\begin{prop}
  Let $p_i \sim \mc{D}$ be independent sequence of p-values for $i = 1, \ldots, n+1$ and some distribution $\mc{D}$, where $p_1$ is the p-value of the time series of interest. Let  $\alpha$ denote the significance level. If $\P(p_1 \leq \alpha) \neq 0.5$, with probability one, the expected misclassification rate for voting in prevalence testing is
  \begin{align*}
    \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases} .
  \end{align*}
\end{prop}

\begin{proof}
 Since $\P(p_1 \leq \alpha)\in \reals$, by Strong Law of Large Numbers,
\begin{align*}
  \frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha)  
   \stackrel{a.s.}{\rightarrow}  \P(p_i \leq \alpha) = \P(p_1 \leq \alpha),
\end{align*}
which follows from the fact that $p_i$ are i.i.d. Define
\begin{align*}
  f \colon [0,1] \mapsto \{0,1\} 
  \text{ with }
  f(x) = I(x \geq 0.5).
\end{align*}
Let $C(f)$ denote the continuity set of $f$. Suppose that $\P(p_1 \leq  \alpha) \neq 0.5$. In this case, notice that 
\begin{align*}
  \P(\P(p_1 \leq  \alpha) \in C(f)) =1.
\end{align*}
By Slutsky's Theorem, we have
\begin{align*}
  I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  \stackrel{a.s.}{\rightarrow} 
  I\{ \P(p_1 \leq  \alpha) \geq 0.5\}.
\end{align*}
Moreover, note that
\begin{align*}
\E \left\{ |I\{ \P(p_1 \leq  \alpha) \geq 0.5\}
  - I( p_1 \leq  \alpha) |\right\}
  & = \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases} \\
  & \leq  0.5.
\end{align*}
That implies that with probability one, 
\begin{align*}
  \E \left\{ \left|I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \right|\right\}
  = \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases}.
\end{align*}
That is, with probability one, the expected misclassification rate is as above.
\end{proof}

\begin{remark}
  Since indicator functions are bounded in $\mc{L}^2$, by Chebyshev-Rachman Strong Law of Large Numbers, the independence assumption can be relaxed to be that those p-values are uncorrelated.
\end{remark}






\vspace*{0.5cm}\noindent\textbf{Possible theoretical approach}: \\


Let $l$ be the block length of the moving block bootstrap (MBB) where we assume that $T_i = lK_i$. Let $I_1$,\ldots,$I_{K_i}$ be iid random variables uniformly distributed on $\{1,\ldots,T_i-l+1\}$, and define the array 
$$
  \tau_{T_i} = \{I_1+1, \ldots, I_1+l, \ldots, I_{K_i}+1, \ldots, I_{K_i}+l\}.
$$
The pseudo time-series is therefore $\textbf{d}^b_{i,t} = \textbf{d}^b_{i,\tau_{T_i}}$, with elements $d^{hb}_{i,t}$.

\begin{equation} \label{whb}
	\left(\hat{\omega}_{i}^{hb}\right)^2 = \frac{1}{K_i}\sum_{k=1}^{K_i}\left[\frac{1}{l}\left(\sum_{t=1}^l d^{hb}_{i,(k-1)l + t} - \bar{d}^{hb}_{i}\right)^2\right],
\end{equation}
where $\bar{d}^{hb}_{i} = \frac{1}{T_i}\sum_{t=1}^{T_i} d^{hb}_{i,t}$. From the conditions of Theorem 1 and Corollary 1 in \cite{quaedvlieg2021multi}  we have 
\begin{equation} \label{boott}
  \sup_{z\in\R}\Big\vert \Prob^b\left[\sqrt{T_i}\frac{\textbf{w}_i'\bar{\dbf}^b_{i} - \textbf{w}_i'\bar{\dbf}_{i} }{\hat{\zeta}^b_{i}}\right] 
  - \Prob\left[\sqrt{T_i}\frac{\textbf{w}_i'\bar{\dbf}_{i} -  \textbf{w}_i'\mbf{\mu}_{i}}{\hat{\zeta}^b_{i}}\right]
  \Big\vert \overset{P}{\longrightarrow} 0,
\end{equation}
as $T_i\to\infty$ where $l=l_{T_i}=o(\sqrt{T_i})$. We can compute a bootstrap $p$-value as 
$$
  \hat{p}_i^B = \frac{1}{B}\sum_{b=1}^B 1\{t_{i,\text{aSPA}} < t^b_{i,\text{aSPA}}\}. 
$$
The results \eqref{boott} implies that 
\begin{equation} \label{bootp}
  |\hat p_i^B - \hat p_i| \overset{P}{\to} 0, \quad \text{as} \quad T_i,B \to \infty, 
\end{equation}
    where $\hat p_i = \Prob(t_{i,\alpha} < t_{i,\text{aSPA}})$ and $t_{i,\alpha}$ is a critical value corresponding to the distribution of $t_{i,\text{aSPA}}$. Therefore bootstrap inference in the donor pool is a good approximation. %We now consider how bootstrap inference in the donor pool is used to test whether a shock is prevalent or transient for the time series under study.


\vspace*{0.5cm}\noindent\textbf{This needs some work (ignore for now)} Now consider a case where there exists a $2 \leq i' < n + 1$ where $\x_{1,t} = \x_{i',t}$. Now suppose for simplicity that $T^*_{j'} = T^*_1$ and that $T_{i'} = T_1$ where $T_1 > T^*_1 + q_1 + q_2 + 2$. This assumes that we have data for the first time series beyond the shock point. We will show that when there is a covariate clone of the time series under study, then estimation in this series will approximate estimation in the time series under study provided that the underlying variability is very small relative to the signal. To see this, consider $t > T^*_1$, where 
\begin{align*}
y_{1,t} &= \eta_1 + \mu_\alpha + \delta_1'\x_{1,T^*_1+1} + \sum_{j=1}^{q_1} (\phi_{1,j} + \tilde{\phi}_{1,j})y_{1,t-j} + \sum_{j=0}^{q_2-1}(\theta_{1,j+1} + \tilde{\theta}_{1,j+1})'\x_{1,t-j} + \tilde{\varepsilon}_{1} + \varepsilon_{1,t}, \\
y_{i',t} &= \eta_{i'} + \mu_\alpha + \delta_{i'}'\x_{1,T^*_1+1} + \sum_{j=1}^{q_1} (\phi_{i',j} + \tilde{\phi}_{i',j})y_{i',t-j} + \sum_{j=0}^{q_2-1}(\theta_{i',j+1} + \tilde{\theta}_{i',j+1})'\x_{1,t-j} + \tilde{\varepsilon}_{i'} + \varepsilon_{i',t}, \\
\hat y_{1,t} &= \hat \eta_1 + \hat{\alpha}_1(\x_{1,T^*_1+1}) + \sum_{j=1}^{q_1} (\hat{\phi}_{1,j} + \hat{\tilde{\phi}}_{1,j})\hat y_{1,t-j} + \sum_{j=0}^{q_2-1}(\hat\theta_{1,j+1} + \hat{\tilde{\theta}}_{1,j+1})'\hat\x_{1,t-j}, \\
\hat y_{i',t} &= \hat\eta_{i'} + \hat{\alpha}_{i'}(\x_{1,T^*_1+1}) + \sum_{j=1}^{q_1} (\hat{\phi}_{i',j} + \hat{\tilde{\phi}}_{i',j})\hat y_{i',t-j} + \sum_{j=0}^{q_2-1}(\hat\theta_{i',j+1} + \hat{\tilde{\theta}}_{i',j+1})'\hat\x_{1,t-j}, 
\end{align*}
where $\alpha_j(\x) = \mu_\alpha + \delta_j'\x$, and we have 
\begin{align*}
  |y_{1,t} - \hat y_{1,t}| &= \Bigg\vert (\eta_1 - \hat\eta_1) + (\alpha_1 - \hat{\alpha}_1(\x_{1,T^*_1+1})) + \sum_{j=1}^{q_1}\left[(\phi_{1,j} + \tilde{\phi}_{1,j})y_{1,t-j} - (\hat\phi_{1,j} + \hat{\tilde{\phi}}_{1,j})\hat{y}_{1,t-j}\left] \\ 
  &\qquad+ \sum_{j=0}^{q_2-1}\left[(\theta_{1,j+1} + \tilde{\theta}_{1,j+1})'\x_{1,t-j} - (\hat\theta_{1,j+1} + \hat{\tilde{\theta}}_{1,j+1})'\hat\x_{1,t-j}\right] + \varepsilon_{1,t} \Bigg\vert \\
  |y_{i',t} - \hat y_{i',t}| &= \Bigg\vert (\eta_{i'} - \hat\eta_{i'}) + (\alpha_{i'} - \hat{\alpha}_{i'}(\x_{1,T^*_1+1})) + \sum_{j=1}^{q_1}\left[(\phi_{i',j} + \tilde{\phi}_{i',j})y_{i',t-j} - (\hat\phi_{i',j} + \hat{\tilde{\phi}}_{i',j})\hat{y}_{i',t-j}\left] \\ 
  &\qquad+ \sum_{j=0}^{q_2-1}\left[(\theta_{i',j+1} + \tilde{\theta}_{i',j+1})'\x_{1,t-j} - (\hat\theta_{i',j+1} + \hat{\tilde{\theta}}_{i',j+1})'\hat\x_{1,t-j}\right] + \varepsilon_{i',t} \Bigg\vert.
\end{align*}
\textbf{Needs finishing, but the argument is simple: estimation uncertainty declines as error variance decreases. And differences in the random effects decrease as variability decreases.} \vspace*{0.5cm}




\vspace*{0.5cm} We now demonstrate an idealized setting in which the above logic holds. We first present some intermediary technical results and definitions. Let $\lambda_{\text{max},\theta_k}$, $\lambda_{\text{max},\theta_\delta}$, $\lambda_{\text{max},\gamma}$ respectively be the largest eigenvalues of $\Sigma_{\theta_k}$, $\Sigma_{\delta}$, and $\Sigma_{\gamma}$ for $k = 1,\ldots,q_2$. Define  
\begin{align*}
  \mu_{\min} &= \min\left(\mu_\eta,\mu_{\phi_j},\mu_{\tilde\phi_j},\min(\mu_\delta),\min(\mu_{\theta_k}),\min(\mu_{\tilde\theta_k}); j = 1,\ldots,q_1, k = 1,\ldots,q_2\right), \\
  \mu_{\max} &= \max\left(\mu_\eta,\mu_{\phi_j},\mu_{\tilde\phi_j},\max(\mu_\delta),\max(\mu_{\theta_k}),\max(\mu_{\tilde\theta_k}); j = 1,\ldots,q_1, k = 1,\ldots,q_2\right), \\
  \lambda_{\max} &= \max\left(\lambda_{\text{max},\theta_k}, \lambda_{\text{max},\theta_\delta}, \lambda_{\text{max},\gamma}; k = 1,\ldots,q_2\right), \\
  \sigma^2_\max &= \max\left(\sigma^2_\eta,\sigma^2_{\phi_j},\sigma^2_i,\sigma^2_\alpha,\sigma^2_{\tilde\phi_j}; i = 1,\ldots,n+1, j = 1,\ldots,q_1\right).  
\end{align*}



\begin{prop} \label{prop:signaltonoise}
	Let $\bf{y}_{i,t}$, $i = 1,\ldots,n_i$, $t = 1,\ldots, T$ be a collection of independent time series. Let each time series $\bf{y}_{i,t}$ be generated by model \eqref{model-gen} with accompanying random effects structure \eqref{random-effects}, where $y_{i,t}$ is a scalar response $\x_{i,t} \in \R^p$ is a fixed vector of covariates. Suppose that there is a common shock time $p + 2 < T^* < T - (p+2)$ where $q_1 = q_2 = 1$, $\|\x_{i,t}\| = 1$, each entry of $\x_{i,t}$ is non-negative, $\mu_{\min} > 0$, and $y_{i,0} = 0$. Then for every $1 \leq t \leq T$ we have that 
	\begin{equation} \label{signal-to-noise}
	  \E(y_{i,t}) \geq C\Var(y_{i,t}),	
	\end{equation}
	provided that 
	\begin{equation} \label{condition-1}
	\begin{split}
		\mu_\alpha + 3\mu_{\min} &\geq C\left(3\sigma^2_{\max} + 2\lambda_{\max}\right), \\
		2\mu_{\min} &\geq C\left(2\sigma^2_{\max} + \lambda_{\max}\right).	
	\end{split}
	\end{equation}
\end{prop}


\textbf{Remarks}:
\begin{enumerate}
	\item The conditions \eqref{condition-1} govern the signal strength necessary in the expected behavior of the underlying random effects structure relative to variability necessary for the overall signal to noise ratio for the overall time series to be lower bounded by some positive value $C$.
	\item The conditions of Proposition~\ref{prop:signaltonoise} state that there is an increasing trend in $y_{i,t}$ for all $i$.
\end{enumerate}



\begin{proof}
For $t > T^* + 1$ we have 
\begin{align*}
  \E(y_{i,t}) &= \E\left[\E(y_{i,t}|y_{i,t-1})\right] \\ 
    &= \E\left[\E(\eta_i + \alpha_i + \tilde\phi_i y_{i,t-1} 
      + \tilde\theta_i'\x_{i,t}|y_{i,t-1})\right] \\
    &= \E(\eta_i + \mu_\alpha + \delta_i'\x_{i,T^*+1} + \tilde\phi_i y_{i,t-1} + \tilde\theta_i'\x_{i,t}) \\
    &= \mu_\eta + \mu_\alpha + \mu_\delta'\x_{i,T^*+1} + \mu_{\tilde\theta}'\x_{i,t} + \mu_{\tilde\phi}\E(y_{i,t-1}) \\
    &\geq (\mu_\alpha + 3\mu_{\min}) + \mu_{\min}\E(y_{i,t-1}) \\
    &\geq \sum_{j=1}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\E(y_{i,T^*}) \\
    &= \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\E\left[\E(y_{i,T^*}|y_{i,T^*-1})\right]\right) \\   
    &= \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\E(\eta_i + \theta_i'\x_{i,T^*} + \phi_i y_{i,T^*-1}) \right) \\
    &\geq \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(2\mu_{\min} + \mu_{\min} \E(y_{i,T^*-1}) \right) \\
    &\geq \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\sum_{j=0}^{T^*-1}\mu_{\min}^j(2\mu_{\min}) \right), 
\end{align*}
and 
\begin{align*}
  \Var(y_{i,t})	&= \E\left[\Var(y_{i,t}|y_{i,t-1})\right] + \Var\left[\E(y_{i,t}|y_{i,t-1})\right] \\
  &= \E\left[\Var(\eta_i + \mu_\alpha + \delta_i'\x_{i,T^*+1} + \tilde\theta_i'\x_{i,t} + \tilde\phi_i y_{i,t-1} + \varepsilon_{i,t} + \varepsilon_{\alpha,i}|y_{i,t-1})\right] \\
  &+ \Var\left[\E(\eta_i + \mu_\alpha + \delta_i'\x_{i,T^*+1} + \tilde\theta_i'\x_{i,t} + \tilde\phi_i y_{i,t-1} + \varepsilon_{i,t} + \varepsilon_{\alpha,i}|y_{i,t-1})\right] \\
  &\leq 3\sigma^2_{\max} + 2\lambda_{\max} + \Var\left[\E(\tilde\phi_i y_{i,t-1}|y_{i,t-1})\right] \\
  &= 3\sigma^2_{\max} + 2\lambda_{\max} + \mu_{\tilde\phi}^2\Var(y_{i,t-1}) \\
  &\leq \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\Var(y_{i,T^*}) \\
  &= \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\E\left[\Var(y_{i,T^*}|y_{i,T^*-1})\right] + \Var\left[\E(y_{i,T^*}|y_{i,T^*-1})\right]\right) \\
  &= \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\E\left[\Var(\eta_i + \theta_i'\x_{i,T^*} + \phi_iy_{i,T^*-1} + \varepsilon_{i,T^*}|y_{i,T^*-1})\right] \right. \\ 
  &\qquad+  \left.\Var\left[\E(\eta_i + \theta_i'\x_{i,T^*} + \phi_iy_{i,T^*-1} + \varepsilon_{i,T^*}|y_{i,T^*-1})\right]\right)  \\
  &\leq \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(2\sigma^2_{\max} + \lambda_{\max} + \mu_{\phi}^2\Var(y_{i,T^*-1}) \right) \\ 
  &\leq \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right).  
\end{align*}
Combining the above with the condition \eqref{condition-1} yields 
\begin{align*}
  \frac{\E(y_{i,t})}{\Var(y_{i,t})} &\geq \frac{\sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\sum_{j=0}^{T^*-1}\mu_{\min}^j(2\mu_{\min}) \right)}{\sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right)} \\
  &\geq \frac{C\sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + C(\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right)}{\sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right)} \\
  &= C,
\end{align*}
for all $i$ and $t$. A similar argument holds when $t \leq T^* + 1$.
\end{proof}


\begin{lem}[Technical Lemma 1] \label{lem1}
	Let $a_j, b_j \in \R$, $j = 1,2$, and $C > 0$ be such that $C \leq a_j/b_j \leq 2C$, $j = 1,2$. Further suppose that 
	$$
	  \frac{|a_1 - a_2|}{a_1 + a_2} < \frac{\varepsilon}{2C},
	$$
	for some $\varepsilon > 0$. Then 
	$$
	  \frac{|a_1 - a_2|}{b_1 + b_2} < \varepsilon.
	$$
\end{lem}

\begin{proof}
Observe that 
$$
  |a_1 - a_2| < \frac{\varepsilon(a_1 + a_2)}{2C} 
	  = \frac{\varepsilon(b_1(a_1/b_1) + b_2(a_2/b_2))}{2C} 
	  \leq \frac{\varepsilon(2b_1C + 2b_2C)}{2C} 
	  = \varepsilon(b_1 + b_2).
$$
The conclusion immediately follows from the above.
\end{proof}


\begin{lem}[Technical Lemma 2] \label{lem2}
Let $a_j, b_j \in \R$, $j = 1,2$ be such that $0 < a_1,a_2 < 1$, $b_1,b_2 > 0$. Let $c_1 = \max(a_1,a_2)$, $c_2 = \min(a_1,a_2)$, $d_1 = \max(b_1,b_2)$, $d_2 = \min(b_1,b_2)$. Then, 
$$
  |a_1b_1 - a_2b_2| \leq |a_1 - a_2|d_1 + |b_1 - b_2|c_2.
$$
\end{lem}

\begin{proof}
Observe that 
$$
  |a_1b_1 - a_2b_2| \leq c_1d_1 - c_2d_2 = (c_1 - c_2)d_1 + (d_1 - d_2)c_2. 
$$
The conclusion follows by noting that $c_1 - c_2 = |a_1 - a_2|$ and $d_1 - d_2 = |b_1 - b_2|$.
\end{proof}


\begin{prop} \label{prop:signaltonoisediff}
	Suppose the conditions of Proposition~\ref{prop:signaltonoise} where $C$ is chosen so that 
	\begin{equation} \label{signal-to-noise2}
	  C \leq \frac{\E(y_{i,t})}{\Var(y_{i,t})} \leq 2C,
	\end{equation}
	for all $i$ and $1 \leq t \leq T$. Let $2 < i' < n + 1$ be the index of a time series in the donor pool for which CONDITIONS then for some $\varepsilon > 0$ we have 
	$$
	  \frac{\left|\E(y_{i,t} - y_{i',t})\right|}{\Var(y_{i,t} - y_{i',t})} < \varepsilon
	$$
\end{prop}

\begin{proof}
	First suppose that we have 
	\begin{equation} \label{prop:cond}
	  \frac{\left|\E(y_{i,t} - y_{i',t})\right|}{\E(y_{i,t} + y_{i',t})} \leq \frac{\varepsilon}{2C}.		
	\end{equation}
	Then Lemma~\ref{lem2} combined with the assumed conditions imply the conclusion of this proposition. We now verify that \eqref{prop:cond} holds.
	
	For $t > T^* + 1$ a similar recursion in the proof of Proposition~\ref{prop:signaltonoise} yields 
	\begin{align*}
		&\E(y_{i,t} - y_{i',t}) = \E\left( \delta_i'\x_{i,T^*+1} + \tilde\theta_i'\x_{i,T^*+1} + \tilde\phi_i y_{i,t-1} - (\delta_{i'}'\x_{i',T^*+1} + \tilde\theta_{i'}'\x_{i',T^*+1} + \tilde\phi_{i'} y_{i',t-1}) \right) \\
		&\qquad= \mu_\delta'\x_{i,T^*+1} + \mu_{\tilde\theta_i}'\x_{i,T^*+1} + \mu_{\tilde\phi_i} \E(y_{i,t-1}) - (\mu_\delta'\x_{i',T^*+1} + \mu_{\tilde\theta_{i'}}'\x_{i',T^*+1} + \mu_{\tilde\phi_{i'}}\E(y_{i',t-1})) \\
		&\qquad= \sum_{j=0}^{t - (T^*+1)}\left[\left(\mu_{\tilde\phi_{i}}^j(\mu_\delta'\x_{i,T^*+1}) - \mu_{\tilde\phi_{i'}}^j(\mu_\delta'\x_{i',T^*+1})\right) + \mu_{\tilde\phi_{i}}^j(\mu_{\tilde\theta_{i}}'\x_{i,T^*+1}) - \mu_{\tilde\phi_{i'}}^j(\mu_{\tilde\theta_{i'}}'\x_{i',T^*+1})\right] \\
		&\qquad\qquad + \mu_{\tilde\phi_{i}}^{t - T^*}\E(y_{i,T^*}) - \mu_{\tilde\phi_{i'}}^{t - T^*}\E(y_{i',T^*}) \\
		&\qquad= \sum_{j=0}^{t - (T^*+1)}\left[\left(\mu_{\tilde\phi_{i}}^j(\mu_\delta'\x_{i,T^*+1}) - \mu_{\tilde\phi_{i'}}^j(\mu_\delta'\x_{i',T^*+1})\right) + \mu_{\tilde\phi_{i}}^j(\mu_{\tilde\theta_{i}}'\x_{i,T^*+1}) - \mu_{\tilde\phi_{i'}}^j(\mu_{\tilde\theta_{i'}}'\x_{i',T^*+1})\right] \\
		&\qquad\qquad + \sum_{j=0}^{T^* - 1}\left[\mu_\eta(\mu_{\tilde\phi_{i}}^{t - T^*} - \mu_{\tilde\phi_{i'}}^{t - T^*})\mu_{\phi}^j + \left(\mu_{\tilde\phi_{i}}^{t - T^*}(\mu_{\theta}'\x_{i,t-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i,t-j})\right)\mu_{\phi}^j\right].
	\end{align*}
Similarly, 
\begin{align*}
	&\E(y_{i,t} + y_{i',t}) = \sum_{j=0}^{t - (T^*+1)}\left[\left(\mu_{\tilde\phi_{i}}^j(\mu_\delta'\x_{i,T^*+1}) + \mu_{\tilde\phi_{i'}}^j(\mu_\delta'\x_{i',T^*+1})\right) + \mu_{\tilde\phi_{i}}^j(\mu_{\tilde\theta_{i}}'\x_{i,T^*+1}) + \mu_{\tilde\phi_{i'}}^j(\mu_{\tilde\theta_{i'}}'\x_{i',T^*+1})\right] \\
		&\qquad + \sum_{j=0}^{T^* - 1}\left[\mu_\eta(\mu_{\tilde\phi_{i}}^{t - T^*} + \mu_{\tilde\phi_{i'}}^{t - T^*})\mu_{\phi}^j + \left(\mu_{\tilde\phi_{i}}^{t - T^*}(\mu_{\theta}'\x_{i,t-j}) + \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i,t-j})\right)\mu_{\phi}^j\right].
\end{align*}
\end{proof}



%Let 
%$$
%  \rho_\text{min} = \min\left\{
%    \frac{\mu_\eta}{\sigma^2_\eta}, 
%    \frac{\|\mu_{\delta}\|}{\|\Sigma_\delta\|},
%    \frac{\|\mu_{\theta}\|}{\|\Sigma_\theta\|},
%    \frac{\|\mu_{\phi}\|}{\|\Sigma_\phi\|},
%    \frac{\|\mu_{\tilde\theta}\|}{\|\Sigma_{\tilde{\theta}}\|},
%    \frac{\|\mu_{\tilde\phi}\|}{\|\Sigma_{\tilde{\phi}}\|}             
%  \right\},
%$$
%and let
%$$
%  \rho_\text{max} = \max\left\{
%    \frac{\mu_\eta}{\sigma^2_\eta}, 
%    \frac{\|\mu_{\delta}\|}{\|\Sigma_\delta\|},
%    \frac{\|\mu_{\theta}\|}{\|\Sigma_\theta\|},
%    \frac{\|\mu_{\phi}\|}{\|\Sigma_\phi\|},
%    \frac{\|\mu_{\tilde\theta}\|}{\|\Sigma_{\tilde{\theta}}\|},
%    \frac{\|\mu_{\tilde\phi}\|}{\|\Sigma_{\tilde{\phi}}\|}             
%  \right\}.
%$$






\section{Simulation Setup}
\label{simulation}

Let $n$ denote the donor pool size, $p$ denote the number of covariates used, $H$ denote the number of horizon used, $T_i$ denote the length of time series to be evaluated for time series $i$, $K_i$ denote the training sample size used for each  forecasting  time series $i$, $T_i^*$ denote the time point just before the realization of the shock for time series $i$ for $i = 1, \ldots, n+1$.


In this setting $n$, $p$, and $H$ are pre-determined. $T_i, K_i \sim \mrm{Gamma}(15, 10)$.  The total sample size for $i$th time series is $T_i + K_i + H$. $T_i^*$ is randomly sampled from $\ceil{\frac{1}{4}T_i}+1$ to $\ceil{\frac{3}{4} T_i} + K_i+ H$. If $T_i, K_i < 90$, we force them to be 90. The adopted model for the data is as below:
\begin{align*}
  y_{i,t} &= \eta_i + \phi_i y_{i,t-1} + \mbf{x}_{i,t} \bs{\beta}_i + \alpha_i I(t > T_i^*) + \varepsilon_{i,t},\\
  \alpha_i &= \mu_{\alpha} + \mbf{x}_{i,T_i^*+1}\bs{\gamma}_i + \tilde{\varepsilon}_{i},
\end{align*}
where
\begin{align*}
  \phi_i & \sim \text{ indep. }  U(0,1) \\
  \eta_i & \sim \text{ indep. }  \mc{N}(0,1) \\
  \varepsilon_{i,t} & \sim \text{ indep. } \mc{N}(0, \sigma^2)\\
  \tilde{\varepsilon}_i & \sim \text{ indep. } \mc{N}(0, \sigma_{\alpha}^2)
  \\
  \bs{\gamma}_i & \sim \text{ indep. } \mc{N}(\mu_{\gamma}\bs{1}_p, \sigma_{\gamma}^2 \mbf{I}_p) \\
  \bs{\beta}_i & \sim \text{ indep. }  \mc{N}(\bs{0}_p, \mbf{I}_p).
\end{align*}
Moreover, the elements of $\mathbf{x}_{i,t}$ are independently distributed as $\mrm{Gamma}(1,\delta)$.

Note that $K_i$ is training sample size for time series $i$. Consider
\begin{align*}
  K_i & \sim \ceil{\mrm{Gamma}(a_{K}, b_K)}\\
   T_i & \sim \ceil{\mrm{Gamma}(a_{T}, b_T)} \\
  T_i^* &\equiv \max\{T_i+1, \ceil{0.5 \cdot(T_i+K_i+H)}\},
\end{align*}

$K_i+H+T_i^* > T_i+K_i+H$

 Then, we consider the following simulation setup
\begin{verbatim}
  ns <- c(5, 10, 20, 40)
  Tscale <- Kscale <- 1 / 2 # b_T, b_K 
  K.T.shape <- c(200, 400, 800, 1600) # for K_i and T_i
  mu.gamma.delta <- 2 # mean for parameter vector of shock
  sigma.delta.gamma <- 0.1 # sd for parameter vector of shock
  sigma.alpha <- 0.05 # sd for shock noise  
  sigma <- 0.1 # sd for response noise
  mu.alpha <- 50 # intercept for shock (relatively large)
  H <- 8 
  ell <- 4
  scale <- 2 # scale for covariates that follow Gamma distribution
\end{verbatim}

\begin{align*}
  y_{i,t} &= \eta_i + \phi_i y_{i,t-1} + \mbf{x}_{i,t} \bs{\beta}_i + \xi_i\cdot  I(t > T_i^*) + \varepsilon_{i,t},\\
  \xi_i &= \alpha_i \cdot e^{-(t-T_i^*-1)} \\
  \alpha_i &= \mu_{\alpha} + \mbf{x}_{i,T_i^*+1}\bs{\gamma}_i + \tilde{\varepsilon}_{i},
\end{align*}









\bibliographystyle{plainnat}
\bibliography{synthetic-prediction-notes}
%\bibliography{../synthetic-prediction-notes}

	
\end{document}


