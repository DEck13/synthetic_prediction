\documentclass[11pt]{article}

% use packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{url}
\usepackage{multirow}

\usepackage{graphicx}
%\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{authblk}
\usepackage{bm}
\usepackage[usenames]{color}
\usepackage{hyperref}

\usepackage{caption}
\usepackage{float}
\usepackage[caption = false]{subfig}
\usepackage{tikz}
\usepackage{multirow}
\usepackage[linesnumbered, ruled,vlined]{algorithm2e}
\usepackage{pdflscape}

% margin setup
\usepackage{geometry}
\geometry{margin=0.8in}


% function definition
\newcommand{\R}{\mathbb{R}}
\newcommand{\w}{\textbf{w}}
\newcommand{\x}{\textbf{x}}
\newcommand{\dbf}{\textbf{d}}
\newcommand{\y}{\textbf{y}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\Llet}{\textbf{L}}
\newcommand{\Hist}{\mathcal{H}}
\newcommand{\Prob}{\mathbb{P}}
\def\mbf#1{\mathbf{#1}} % bold but not italic
\def\ind#1{\mathrm{1}(#1)} % indicator function
\newcommand{\simiid}{\stackrel{iid}{\sim}} %[] IID 
\def\where{\text{ where }} % where
\newcommand{\indep}{\perp \!\!\! \perp } % independent symbols
\def\cov#1#2{\mathrm{Cov}(#1, #2)} % covariance 
\def\mrm#1{\mathrm{#1}} % remove math
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\def\t#1{\tilde{#1}} % tilde
\def\normal#1#2{\mathcal{N}(#1,#2)} % normal
\def\mbi#1{\boldsymbol{#1}} % Bold and italic (math bold italic)
\def\v#1{\mbi{#1}} % Vector notation
\def\mc#1{\mathcal{#1}} % mathical
\DeclareMathOperator*{\argmax}{arg\,max} % arg max
\DeclareMathOperator*{\argmin}{arg\,min} % arg min
\def\E{\mathbb{E}} % Expectation symbol
\def\mc#1{\mathcal{#1}}
\def\var#1{\mathrm{Var}(#1)} % Variance symbol
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} % checkmark
\newcommand\red[1]{{\color{red}#1}}
\def\bs#1{\boldsymbol{#1}}
\def\P{\mathbb{P}}
\def\var{\mathbf{Var}}
\def\naturals{\mathbb{N}}
\def\cp{\overset{p}{\to}}
\def\clt{\overset{\mathcal{L}^2}{\to}}

\newtheorem{corollary}{Corollary}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % A norm with 1 argument
\DeclareMathOperator{\Var}{Var} % Variance symbol

\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\hypersetup{
  linkcolor  = blue,
  citecolor  = blue,
  urlcolor   = blue,
  colorlinks = true,
} % color setup

% proof to proposition 
\newenvironment{proof-of-proposition}[1][{}]{\noindent{\bf
    Proof of Proposition {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}
% general proof of corollary
  \newenvironment{proof-of-corollary}[1][{}]{\noindent{\bf
    Proof of Corollary {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}
% general proof of lemma
  \newenvironment{proof-of-lemma}[1][{}]{\noindent{\bf
    Proof of Lemma {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}

\allowdisplaybreaks

\title{Prospective testing for the prevalence or transience of a shock effect over a set time horizon before it occurs}
\author{}
\date{}

\begin{document}

\maketitle

%We develop a forecasting methodology for providing credible forecasts for time series that have recently undergone a shock. We achieve this by borrowing knowledge from other time series that have undergone similar shocks for which post-shock outcomes are observed. Three shock effect estimators are motivated with the aim of minimizing average forecast risk. We propose risk-reduction propositions that provide conditions that establish when our methodology works. Bootstrap and leave-one-out cross validation procedures are provided to prospectively assess the performance of our methodology. Several simulated data examples, and two real data examples of forecasting Conoco Phillips and Apple stock price are provided for verification and illustration

\begin{abstract}
We develop a hypothesis testing procedure to prospectively test whether an anticipated shock is likely to be transient or permanent over a set time horizon. We achieve this by borrowing knowledge from other time series that have undergone similar shocks for which post-shock outcomes are observed. These additional time series form a donor pool. 
For each time series in the donor pool we calculate a p-value corresponding to a hypothesis test on the relevance of the inclusion of shock-effect information in predicting the response over the time horizon.
These p-values are then combined to form an aggregated p-value which guides one decision in determining whether the shock effect for the time series under study is expected to be prevalent or transient. This p-value can be computed before the shock-effect is observed in the time series under study provided one can form a suitable donor pool. Two real data examples for forecasting daily Conoco Phillips stock prices and monthly employment data as well as several simulations are provided for verification and illustration.
\end{abstract}


\section{Introduction}

%We provide forecasting adjustment techniques with the goal of lowering overall forecast error when the time series under study has undergone a structural shock.
In this article we provide forecasting methodology for assessing the transient or permanent behavior of an anticipated structural shock that has yet to have a measured impact on a time series under study. We focus on the setting in which a structural shock has occurred and one desires predictions to be made before post-shock responses are observed over a set time horizon $H$. Specific interest is in determining whether the shock is expected to be permanent or transient over $H$. 

This is a general problem facing many real life applications. For example, one may acquire terrible or great news about a company and desire to determine whether that news is bound to impact the stock price of that company over a relevant time period.  Companies may be interested in forecasting the demand of their products after they were involved in a brand crisis, but they only have recent sales data from pre-crises times. Policy makers, economists, and citizens alike may be interested in the expected future behavior of the unemployment rate in the upcoming quarter or year. %Policy makers, economists, and citizens alike may be interested in determining whether inflation will dissipate or persist for the foreseeable future
Standard forecasting methods may not yield any guidance on the post-shock trajectories \citep{baumeister2014real}. That being said, one may be able to construct credible forecasts in this setting \citep{lin2021minimizing}. The procedure of \cite{lin2021minimizing} works by supplementing the present forecast with past data borrowed from other time series which contain post-shock trajectories arising from materially similar structural shocks.

The method of \cite{lin2021minimizing} showed success in the nowcasting setting in which one is interested in predicting the value of the first post-shock response. As it currently stands this method is not appropriate for more nuanced forecasts involving $h$-ahead time points. In this article we develop a  hypothesis testing procedure motivated by the methodology of \cite{lin2021minimizing} and the multi-horizon forecast testing procedure of \cite{quaedvlieg2021multi} in order to test whether or not time series which incorporate shock information exhibit superior predictive performance over forecasts which do not over a set time horizon. This effectively tests whether a shock is transient or permanent over a set time horizon.


\textcolor{red}{This needs work} The core idea of our methodology is to sensibly aggregate similar past realized shock effects which arose from other time series, and then incorporate the aggregated shock effect estimator into the present forecast. Our testing method embraces ideas from 
forecast aggregation in the post-shock setting \citep{lin2021minimizing}, 
forecast comparison \citep{diebold1995comparing, quaedvlieg2021multi}, 
and p-value combination. 
It also has similarities with 
conditional forecasting \citep{baumeister2014real, kilian2017structural}, 
time series pooling using cross-sectional panel data \citep{ramaswamy1993empirical, pesaran1999pooled, hoogstrate2000pooling, baltagi2008forecasting, koop2012forecasting, liu2020forecasting},
forecasting with judgement and models \citep{svensson2005monetary, monti2008forecast}, 
synthetic control methodology \citep{abadie2010synthetic, agarwal2020two},  
expectation shocks \citep{croushore2006data, baumeister2014general, clements2019measuring}.

\vspace*{0.5cm}\noindent\textcolor{red}{Need more pointed references on similar methods. Need to discuss how this method extends \cite{lin2021minimizing} and is an application of \cite{quaedvlieg2021multi}.}



\section{Setting}

\label{section2}

We will suppose that a researcher has multivariate time series data $\y_{i,t}$, $t = 1, \ldots,  T_i$ and $i = 1, \ldots, n+1$. We let $\y_{i,t} = (y_{i,t}$, $\x_{i,t}$) where $y_{i,t}$ is a scalar response and $\x_{i,t}$ is a vector of covariates that are revealed to the analyst prior to the observation of $y_{1,t}$.  We will suppose that the analyst in interesting in forecasts over a time horizon $H$ where $H > 1$ to define an interesting problem that differs from \cite{lin2021minimizing}.
%Suppose that the analyst is interested in forecasting $y_{1,t}$, the first time series in the collection. 
We will suppose that each time series $\y_{i,t}$ undergoes a shock at time $T^*_i \leq T_i + H$, and we will suppose that $T^*_1 = T_1 + 1$ and $1 < T^*_i < T_i + H$ for $i \geq 2$. 
We will specify that $\x_{i,t=T^*_i}$ is observed before the shock takes effect on $y_{i,t=T^*_i}$.

We are interested in point forecasts $y_{i,t}^h$ at multiple horizons, $h = 1, \ldots, H$ with the aim of determining whether the shock has an effect on $y_{i,t}^h$.
\cite{quaedvlieg2021multi} provided a methodology for comparing forecasts jointly across all horizons of a forecast path, $h = 1,\ldots, H$. In our post-shock setting, we want to compare the forecasts 
$$
  \hat y^{1,h}_{i,t} \; \text{and} \; \hat y^{2,h}_{i,t}
$$
where $y^{1,h}_{i,t}$ is the forecast for $y_{i,t}$ that accounts for the yet-to-be observed structural shock and is based on the information set $\mathcal{F}_{t-h}$, and $\hat y^{2,h}_{i,t}$ is defined similarly for the forecast that does not include any shock effect information. We will compare these forecasts in terms of their loss differential
$$
  \dbf_{i,t} = \Llet_{i,t,1} - \Llet_{i,t,2},
$$
where $\Llet_{i,t,j} \in \R^H$ has elements $L^h(y_{i,t}$, $\hat y_{i,t}^{j,h})$, $j = 1,2$, and $L$ is a loss function. Hypothesis tests in \cite{quaedvlieg2021multi} are with respect to $E(\dbf_{i,t}) = \mbf\mu_{i,t}$. Conditions for these tests require conditions of \cite{giacomini2006tests}. 

We will be interested in $\mathbf{\mu}_i = \lim_{T\to\infty}\frac{1}{T}\sum_{i=1}^T \mathbf{\mu}_{i,t}$.

\vspace*{0.5cm}\noindent\textbf{Note}: We need more formality for constructing $\hat y^{1,h}_{1,t}$. We could use the forecasts in \cite{lin2021minimizing} and then consider $h$-ahead methods after adjusting for the shock. Or we could consider aggregation approaches which average all post-shock responses of the series in the donor pool. 
\vspace*{0.5cm}

We will consider the average superior predictive ability (aSPA) to assess whether or not a shock is permanent or transitory. The aSPA investigates forecast comparisons based on their weighted average loss difference
$$
  \mu^{(\text{Avg})}_i = \textbf{w}_i'\mathbf{\mu}_i = \sum_{h=1}^H w_{i,h} \mu_i^h,
$$
with weights $\textbf{w}_i$ that sum to one. Note that aSPA requires the user to take a stand on the relative importance of under-performance at one horizon against out-performance at another, and note that it is likely that $\mu_i^h > 0$ for $h$ closer to 1 since the user expects that a structural shock will occur and the structural shock is taken into account by forecast 1. 

We consider a simple test for average SPA, based on the weighted-average loss differential. The associated null is
\begin{equation} \label{test}
	H_{i,\text{aSPA}}^0: \mu^{(\text{Avg})}_i \leq 0.
\end{equation}
A studentized test statistic corresponding to the null hypothesis \eqref{test} is of the form 
\begin{equation} \label{tstat}
	t_{i,\text{aSPA}} = \frac{\sqrt{T}_i\bar{d}_{i}}{\hat{\zeta}_{i}},
\end{equation}
where $\bar{d}_{i} = \textbf{w}_i'\textbf{d}_{i}$ and we choose to estimate $\zeta_{i} = \sqrt{\textbf{w}_i'\Omega_{i}\textbf{w}_i}$ directly based on $\textbf{w}_i'\textbf{d}_{i,t}$ using the HAC estimator \citep{giacomini2006tests} where $\Omega_{i} = \text{avar}\left(\sqrt{T}_i(\bar{d}_{i} - \mbf{\mu}_{i})\right)$.


We now demonstrate that computing p-values corresponding to the hypothesis test \eqref{test} using the moving block bootstrap (MBB) is a good approximation. Let $l$ be the block length of the MBB where we assume that $T_i = lK_i$. Let $I_1$,\ldots,$I_{K_i}$ be iid random variables uniformly distributed on $\{1,\ldots,T_i-l+1\}$, and define the array 
$$
  \tau_{T_i} = \{I_1+1, \ldots, I_1+l, \ldots, I_{K_i}+1, \ldots, I_{K_i}+l\}.
$$
The pseudo time-series is therefore $\textbf{d}^b_{i,t} = \textbf{d}^b_{i,\tau_{T_i}}$, with elements $d^{hb}_{i,t}$.

\begin{equation} \label{whb}
	\left(\hat{\omega}_{i}^{hb}\right)^2 = \frac{1}{K_i}\sum_{k=1}^{K_i}\left[\frac{1}{l}\left(\sum_{t=1}^l d^{hb}_{i,(k-1)l + t} - \bar{d}^{hb}_{i}\right)^2\right],
\end{equation}
where $\bar{d}^{hb}_{i} = \frac{1}{T_i}\sum_{t=1}^{T_i} d^{hb}_{i,t}$. From the conditions of Theorem 1 and Corollary 1 in \cite{quaedvlieg2021multi}  we have 
\begin{equation} \label{boott}
  \sup_{z\in\R}\Big\vert \Prob^b\left[\sqrt{T_i}\frac{\textbf{w}_i'\bar{\dbf}^b_{i} - \textbf{w}_i'\bar{\dbf}_{i} }{\hat{\zeta}^b_{i}}\right] 
  - \Prob\left[\sqrt{T_i}\frac{\textbf{w}_i'\bar{\dbf}_{i} -  \textbf{w}_i'\mbf{\mu}_{i}}{\hat{\zeta}^b_{i}}\right]
  \Big\vert \overset{P}{\longrightarrow} 0,
\end{equation}
as $T_i\to\infty$ where $l=l_{T_i}=o(\sqrt{T_i})$. We can compute a bootstrap $p$-value as 
$$
  \hat{p}_i^B = \frac{1}{B}\sum_{b=1}^B 1\{t_{i,\text{aSPA}} < t^b_{i,\text{aSPA}}\}. 
$$
The results \eqref{boott} implies that 
\begin{equation} \label{bootp}
  |\hat p_i^B - \hat p_i| \overset{P}{\to} 0, \quad \text{as} \quad T_i,B \to \infty, 
\end{equation}
    where $\hat p_i = \Prob(t_{i,\alpha} < t_{i,\text{aSPA}})$ and $t_{i,\alpha}$ is a critical value corresponding to the distribution of $t_{i,\text{aSPA}}$. Therefore bootstrap inference in the donor pool is a good approximation. %We now consider how bootstrap inference in the donor pool is used to test whether a shock is prevalent or transient for the time series under study.





\subsection{Model setup}
\label{modelsetup}

We now describe the assumed autoregressive models with random effects for which post-shock aggregated estimators are provided. The model $\mc{M}$ is defined as
\begin{align}
\mc{M} \colon \begin{array}{l}
  y_{i,t} = \left(\eta_i + \sum_{j=1}^{q_1}\phi_{i,j}y_{i, t-j} + \sum_{j=0}^{q_2-1}\theta_{i,j+1}'\mbf{x}_{i,t-j}\right)(1 - D_{i,t}) + f(\mathcal{F}_{i,t},\alpha_i)D_{i,t} + \varepsilon_{i,t},\\[.2cm]
  \; f(\mc{F}_{i,t},\alpha_i) = \alpha_i +  \sum_{j=1}^{q_{1}}\t\phi_{i,j}y_{i, t-j} + \sum_{j=0}^{q_2-1}\t\theta_{i,j+1}'\mbf{x}_{i,t-j}, \\[.2cm]
  \; \alpha_i = \mu_{\alpha} + \delta_i'\mbf{x}_{i, T_i^*+1} + \varepsilon_{\alpha, i},
  %\sum_{j=1}^{q_3}\delta_{i,j}'\mbf{x}_{i, T_i^*-j+1}+ \t{\varepsilon}_{i}, 
\end{array}\label{model-gen}
\end{align}
where $D_{i,t} = I(t \geq T_i^* + 1)$, $I(\cdot)$ is the indicator function, $\x_{i,t} \in \R^{p}$ are fixed with $p \geq 1$. Let 
$\mathbf{\phi}_i = (\phi_{i,1},\ldots,\phi_{i,q_1})'$, 
$\mathbf{\theta}_i = (\theta_{i,1},\ldots,\theta_{i,q_2})'$, 
$\tilde{\mathbf{\phi}}_i = (\t\phi_{i,1},\ldots,\t\phi_{i,q_{1}})'$, 
$\tilde{\mathbf{\theta}}_i = (\t\theta_{i,1},\ldots,\t\theta_{i, q_{2}})'$, 
$\mathbf{\delta}_i = (\delta_{i,1},\ldots,\delta_{i,p})'$, 
and suppose that the regression coefficients in \eqref{model-gen} have the following hierarchical random effects structure: 
\begin{equation} \label{random-effects}
\begin{split}
  \eta_i &\simiid \mc{F}_{\eta} \text{ with }  \; \mrm{E}_{\mc{F}_{\eta}}(\eta_i) = \mu_\eta, \mrm{Var}_{\mc{F}_{\eta}}(\eta_i)  = \sigma^2_{\eta}, \\
  \mathbf{\phi}_{i,j} &\simiid \mc{F}_{\mathbf{\phi}_j} \text{ where } |\phi_{i,j}| < 1, \\
  \mathbf{\theta}_{i,j} &\simiid \mc{F}_{\mathbf{\theta}_j} \text{ with }  \; \mrm{E}_{\mc{F}_{\mathbf{\theta}_j}}(\theta_{i,j}) = \mu_{\mathbf{\theta}_j}, \mrm{Var}_{\mc{F}_{\mathbf{\theta}_j}}(\mathbf{\theta}_{i,j})  = \Sigma^2_{\mathbf{\theta}_j}, \\
  \mathbf{\delta}_i &\simiid  \mc{F}_{\mathbf{\delta}} \text{ with } \mrm{E}_{\mc{F}_{\mathbf{\delta}}}(\mathbf{\delta}_i)=\mu_{\mathbf{\delta}}, \mrm{Var}_{\mc{F}_{\mathbf{\delta}}}(\mathbf{\delta}_i)=\Sigma_{\mathbf{\delta}}, \\
\varepsilon_{i,t} & \simiid  \mc{F}_{\varepsilon_i} \text{ with }  \; \mrm{E}_{\mc{F}_{\varepsilon_i}}(\varepsilon_{i,t}) = 0, \mrm{Var}_{\mc{F}_{\varepsilon_i}}(\varepsilon_{i,t})  = \sigma^2_i,  \\
\varepsilon_{\alpha, i} &\simiid  \mc{F}_{\varepsilon_\alpha} \text{ with }\mrm{E}_{\mc{F}_{\varepsilon_\alpha}}(\varepsilon_{\alpha, i})=0, \mrm{Var}_{\mc{F}_{\varepsilon_{\alpha}}}(\varepsilon_{\alpha, i})=\sigma^2_{\alpha}, \\
  \tilde{\mathbf{\phi}}_{i,j} &\overset{ind}{\sim} \mathcal{F}_{\tilde{\mathbf{\phi}_j}}(\x_{i,T_i^*+1}) \; \text{where} \; |\tilde{\mathbf{\phi}}_{i,j}| < 1,  \\
  \tilde{\mathbf{\theta}}_{i,j} &\overset{ind}{\sim} \mathcal{F}_{\tilde{\mathbf{\theta}_j}}(\x_{i,T_i^*+1}),  \\  
 % \t q_{i,k} &\overset{ind}{\sim} \mathcal{F}_k(\x_{i,T_i^*}), \; k = 1,2, \\
&\eta_i \indep \mathbf{\phi}_{i,j} \indep \mathbf{\theta}_{i,j} \indep 
\tilde{\mathbf{\phi}}_{i,j} \indep \lambda_{i,j} \indep \mathbf{\delta}_i \indep \varepsilon_{i,t} \indep \tilde\varepsilon_{i}, 
\end{split}
\end{equation}
where the distributions $\mathcal{F}_{\tilde{\mathbf{\phi}}_j}(\x_{i,T_i^*+1})$ and $\mathcal{F}_{\tilde{\mathbf{\theta}}_{i'}}(\x_{i,T_i^*+1})$ satisfy $\mathcal{F}_{\tilde{\mathbf{\phi}}_j}(\x_{i,T_i^*+1}) \overset{d}{=} \mathcal{F}_{\tilde{\mathbf{\phi}}_{j}}(\x_{i',T_i^*+1})$ and $\mathcal{F}_{\tilde{\mathbf{\theta}}_j}(\x_{i,T_i^*+1}) \overset{d}{=} \mathcal{F}_{\tilde{\mathbf{\theta}}_{j}}(\x_{i',T_i^*+1})$ when $\x_{i,T_i^*+1} = \x_{i',T_i^*+1}$. Note that for model \eqref{model-gen} to be of use for post-shock forecasting, the variation in $\mathcal{F}_{\tilde{\mathbf{\phi}}}$ and $\mathcal{F}_{\tilde{\mathbf{\theta}}}$ must be small relative to the signal strength.


We see that model \eqref{model-gen} with its accompanying random effects structure \eqref{random-effects} is flexible enough to capture changing structural dynamics as well as a mean-shift, and is a generalization of the model in \cite{lin2021minimizing}. Note that the flexibility of allowing for changing structural dynamics is useful for fitting purposes, but it does not necessarily reflect the state of the world in the post-shock setting. It is of course possible that there is no shock or a simple mean-shift. 

The post-shock dynamic changes depend heavily on the value of $\mbf{x}_{i,T_i^*+1}$, the covariates recorded right before the first post-shock response is observed. Under this setup any two series $i,j$ with small $\|\mbf{x}_{i,T_i^*+1} - \mbf{x}_{j,T_i^*+1}\|_2$ are expected to experience similar structural changes. This makes distance based weighting an attractive avenue.



%We further define the parameter sets
%\begin{align}
%\begin{array}{l}
% \;\, \Theta = \{(\eta_i, \mathbf{\phi}_i, \mathbf{\theta}_i, \mathbf{\delta}_i, \alpha_i, \mbf{x}_{i,t}, y_{i,t-1})\colon    t= 1, \ldots, T_i, i = 2, \ldots, n +1\} \\
%   \Theta_1 = \{(\eta_i, \mathbf{\phi}_i, \mathbf{\theta}_i, \mathbf{\delta}_i, \alpha_i, \mbf{x}_{i,t}, y_{i,t-1})\colon  t= 1, \ldots, T_i, i = 1\}
%\end{array}\label{parameter} 
%\end{align}







\subsection{Forecasting and testing for shock persistence}
\label{forecast}


In this section, we motivates the relation between shock persistence testing and forecasting comparison. In time series literature, under a squared error loss $L(\cdot)$, the forecast using the conditional mean is the best point forecast that minimizes the expected loss if the model is specified correctly. That being said, even if the time series does experience a shock but the shock is not persistent, the expected loss of the conditional mean forecast should converge to the minimum expected loss that any forecast can achieve as sample size increases. On the other hand, if the shock is persistent, the conditional mean forecast that incorporates the shock information and makes good use of it should enjoy a smaller expected loss than any conditional mean forecast without such information does. In other words, the problem of testing for shock persistence boils down into comparing the conditional mean forecast that is adjusted by shock information and the one that is unadjusted. To deal with this problem, we applied the forecast comparison procedure of  \cite{quaedvlieg2021multi} to conduct shock persistence testing.

In this paper, we extend the framework of post-shock forecast \citep{lin2021minimizing}, which aggregates outside information by constructing a donor pool, to post-shock persistence testing. The donor pool consists of time series called  ``donors'' that are similar to the time series of interest. As we do not observe the future data, information from donors can aid our decision making and forecasts. Note that what  similarity exactly means here can be vague but the donors and the time series of interest should come from a common population in principle; see more details of how to select donors in \cite{lin2021minimizing}. In this paper, we also proposed several theoretical results with some detailed conditions on the donor pool, which may alleviate the difficulty of dealing with similarity when choosing donors.


In this paper, we propose a new way of aggregating information from donor pool to conduct post-shock persistence testing, which is quite different from \cite{lin2021minimizing}. The general structure of this aggregation technique is illustrated as below. Suppose we are given a nonempty donor pool with size $n$ that consists of donors $\mc{T}_2, \ldots, \mc{T}_{n+1}$, whose data, including shock information, are observed. With shock information, we are able to compute unadjusted forecast that does not incorporate shock information and adjusted forecast that does.  The forecast comparison using methodology of \cite{quaedvlieg2021multi} for each donor yields a $p$-value, $p_{i+1}$ for $i$th donor in the donor pool. Given $\{p_i \colon i = 2, \ldots, n+1\}$, we propose a methodology for judging whether the shock of the time series of interest is persistent or not. Next, we illustrate the details of our methodology as below.



For each donor $\mc{T}_i$, $i = 2, \ldots, n+1$, to compare adjusted and unadjusted forecasts,  a model needs to be specified. The model should describe how the shock impacts the time series at and after a certain time point $t= T_i^* + 1$. For clarity, we refers the unadjusted forecast to the conditional mean forecast based on the part of the model that ignores shock effect; and the adjusted forecast refers to the conditional mean forecast based on the full model. Note that the model needs not to be the same for every donor in the donor pool. However, certain conditions need to be imposed as discussed previously. 

 Given a model for each donor pool, we compute the unadjusted conditional mean forecast and adjusted conditional mean forecast in the framework of \cite{quaedvlieg2021multi}. As introduced in the beginning of Section \ref{section2}, for a given horizon $h$, to forecast $y_{i, t}$, we compute the two forecasts based on the information set $\mathcal{F}_{i, t-h}$ of each donor pool (i.e., the data before $t-h$) for $i = 2, \ldots, n+1$. Note that in practice, a training sample size $K_i$ should be specified for $i = 2, \ldots, n+1$ such that we use the information set $\mathcal{F}_{i, t-h-K_i + 1:t-h}$. 
 
 Even if the data contain shock information for each donor, it is impossible to compute the adjusted conditional mean forecast based on $\mathcal{F}_{i, t-h}$ when $t -h \leq T_i^*$, i.e., we only have training data that do no have any shock information. Note that this is not a big problem when $t-h \geq T_i^* +1$ because  $\mathcal{F}_{i, t-h}$ starts to incorporate shock information. In the literature, \cite{lin2021minimizing} utilizes donor pool information to provide an consistent estimator of the shock effect such that the adjusted  conditional mean forecast is the unadjusted one plus the shock effect estimate. However, the consistent estimation comes with the cost of assuming certain models (in terms of first and second moments) for the shock effect across the donor pool and the method only works for additive shock effect model.  To our best knowledge, there  is no other available methods that can accommodate the case when the shock model is nonlinear. However, even if the model is misspecified, using additive shock effect model to approximate the scalar shock effect that occurs at $t=T_i^*+1$ may still help as opposed to doing nothing. 
 
 With the presence of autoregressive parameters in some models, unbiased estimation with OLS  is not feasible but the bias is $O(T^{-1})$, where $T$ is the time series length \citep{shaman1988bias}. In this paper, we use the OLS estimates to approximate the adjusted conditional mean forecast and unadjusted one. The bias decades away as $T\to \infty$.

\vspace*{0.5cm}

\noindent\textbf{Note}: Our forecast needs to be written with respect to our general model. Specifics can be given when we conduct our numerical examples.
\vspace*{0.5cm}

In our post-shock setting we consider the following candidate forecasts: 
\begin{align*}
  &\text{Forecast 1}: \\
  &\text{Forecast 2}: 
\end{align*}
We want to determine which forecast is appropriate over a horizon while the methods in \cite{lin2021minimizing} were only appropriate in the nowcasting setting in which prediction was only focused on the response immediately following the shock.





\newpage

\noindent\textbf{Note}: We need to explain what the voting method is, possibly in algorithmic format. Also note that the p-values that we obtain are computed using a bootstrap procedure. Perhaps an additional proposition that states the performance of these bootstrap p-values would further guarantee reliability. 

\begin{prop}

\label{votprop}Suppose $p_2, \ldots, p_{n+1}$ is  a sequence of pairwise independent $p$-values  with $\P(p_i \leq \alpha)=\kappa$ for $i = 1, \ldots, n+1$, where $\alpha$ is the significance level, $\kappa$ is a real-valued constant in $[0,1]$, and  $p_1$ is the p-value of the time series of interest. If $\P(p_1 \leq \alpha) \neq 0.5$,  as $n\to \infty$, 
\begin{align*}
  \E \bigg\{\left|I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \right|\bigg\}
  \to  \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases}
\end{align*}
\end{prop}

\begin{proof}
Notice that as $\P(p_i \leq \alpha)=\kappa$ for $i = 1, \ldots, n+1$ and $I(p_i \leq  \alpha) $ is a Bernoulli random variable, $I(p_i \leq  \alpha)$ is identically distributed for  $i = 1, \ldots, n+1$. Since $p_i$ is pairwise independent and $p_i$ are identically distributed for $i = 2, \ldots, n+1$, by Weak Law of Large Numbers, 
\begin{align*}
  \frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha)  
   \stackrel{p}{\rightarrow}  \P(p_i \leq \alpha) = \P(p_1 \leq \alpha),
\end{align*}
Define
\begin{align*}
  f \colon [0,1] \mapsto \{0,1\} 
  \text{ with }
  f(x) = I(x \geq 0.5).
\end{align*}
Let $C(f)$ denote the continuity set of $f$. Suppose that $\P(p_1 \leq  \alpha) \neq 0.5$. In this case, notice that 
\begin{align*}
  \P(\P(p_1 \leq  \alpha) \in C(f)) =1.
\end{align*}
By Slutsky's Theorem, we have
\begin{align*}
  I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  \stackrel{p}{\rightarrow} 
  I\{ \P(p_1 \leq  \alpha) \geq 0.5\}.
\end{align*}
It follows that
\begin{align*}
   I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}- I( p_1 \leq  \alpha) 
  \stackrel{p}{\rightarrow} 
  I\{ \P(p_1 \leq  \alpha) \geq 0.5\} - I( p_1 \leq  \alpha) 
\end{align*}
Since the function $g(x)= |x|$ is continuous in $x$,  by continuous mapping theorem, 
\begin{align*}
  \bigg|  I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \bigg| \stackrel{p}{\rightarrow}   |I\{ \P(p_1 \leq  \alpha) \geq 0.5\}
  - I( p_1 \leq  \alpha) |
\end{align*}
Moreover, note that
\begin{align*}
\E \left\{ |I\{ \P(p_1 \leq  \alpha) \geq 0.5\}
  - I( p_1 \leq  \alpha) |\right\}
  & = \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases} \\
  & \leq  0.5.
\end{align*}
Due to the fact that
\begin{align*}
 \left|I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \right| \leq 1
\end{align*}
is bounded by 1, by Dominated Convergence Theorem for the version of convergence in measure,
\begin{align*}
 \left|I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \right|
  \stackrel{\mathcal{L}_1}{\rightarrow}  |I\{ \P(p_1 \leq  \alpha) \geq 0.5\}
  - I( p_1 \leq  \alpha) |.
\end{align*}
That would imply that 
\begin{align*}
  \E \bigg\{\left|I\left\{\frac{1}{n}\sum_{i=2}^{n+1}I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \right|\bigg\}
  \to  \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases}
\end{align*}
That is, the expected misclassification rate of voting converges to 
\begin{align*}
  \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases}
\end{align*}
\end{proof}

\begin{lem}
  \label{gwlln}  For each $n\in \naturals$, let $c_{n1}, \ldots, c_{nn}$ be real numbers bounded by some $K > 0$, and let $X_{n1}, \ldots, X_{nn}$ be pairwise independent random variables defined on a probability space $(\Omega_n, \mc{F}_n, \P_n)$, and let $\E_n$ and $\var_n$ denote the corresponding expectation and variance. If $(b_n)$ is a sequence of positive numbers such that $b_n \uparrow \infty$ such that
    \begin{align*}
      \sum_{i=1}^n \P_n (|X_{ni}| > b_n) \to 0
      \quad \text{ and } 
      \quad \frac{1}{(b_n)^2} \sum_{i=1}^n \E_n[X_{ni}^2; |X_{ni}|\leq b_n] \to 0,
    \end{align*}
    then
    \begin{align*}
      \frac{1}{b_n} \sum_{i=1}^n c_{ni} (X_{ni} - \E_n[X_{ni}; X_{ni} \leq b_n]) \cp 0.
    \end{align*}
\end{lem}

\begin{proof}
  For each $n\in \naturals$ set
    \begin{align*}
      S_n = \sum_{i=1}^n c_{ni} X_{ni}, \quad T_n = \sum_{i=1}^n c_{ni} Y_{ni},
      \quad Y_{ni} = X_{ni} I(|X_{ni}|\leq b_n), \quad i = 1, \ldots, n.
    \end{align*}
   The goal is to show that  $(S_n- \E_n[T_n])/b_n \cp 0$. For this, it suffices to show that (1) $(T_n - S_n)/b_n \cp 0$ and (2) $(T_n - \E_n[T_n])/b_n \cp 0$ because
    \begin{align*}
      (T_n - \E_n[T_n])/b_n -(T_n - S_n)/b_n = (S_n- \E_n[T_n])/b_n .
    \end{align*}
     We first prove (1). Notice that for every $\varepsilon > 0$ and $n\in \naturals$,
    \begin{align*}
      \{|S_n- T_n| > \varepsilon\}\subseteq \bigcup_{i=1}^n \{X_{ni}\neq Y_{ni}\}
      = \bigcup_{i=1}^n \{|X_{ni}|>b_n\},
    \end{align*}
    and, as a result, by Boole's inequality we have
    \begin{align*}
      \P_n (|S_n - T_n|>\varepsilon) \leq \sum_{i=1}^n \P_n(|X_{ni}|>b_n) \to 0,
    \end{align*}
    which proves (1). Next, we prove (2). Since $\mc{L}^2$ implies convergence in probability, it suffices to show that $(T_n-\E[T_n])/b_n \clt 0$, i.e., $\var[T_n]/(b_n)^2 \to 0$. Since $X_{n1}, \ldots, X_{nn}$ are pairwise independent, so are $Y_{n1}, \ldots, Y_{nn}$, and consequently
    \begin{align*}
      \Var_n[T_n]=\sum_{i=1}^n \Var_n[Y_{ni}] \leq \sum_{i=1}^n \E_n[(c_{ni}X_{ni})^2; |X_{ni}|\leq b_n] \leq K^2\sum_{i=1}^n \E_n[(X_{ni})^2; |X_{ni}|\leq b_n].
    \end{align*}
    As a result,
    \begin{align*}
    0\leq   \frac{\Var_n[T_n]}{b_n^2}\leq K^2 \cdot  \frac{1}{b_n^2}\sum_{i=1}^n \E_n[(X_{ni})^2; |X_{ni}|\leq b_n] \to 0,
    \end{align*}
    which proves the result by sandwich theorem.
\end{proof}


\begin{corollary}
\label{coro1}Let $(w_2, \ldots, w_{n+1})$ be weights such that $w_i \in [0,1]$ and $\sum_{i=2}^{n+1} w_i=1$. Define
\begin{align*}
  \mc{I}_n = \{i =  2,\ldots, n+1\colon 0< w_i < 1\}.
\end{align*}
Suppose that  $\mc{I}_n$  is non-empty, $|\mc{I}_n| \to \infty$ as $n \to \infty$, and $w_i b_n\leq K$ for $i\in \mc{I}_n$ and some $K>0$, where $b_n \geq 1$ and $b_n \to \infty$ as $n\to \infty$. Assume for $i \in \mc{I}_n$,  $p_i$ are pairwise independent $p$-values  with 
\begin{align*}
  \sum_{i\in \mc{I}_n}\P(p_i \leq \alpha) = \sum_{i\in \mc{I}_n} w_i \kappa_i \to \kappa_{1},
\end{align*}
where $\kappa_i =  \P(p_i \leq \alpha)\in [0,1]$ for $i \in \mc{I}_n$, $\alpha$ is the significance level, $\kappa_1 = \P(p_1 \leq \alpha)$, and  $p_1$ is the p-value of the time series of interest. If $\P(p_1 \leq \alpha) \neq 0.5$, as $n\to \infty$, 
\begin{align*}
  \E \bigg\{\left|I\left\{ \sum_{i\in \mc{I}_n} w_i I(p_i \leq  \alpha) \geq 0.5\right\}
  - I( p_1 \leq  \alpha) \right|\bigg\}
  \to  \begin{cases}
    1- \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) > 0.5 \\
    \P(p_1 \leq \alpha) & \text{ if } \P(p_1 \leq \alpha) < 0.5 
  \end{cases}
\end{align*}
\end{corollary}

\begin{proof}
  The proof is rather similar to Proposition \ref{votprop}. It suffices to show that
  \begin{align*}
   \sum_{i\in \mc{I}_n} w_i I(p_i \leq  \alpha) \cp \kappa_1 =  \P(p_1 \leq \alpha).
  \end{align*}
  and the remaining proof is the same as that of Proposition \ref{votprop} in terms of applying Dominated Convergence Theorem in the version of  convergence in probability. As $p_i$ are pairwise independent for $i\in \mc{I}_n$, $I(p_i\leq \alpha)$ are pairwise independent for $i \in \mc{I}_n$. The idea is to prove the required condition of Lemma \ref{gwlln} holds. As  $\mc{I}_n$  is non-empty, $|\mc{I}_n| \to \infty$, $b_n > 1$, and  $b_n \to \infty$ as $n \to \infty$, 
  \begin{align*}
    & \sum_{i\in \mc{I}_n} \P\big( I(p_i \leq  \alpha) > b_n\big) \to 0\\
   & \frac{1}{b_n^2} \sum_{i\in \mc{I}_n} \E\big[ I(p_i \leq  \alpha) \,|\, I(p_i \leq  \alpha) \leq b_n\big]
     =  \frac{1}{b_n^2} \sum_{i\in \mc{I}_n} \P\big(p_i \leq\alpha \big)
     = \frac{1}{b_n^2} \sum_{i\in \mc{I}_n} w_i \kappa_i  \to 0
  \end{align*}
  because $ \sum_{i\in \mc{I}_n} w_i \kappa_i = O(1)$ by the assumption $\sum_{i\in \mc{I}_n} w_i \kappa_i \to \kappa_{1}$. Let $c_{ni} = w_i b_n$ for $i \in \mc{I}_n$. Since $w_i b_n\leq K$ for some $K > 0$ and $b_n$, we have 
  \begin{align*}
    & \frac{1}{b_n} \sum_{i\in \mc{I}_n}  c_{ni} \big(  I(p_i \leq  \alpha) - \P( I(p_i \leq  \alpha) \, |\,  I(p_i \leq  \alpha)  \leq b_n )  \big) \\
  = \quad     &\frac{1}{b_n} \sum_{i\in \mc{I}_n}  c_{ni} \big(  I(p_i \leq  \alpha) - \P( I(p_i \leq  \alpha) \big)  \cp 0
  \end{align*}
  which follows from Lemma \ref{gwlln}. The above is equivalent to
  \begin{align*}
   \sum_{i\in \mc{I}_n}  w_i I(p_i \leq  \alpha) - \sum_{i\in \mc{I}_n} w_i  \kappa_i  \cp 0
  \end{align*}
  As $\sum_{i\in \mc{I}_n} w_i  \kappa_i\to \kappa_1$ as $n \to \infty$, by Slutsky's Theorem,
  \begin{align*}
     \sum_{i\in \mc{I}_n}  w_i I(p_i \leq  \alpha)  \cp \kappa_1,
  \end{align*}
  which finishes the proof.
\end{proof}


Note that asymptotic approximation for the misclassification error does not work when the weight is concentrated, i.e., $\mc{I}_n$ is finite even if $n\to \infty$, where $\mc{I}_n = \{i =  2,\ldots, n+1\colon 0< w_i < 1\}$. The intuition is that  it is impossible to find a weight to constrain the random behavior of the Bernoulli random variables $I(p_i\leq \alpha)$ for $i \in \mc{I}_n$ such that the weak law of large number can apply.  Nevertheless, in this case, if $\kappa_i = \P(p_i\leq \alpha)$ for $i\in \mc{I}_n$ is small, the approximation for the misclassification in Corollary \ref{coro1} is favorable. This situation occurs very frequently in practice. It can be shown as below.

Note that the mean squared error of estimating $\P(p_1\leq \alpha)=\kappa_1$ is
\begin{align*}
	\E\bigg\{ \Big\{ \sum_{i\in \mc{I}_n} w_i I(p_i\leq \alpha)-\kappa_1 \Big\}^2 \bigg\}
	&= \Big\{ \sum_{i\in \mc{I}_n} w_i \kappa_i -\kappa_1 \Big\}^2 + \var 
	\Big\{\sum_{i\in \mc{I}_n} w_i I(p_i\leq \alpha) \Big\}\\
	&= \Big\{ \sum_{i\in \mc{I}_n} w_i \kappa_i -\kappa_1 \Big\}^2  + \sum_{i\in \mc{I}_n} w_i^2 \kappa_i (1-\kappa_i) \\
	& \overset{n\to \infty }{\to} \sum_{i\in \mc{I}_n} w_i^2 \kappa_i (1-\kappa_i) ,
\end{align*}
where the last second step assumes that $\{I(p_i\leq \alpha)\colon i\in \mc{I}_n\}$ are independent and the last step uses the condition $\sum_{i\in \mc{I}_n} w_i \kappa_i \to \kappa_1$ as $n\to \infty$, which is used in Corollary \ref{coro1}. Notice that if $\kappa_i$ is very small, the mean squared error of $\sum_{i\in \mc{I}_n} w_i I(p_i\leq \alpha)$ turns out to be small such that by the rationale of the proof of Proposition \ref{votprop}, the asymptotic approximation for the misclassification error in  Corollary \ref{coro1} should be acceptable.\\


\newpage



Suppose we are interested in comparing two models $\mc{M}_1$ and $\mc{M}_2$ using AIC for forecasting the response in the time series of interest. However, direct comparison is not feasible as the data are not observed. Instead, we can conduct such inference based on the donor pool. Let the AIC of $\mc{M}_j$ for $i$th time series be $a_i^j$ for $i = 1, \ldots, n+1$ and $j = 1, 2$. Let $\xi_i = \P(a_i^1 < a_i^2)$. Note that if the data generating process is based on $\mc{M}_1$ and is true for all the donors and time series of interest, $\xi_i$ is expected to be high. Additionally, it is reasonable to assume $\xi_i =\xi $ for $\xi \in [0,1]$ and $i = 1, \ldots, n+1$.  Notice that this setup is similar to the setup of weighted voting or voting we discussed before. So, under similar arguments, we propose the following corollary for synthetic pairwise model selection.


\begin{corollary}[Synthetic pairwise model selection]
\label{coro2}Let $(w_2, \ldots, w_{n+1})$ be weights such that $w_i \in [0,1]$ and $\sum_{i=2}^{n+1} w_i=1$. Define
\begin{align*}
  \mc{I}_n = \{i =  2,\ldots, n+1\colon 0< w_i < 1\}.
\end{align*}
Suppose that  $\mc{I}_n$  is non-empty, $|\mc{I}_n| \to \infty$ as $n \to \infty$, and $w_i b_n\leq K$ for $i\in \mc{I}_n$ and some $K>0$, where $b_n \geq 1$ and $b_n \to \infty$ as $n\to \infty$. Assume for $i \in \mc{I}_n$,  $\{a_i^1, a_i^2\}$ are AICs of $\mc{M}_1$ and  $\mc{M}_2$ for $i$th time series, which are pairwise independent of $\{a_j^1, a_j^2\}$ for $j \neq i$ and that
\begin{align*}
  \sum_{i\in \mc{I}_n}\P(a_i^1 < a_i^2) = \sum_{i\in \mc{I}_n} w_i \xi_i \to \xi_{1},
\end{align*}
where $\xi_i =  \P(a_i^1 < a_i^2)\in [0,1]$ for $i \in \mc{I}_n$. If $\xi  \neq 0.5$, as $n\to \infty$, 
\begin{align*}
  \E \bigg\{\left|I\left\{ \sum_{i\in \mc{I}_n} w_i I(a_i^1 < a_i^2) \geq 0.5\right\}
  - I( a_1^1 < a_1^2) \right|\bigg\}
  \to  \begin{cases}
    1- \xi_1 & \text{ if } \xi_1 > 0.5 \\
   \xi_1 & \text{ if } \xi_1 < 0.5 
  \end{cases}
\end{align*}
\end{corollary}

Note that Corollary \ref{coro2} gives an asymptotic upper bound for the misclassification rate of synthetic pairwise model selection, which is small if $\xi_1$ is large.


\newpage

\vspace*{0.5cm} We now demonstrate an idealized theoretical setting for which our post-shock testing methodology involving a distance based weighted average of p-values works well. The idea is that inferences based on distance based weighted nearly recovers the time series under study when there exists a time series in the donor pool, indexed by $i'$, such that $\x_{1,t} \approx \x_{i',t}$ and the signal strength in our modeling setup \eqref{model-gen} and \eqref{random-effects} is large relative to variation. We first present some intermediary technical results and definitions. Let $\lambda_{\text{max},\theta_k}$, $\lambda_{\text{max},\theta_\delta}$, $\lambda_{\text{max},\gamma}$ respectively be the largest eigenvalues of $\Sigma_{\theta_k}$, $\Sigma_{\delta}$, and $\Sigma_{\gamma}$ for $k = 1,\ldots,q_2$. Define  
\begin{align*}
  \mu_{\min} &= \min\left(\mu_\eta,\mu_{\phi_j},\mu_{\tilde\phi_j},\min(\mu_\delta),\min(\mu_{\theta_k}),\min(\mu_{\tilde\theta_k}); j = 1,\ldots,q_1, k = 1,\ldots,q_2\right), \\
  \mu_{\max} &= \max\left(\mu_\eta,\mu_{\phi_j},\mu_{\tilde\phi_j},\max(\mu_\delta),\max(\mu_{\theta_k}),\max(\mu_{\tilde\theta_k}); j = 1,\ldots,q_1, k = 1,\ldots,q_2\right), \\
  \lambda_{\max} &= \max\left(\lambda_{\text{max},\theta_k}, \lambda_{\text{max},\theta_\delta}, \lambda_{\text{max},\gamma}; k = 1,\ldots,q_2\right), \\
  \sigma^2_{\max} &= \max\left(\sigma^2_{\eta},\sigma^2_{\phi_j},\sigma^2_i,\sigma^2_{\alpha},\sigma^2_{\tilde{\phi}_j}; i = 1,\ldots,n+1, j = 1,\ldots,q_1\right).  
\end{align*}



\begin{prop} \label{prop:signaltonoise}
	Let $\bf{y}_{i,t}$, $i = 1,\ldots,n_i$, $t = 1,\ldots, T$ be a collection of independent time series. Let each time series $\bf{y}_{i,t}$ be generated by model \eqref{model-gen} with accompanying random effects structure \eqref{random-effects}, where $y_{i,t}$ is a scalar response $\x_{i,t} \in \R^p$ is a fixed vector of covariates. Suppose that there is a common shock time $p + 2 < T^* < T - (p+2)$ where $q_1 = q_2 = 1$, $\|\x_{i,t}\| = 1$, each entry of $\x_{i,t}$ is non-negative, $\mu_{\min} > 0$, and $y_{i,0} = 0$. Then for every $1 \leq t \leq T$ we have that 
	\begin{equation} \label{signal-to-noise}
	  \E(y_{i,t}) \geq C\Var(y_{i,t}),	
	\end{equation}
	provided that 
	\begin{equation} \label{condition-1}
	\begin{split}
		\mu_\alpha + 3\mu_{\min} &\geq C\left(3\sigma^2_{\max} + 2\lambda_{\max}\right), \\
		2\mu_{\min} &\geq C\left(2\sigma^2_{\max} + \lambda_{\max}\right).	
	\end{split}
	\end{equation}
\end{prop}


\textbf{Remarks}:
\begin{enumerate}
	\item The conditions \eqref{condition-1} govern the signal strength necessary in the expected behavior of the underlying random effects structure relative to variability necessary for the overall signal to noise ratio for the overall time series to be lower bounded by some positive value $C$.
	\item The conditions of Proposition~\ref{prop:signaltonoise} state that there is an increasing trend in $y_{i,t}$ for all $i$. This can be relaxed, but a fully general argument is cumbersome.
\end{enumerate}



\begin{proof}
For $t > T^* + 1$ we have 
\begin{align*}
  \E(y_{i,t}) &= \E\left[\E(y_{i,t}|y_{i,t-1})\right] \\ 
    &= \E\left[\E(\eta_i + \alpha_i + \tilde\phi_i y_{i,t-1} 
      + \tilde\theta_i'\x_{i,t}|y_{i,t-1})\right] \\
    &= \E(\eta_i + \mu_\alpha + \delta_i'\x_{i,T^*+1} + \tilde\phi_i y_{i,t-1} + \tilde\theta_i'\x_{i,t}) \\
    &= \mu_\eta + \mu_\alpha + \mu_\delta'\x_{i,T^*+1} + \mu_{\tilde\theta}'\x_{i,t} + \mu_{\tilde\phi}\E(y_{i,t-1}) \\
    &\geq (\mu_\alpha + 3\mu_{\min}) + \mu_{\min}\E(y_{i,t-1}) \\
    &\geq \sum_{j=1}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\E(y_{i,T^*}) \\
    &= \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\E\left[\E(y_{i,T^*}|y_{i,T^*-1})\right]\right) \\   
    &= \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\E(\eta_i + \theta_i'\x_{i,T^*} + \phi_i y_{i,T^*-1}) \right) \\
    &\geq \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(2\mu_{\min} + \mu_{\min} \E(y_{i,T^*-1}) \right) \\
    &\geq \sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\sum_{j=0}^{T^*-1}\mu_{\min}^j(2\mu_{\min}) \right), 
\end{align*}
and 
\begin{align*}
  \Var(y_{i,t})	&= \E\left[\Var(y_{i,t}|y_{i,t-1})\right] + \Var\left[\E(y_{i,t}|y_{i,t-1})\right] \\
  &= \E\left[\Var(\eta_i + \mu_\alpha + \delta_i'\x_{i,T^*+1} + \tilde\theta_i'\x_{i,t} + \tilde\phi_i y_{i,t-1} + \varepsilon_{i,t} + \varepsilon_{\alpha,i}|y_{i,t-1})\right] \\
  &+ \Var\left[\E(\eta_i + \mu_\alpha + \delta_i'\x_{i,T^*+1} + \tilde\theta_i'\x_{i,t} + \tilde\phi_i y_{i,t-1} + \varepsilon_{i,t} + \varepsilon_{\alpha,i}|y_{i,t-1})\right] \\
  &\leq 3\sigma^2_{\max} + 2\lambda_{\max} + \Var\left[\E(\tilde\phi_i y_{i,t-1}|y_{i,t-1})\right] \\
  &= 3\sigma^2_{\max} + 2\lambda_{\max} + \mu_{\tilde\phi}^2\Var(y_{i,t-1}) \\
  &\leq \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\Var(y_{i,T^*}) \\
  &= \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\E\left[\Var(y_{i,T^*}|y_{i,T^*-1})\right] + \Var\left[\E(y_{i,T^*}|y_{i,T^*-1})\right]\right) \\
  &= \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\E\left[\Var(\eta_i + \theta_i'\x_{i,T^*} + \phi_iy_{i,T^*-1} + \varepsilon_{i,T^*}|y_{i,T^*-1})\right] \right. \\ 
  &\qquad+  \left.\Var\left[\E(\eta_i + \theta_i'\x_{i,T^*} + \phi_iy_{i,T^*-1} + \varepsilon_{i,T^*}|y_{i,T^*-1})\right]\right)  \\
  &\leq \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(2\sigma^2_{\max} + \lambda_{\max} + \mu_{\phi}^2\Var(y_{i,T^*-1}) \right) \\ 
  &\leq \sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right).  
\end{align*}
Combining the above with the condition \eqref{condition-1} yields 
\begin{align*}
  \frac{\E(y_{i,t})}{\Var(y_{i,t})} &\geq \frac{\sum_{j=0}^{t - (T^* + 1)} \mu_{\min}^j(\mu_{\alpha} + 3\mu_{\min}) + \mu_{\min}^{t-T^*}\left(\sum_{j=0}^{T^*-1}\mu_{\min}^j(2\mu_{\min}) \right)}{\sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right)} \\
  &\geq \frac{C\sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + C(\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right)}{\sum_{j=0}^{t - (T^*+1)} (\mu_{\tilde\phi}^2)^j\left(3\sigma^2_{\max} + 2\lambda_{\max}\right) + (\mu_{\tilde\phi}^2)^{t - T^*}\left(\sum_{j=0}^{T^*-1}\left(\mu_{\phi}^2\right)^{j}(2\sigma^2_{\max} + \lambda_{\max})\right)} \\
  &= C,
\end{align*}
for all $i$ and $t$. A similar argument holds when $t \leq T^* + 1$.
\end{proof}


\begin{lem}[Technical Lemma 1] \label{lem:absdiff}
Let $a_j, b_j \in \R$, $j = 1,2$ be such that $0 < a_1,a_2 < 1$, $b_1,b_2 > 0$. Let $c_1 = \max(a_1,a_2)$, $c_2 = \min(a_1,a_2)$, $d_1 = \max(b_1,b_2)$, $d_2 = \min(b_1,b_2)$. Then, 
$$
  |a_1b_1 - a_2b_2| \leq |a_1 - a_2|d_1 + |b_1 - b_2|c_2.
$$
\end{lem}

\begin{proof}
Observe that 
$$
  |a_1b_1 - a_2b_2| \leq c_1d_1 - c_2d_2 = (c_1 - c_2)d_1 + (d_1 - d_2)c_2. 
$$
The conclusion follows by noting that $c_1 - c_2 = |a_1 - a_2|$ and $d_1 - d_2 = |b_1 - b_2|$.
\end{proof}


\begin{prop}
	Let $\bf{y}_{i,t}$, $i = 1,\ldots,n_i$, $t = 1,\ldots, T$ be a collection of independent time series. Let each time series $\bf{y}_{i,t}$ be generated by model \eqref{model-gen} with accompanying random effects structure \eqref{random-effects}, where $y_{i,t}$ is a scalar response $\x_{i,t} \in \R^p$ is a fixed vector of covariates. Suppose that there is a common shock time $p + 2 < T^* < T - (p+2)$ where $q_1 = q_2 = 1$, $\|\x_{i,t}\| = 1$, each entry of $\x_{i,t}$ is non-negative, $\mu_{\min} > 0$, and $y_{i,0} = 0$. Let $i' \geq 2$ be an index. Then, for any $\epsilon > 0$ we can choose an $\gamma > 0$ such that 
	$$
	  \left\vert\E(y_{i,t} - y_{i',t})\right\vert \leq \epsilon, \qquad |\Var(y_{1,t}) - \Var(y_{i',t})| < \varepsilon,
	$$
	provided that 
	\begin{align*}
  |\mu_\delta'\x_{1,T^*+1} - \mu_\delta'\x_{i',T^*+1}| &< \gamma, \\
%  |\mu_{\tilde\theta_1}'\x_{1,T^*+1} - \mu_{\tilde\theta_{i'}}'\x_{i',T^*+1}| &< \gamma, \\
  \left\vert\mu_{\tilde\theta_{1}}'\x_{1,t} - \mu_{\tilde\theta_{i'}}'\x_{i',t}\right\vert &< \gamma, \qquad \text{for all} \; t, \\
  |\mu_{\tilde\phi_1}^j - \mu_{\tilde\phi_{i'}}^j| &< \gamma, \qquad \text{for all} \; j, \\
  |\x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1}| &< \gamma, \\
  |\x_{1,t}'\Sigma_{\tilde\theta_{1}}\x_{1,t} - \x_{i',t}'\Sigma_{\tilde\theta_{i'}}\x_{i',t}| &< \gamma, \qquad \text{for all} \; t.
	\end{align*}
\end{prop}

\begin{proof}
A similar recursion to that seen in the proof of Proposition~\ref{prop:signaltonoise} yields
\begin{align*}
	&|\E(y_{1,t} - y_{i',t})| =  \left\vert \sum_{j=0}^{t - (T^*+1)}\left[\left(\mu_{\tilde\phi_{1}}^j(\mu_\delta'\x_{1,T^*+1}) - \mu_{\tilde\phi_{i'}}^j(\mu_\delta'\x_{i',T^*+1})\right) + \mu_{\tilde\phi_{1}}^j(\mu_{\tilde\theta_{1}}'\x_{1,t-j}) - \mu_{\tilde\phi_{i'}}^j(\mu_{\tilde\theta_{i'}}'\x_{i',t-j})\right]\right. \\
		&\qquad\qquad\left. + \sum_{j=0}^{T^* - 1}\left[\mu_\eta(\mu_{\tilde\phi_{1}}^{t - T^*} - \mu_{\tilde\phi_{i'}}^{t - T^*})\mu_{\phi}^j + \left(\mu_{\tilde\phi_{1}}^{t - T^*}(\mu_{\theta}'\x_{1,t-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i',t-j})\right)\mu_{\phi}^j\right]\right\vert \\
	&\qquad\leq |\mu_\delta'\x_{1,T^*+1} - \mu_\delta'\x_{i',T^*+1}| 
	  + \sum_{j=1}^{t - (T^*+1)}\left\vert\mu_{\tilde\phi_{1}}^j(\mu_\delta'\x_{1,T^*+1}) - \mu_{\tilde\phi_{i'}}^j(\mu_\delta'\x_{i',T^*+1})\right\vert \\ 
	&\qquad\qquad+ |\mu_{\tilde\theta_{1}}'\x_{1,t} - \mu_{\tilde\theta_{i'}}'\x_{i',t}| + \sum_{j=1}^{t - (T^*+1)}\left\vert \mu_{\tilde\phi_{1}}^j(\mu_{\tilde\theta_{1}}'\x_{1,t-j}) - \mu_{\tilde\phi_{i'}}^j(\mu_{\tilde\theta_{i'}}'\x_{i',t-j})\right\vert \\
	&\qquad\qquad + \mu_\eta\sum_{j=0}^{T^*-1}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*} - \mu_{\tilde\phi_{i'}}^{t - T^*}\right\vert\mu_{\phi}^j + \sum_{j=0}^{T^*-1}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*}(\mu_{\theta}'\x_{1,T^*-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i',T^*-j}) \right\vert\mu_{\phi}^j \\
	&\qquad\leq |\mu_\delta'\x_{1,T^*+1} - \mu_\delta'\x_{i',T^*+1}| + |\mu_{\tilde\theta_{1}}'\x_{1,t} - \mu_{\tilde\theta_{i'}}'\x_{i',t}| \\
	&\qquad\qquad + \sum_{j=1}^{t - (T^*+1)}\left[\vert\mu_{\tilde\phi_{1}}^j - \mu_{\tilde\phi_{i'}}^j\vert\max(\mu_\delta'\x_{1,T^*+1},\mu_\delta'\x_{i',T^*+1}) + \left\vert\mu_\delta'\x_{1,T^*+1} -\mu_\delta'\x_{i',T^*+1}\right\vert\mu_{\tilde\phi_{1}}^j\right] \\
	&\qquad\qquad + \sum_{j=1}^{t - (T^*+1)}\left[\vert\mu_{\tilde\phi_{1}}^j - \mu_{\tilde\phi_{i'}}^j\vert\max(\mu_\delta'\x_{1,t-j},\mu_\delta'\x_{i',t-j}) + \left\vert\mu_{\tilde\theta_{1}}'\x_{1,t-j} - \mu_{\tilde\theta_{i'}}'\x_{i',t-j}\right\vert\mu_{\tilde\phi_{1}}^j\right] \\
	&\qquad\qquad + \mu_\eta\sum_{j=0}^{T^*-1}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*} - \mu_{\tilde\phi_{i'}}^{t - T^*}\right\vert\mu_{\phi}^j + \sum_{j=0}^{T^*-1}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*}(\mu_{\theta}'\x_{1,T^*-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i',T^*-j}) \right\vert\mu_{\phi}^j,	
\end{align*}
where the second inequality follows from applications of Lemma~\ref{lem:absdiff}. Continuing from the second inequality, the conditions of this proposition imply that
\begin{align*}
  &|\E(y_{1,t} - y_{i',t})|	\\
  &\qquad\leq 2\gamma + \sum_{j=1}^{\infty}\left[\vert\mu_{\tilde\phi_{1}}^j - \mu_{\tilde\phi_{i'}}^j\vert\max(\mu_\delta'\x_{1,T^*+1},\mu_\delta'\x_{i',T^*+1}) + \gamma\mu_{\tilde\phi_{1}}^j\right] \\
  &\qquad\qquad + \sum_{j=1}^{\infty}\left[\vert\mu_{\tilde\phi_{1}}^j - \mu_{\tilde\phi_{i'}}^j\vert\max(\mu_\delta'\x_{1,t-j},\mu_\delta'\x_{i',t-j}) + \gamma\mu_{\tilde\phi_{1}}^j\right] \\
  &\qquad\qquad + \mu_\eta\sum_{j=0}^{\infty}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*} - \mu_{\tilde\phi_{i'}}^{t - T^*}\right\vert\mu_{\phi}^j + \sum_{j=0}^{\infty}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*}(\mu_{\theta}'\x_{1,T^*-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i',T^*-j}) \right\vert\mu_{\phi}^j  \\
  &\qquad\leq 2\gamma + 2\gamma\frac{\max(\mu_\delta)}{(1 - \mu_{\tilde\phi_{1}})(1 - \mu_{\tilde\phi_{i'}})} \\
  &\qquad\qquad + \mu_\eta\sum_{j=0}^{\infty}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*} - \mu_{\tilde\phi_{i'}}^{t - T^*}\right\vert\mu_{\phi}^j + \sum_{j=0}^{\infty}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*}(\mu_{\theta}'\x_{1,T^*-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i',T^*-j}) \right\vert\mu_{\phi}^j \\
  &\qquad\leq 2\gamma + 2\gamma\frac{\max(\mu_\delta)}{(1 - \mu_{\tilde\phi_{1}})(1 - \mu_{\tilde\phi_{i'}})} + \gamma\frac{\mu_{\eta}}{1 - \mu_{\phi}} + \sum_{j=0}^{\infty}\left\vert \mu_{\tilde\phi_{1}}^{t - T^*}(\mu_{\theta}'\x_{1,T^*-j}) - \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i',T^*-j}) \right\vert\mu_{\phi}^j \\
  &\qquad\leq 2\gamma + 2\gamma\frac{\max(\mu_\delta)}{(1 - \mu_{\tilde\phi_{1}})(1 - \mu_{\tilde\phi_{i'}})} + \gamma\frac{\mu_{\eta}}{1 - \mu_{\phi}} + \gamma\frac{\max(\mu_\theta) + 1}{1 - \mu_\phi}, 
\end{align*}  
where the last inequality follows from Lemma~\ref{lem:absdiff}. We can choose $\gamma$ small enough so that $|\E(y_{1,t} - y_{i',t})| < \varepsilon$. A similar recursion for the variance yields 
\begin{align*}
 &\left\vert \Var(y_{1,t}) - \Var(y_{i',t})\right\vert 
   = \left\vert \x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1} + \x_{1,t}'\Sigma_{\tilde\theta_i}\x_{1,t} -   \x_{i',t}'\Sigma_{\tilde\theta_{i'}}\x_{i',t} \right. \\
   &\qquad\left. + \mu_{\tilde\phi_1}^2\Var(y_{1,t-1}) - \mu_{\tilde\phi_{i'}}^2\Var(y_{i',t-1}) \right\vert \\
   &\leq \left\vert \x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1} + \x_{1,t}'\Sigma_{\tilde\theta_i}\x_{1,t} -   \x_{i',t}'\Sigma_{\tilde\theta_{i'}}\x_{i',t}\right\vert \\
   &\qquad + \left\vert\mu_{\tilde\phi_1}^2\Var(y_{1,t-1}) - \mu_{\tilde\phi_{i'}}^2\Var(y_{i',t-1})\right\vert \\
   &= \left\vert \x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1} + \x_{1,t}'\Sigma_{\tilde\theta_1}\x_{1,t} -   \x_{i',t}'\Sigma_{\tilde\theta_{i'}}\x_{i',t}\right\vert \\
   &\qquad + \left\vert \mu_{\tilde\phi_1}^2\left(\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2 + \x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} + \x_{1,t-1}'\Sigma_{\tilde\theta_1}\x_{1,t-1} + \mu_{\tilde\phi_1}^2\Var(y_{1,t-2})\right) \right.\\
   &\qquad\qquad\left.- \mu_{\tilde\phi_{i'}}^2\left(\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2 + \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1} + \x_{i',t-1}'\Sigma_{\tilde\theta_{i'}}\x_{i',t-1} + \mu_{\tilde\phi_{i'}}^2\Var(y_{i',t-2})\right) \right\vert \\
   &\leq \left\vert \x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1}\right\vert + \left\vert\x_{1,t}'\Sigma_{\tilde\theta_1}\x_{1,t} -   \x_{i',t}'\Sigma_{\tilde\theta_{i'}}\x_{i',t}\right\vert \\ 
   &\qquad+ \left\vert \mu_{\tilde\phi_1}^2\left(\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2 + \x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} + \x_{1,t-1}'\Sigma_{\tilde\theta_1}\x_{1,t-1}\right) \right.\\
   &\qquad\qquad\left.- \mu_{\tilde\phi_{i'}}^2\left(\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2 + \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1} + \x_{i',t-1}'\Sigma_{\tilde\theta_{i'}}\x_{i',t-1}\right) \right\vert \\
   &\qquad + \left\vert \mu_{\tilde\phi_1}^4\Var(y_{1,t-2}) - \mu_{\tilde\phi_{i'}}^4\Var(y_{i',t-2}) \right\vert \\
   &\leq \sum_{j=0}^{t - (T^* + 1)}\left[ \left\vert \mu_{\tilde\phi_1}^{2j}\x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \mu_{\tilde\phi_{i'}}^{2j}\x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1}\right\vert \right. \\
   &\qquad\left. + \left\vert \mu_{\tilde\phi_1}^{2j}\x_{1,t-j}'\Sigma_{\tilde\theta_1}\x_{1,t-j} - \mu_{\tilde\phi_{i'}}^{2j}\x_{i',t-j}'\Sigma_{\tilde\theta_{i'}}\x_{i',t-j} \right\vert + \left\vert\mu_{\tilde\phi_1}^{2j} - \mu_{\tilde\phi_{i'}}^{2j}\right\vert(\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2)\right] \\
   &\qquad + \left\vert \mu_{\tilde\phi_1}^{2(t - T^*)}\Var(y_{1,t - T^*}) - \mu_{\tilde\phi_{i'}}^{2(t - T^*)}\Var(y_{i',t - T^*}) \right\vert \\
   &\leq \sum_{j=0}^{t - (T^* + 1)}\left[ \left\vert \mu_{\tilde\phi_1}^{2j} - \mu_{\tilde\phi_{i'}}^{2j}\right\vert \lambda_{\text{max}} +  \left\vert\x_{1,T^*+1}'\Sigma_\delta\x_{1,T^*+1} - \x_{i',T^*+1}'\Sigma_\delta\x_{i',T^*+1}\right\vert\mu_{\tilde\phi_1}^{2j} \right. \\
   &\qquad\left. + \left\vert \mu_{\tilde\phi_1}^{2j} - \mu_{\tilde\phi_{i'}}^{2j}\right\vert \lambda_{\text{max}} +  \left\vert\x_{1,t-j}'\Sigma_{\tilde\theta_1}\x_{1,t-j} - \x_{i',t-j}'\Sigma_{\tilde\theta_{i'}}\x_{i',t-j}\right\vert\mu_{\tilde\phi_1}^{2j} + \left\vert\mu_{\tilde\phi_1}^{2j} - \mu_{\tilde\phi_{i'}}^{2j}\right\vert(\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2)\right] \\
   &\qquad + \left\vert \mu_{\tilde\phi_1}^{2(t - T^*)}\sum_{j=0}^{T^*-1}\left(\sigma^2_\eta + \sigma^2 + \x_{1,T^*-j}'\Sigma_\theta\x_{1,T^*-j}\right)\mu_{\phi}^{2j} \right. \\ 
   &\qquad\qquad\left. - \mu_{\tilde\phi_{i'}}^{2(t - T^*)}\sum_{j=0}^{T^*-1}\left(\sigma^2_\eta + \sigma^2 + \x_{i',T^*-j}'\Sigma_\theta\x_{i',T^*-j}\right)\mu_{\phi}^{2j} \right\vert \\
   &\leq 2\gamma\frac{\lambda_{\max}}{(1 - \mu_{\tilde\phi_1}^{2})(1 - \mu_{\tilde\phi_{i'}}^{2})} + 2\gamma\frac{1}{1 - \mu_{\tilde\phi_1}^2} + \gamma\frac{\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2}{(1 - \mu_{\tilde\phi_1}^{2})(1 - \mu_{\tilde\phi_{i'}}^{2})} \\
   &\qquad+ (\sigma^2_\eta + \sigma^2)\left\vert \mu_{\tilde\phi_1}^{2(t - T^*)} - \mu_{\tilde\phi_{i'}}^{2(t - T^*)} \right\vert\sum_{j=0}^{T^*-1}\mu_{\phi}^{2j} \\
   &\qquad+ \sum_{j=0}^{2(t - T^*)}\left\vert \mu_{\tilde\phi_1}^{2(t - T^*)}\x_{1,T^*-j}'\Sigma_\theta\x_{1,T^*-j} - \mu_{\tilde\phi_{i'}}^{2(t - T^*)}\x_{i',T^*-j}'\Sigma_\theta\x_{i',T^*-j} \right\vert \mu_{\phi}^{2j} \\
   &\leq 2\gamma\frac{\lambda_{\max}}{(1 - \mu_{\tilde\phi_1}^{2})(1 - \mu_{\tilde\phi_{i'}}^{2})} + 2\gamma\frac{1}{1 - \mu_{\tilde\phi_1}^2} + \gamma\frac{\sigma^2_\eta + \sigma^2_{\alpha}+ \sigma^2}{(1 - \mu_{\tilde\phi_1}^{2})(1 - \mu_{\tilde\phi_{i'}}^{2})} \\ 
   &\qquad + \gamma\frac{\sigma^2_\eta + \sigma^2}{1 - \mu_\phi^2} + \gamma\frac{\lambda_{\max}}{1 - \mu_\phi^2} + \gamma\frac{1}{1 - \mu_\phi^2}
   \end{align*}
We can choose $\gamma$ small enough so that $|\Var(y_{1,t}) - \Var(y_{i',t})| < \varepsilon$.
\end{proof}

\begin{lem}[Technical Lemma 2] \label{lem:diffsig2noise}
Let $a_j, b_j \in \R$, $j = 1,2$. Suppose that $b_j > b > 0$ for $j = 1,2$. Then for any $\varepsilon > 0$ we have 
$$
  \left\vert\frac{a_1}{b_1} - \frac{a_2}{b_2}\right\vert \leq \varepsilon,
$$
provided that $|a_1-a_2|$,$|b_1-b_2| < \delta$ for some chosen $\delta > 0$.
\end{lem}

\begin{proof}
\begin{align*}
  \left\vert\frac{a_1}{b_1} - \frac{a_2}{b_2}\right\vert 
    &= \left\vert\frac{a_1b_2 - a_2b_1}{b_1b_2}\right\vert 
    = \left\vert\frac{(a_1 - a_2 + a_2)(b_2 - b_1 + b_1) - a_2b_1}{b_1b_2}\right\vert \\
    &= \left\vert \frac{a_1(b_2 - b_1) + a_2b_1 + (a_1 - a_2)b_1 - a_2b_1}{b_1b_2} \right\vert \leq \frac{2\delta}{b^2} \\
    &\leq \frac{|a_1|\delta + |b_1|\delta}{b^2} \leq \varepsilon, 
\end{align*}
where $\delta \leq \varepsilon b^2/(|a_1| + |b_1|)$.
\end{proof}



\noindent\textbf{This needs some work (ignore for now)} Now consider a case where there exists a $2 \leq i' < n + 1$ where $\x_{1,t} = \x_{i',t}$. Now suppose for simplicity that $T^*_{j'} = T^*_1$ and that $T_{i'} = T_1$ where $T_1 > T^*_1 + q_1 + q_2 + 2$. This assumes that we have data for the first time series beyond the shock point. We will show that when there is a covariate clone of the time series under study, then estimation in this series will approximate estimation in the time series under study provided that the underlying variability is very small relative to the signal. To see this, consider $t > T^*_1$, where 
\begin{align*}
y_{1,t} &= \eta_1 + \mu_\alpha + \delta_1'\x_{1,T^*_1+1} + \sum_{j=1}^{q_1} (\phi_{1,j} + \tilde{\phi}_{1,j})y_{1,t-j} + \sum_{j=0}^{q_2-1}(\theta_{1,j+1} + \tilde{\theta}_{1,j+1})'\x_{1,t-j} + \tilde{\varepsilon}_{1} + \varepsilon_{1,t}, \\
y_{i',t} &= \eta_{i'} + \mu_\alpha + \delta_{i'}'\x_{1,T^*_1+1} + \sum_{j=1}^{q_1} (\phi_{i',j} + \tilde{\phi}_{i',j})y_{i',t-j} + \sum_{j=0}^{q_2-1}(\theta_{i',j+1} + \tilde{\theta}_{i',j+1})'\x_{1,t-j} + \tilde{\varepsilon}_{i'} + \varepsilon_{i',t}, \\
\hat y_{1,t} &= \hat \eta_1 + \hat{\alpha}_1(\x_{1,T^*_1+1}) + \sum_{j=1}^{q_1} (\hat{\phi}_{1,j} + \hat{\tilde{\phi}}_{1,j})\hat y_{1,t-j} + \sum_{j=0}^{q_2-1}(\hat\theta_{1,j+1} + \hat{\tilde{\theta}}_{1,j+1})'\hat\x_{1,t-j}, \\
\hat y_{i',t} &= \hat\eta_{i'} + \hat{\alpha}_{i'}(\x_{1,T^*_1+1}) + \sum_{j=1}^{q_1} (\hat{\phi}_{i',j} + \hat{\tilde{\phi}}_{i',j})\hat y_{i',t-j} + \sum_{j=0}^{q_2-1}(\hat\theta_{i',j+1} + \hat{\tilde{\theta}}_{i',j+1})'\hat\x_{1,t-j}, 
\end{align*}
where $\alpha_j(\x) = \mu_\alpha + \delta_j'\x$, and we have 
\begin{align*}
  |y_{1,t} - \hat y_{1,t}| &= \Bigg\vert (\eta_1 - \hat\eta_1) + (\alpha_1 - \hat{\alpha}_1(\x_{1,T^*_1+1})) + \sum_{j=1}^{q_1}\left[(\phi_{1,j} + \tilde{\phi}_{1,j})y_{1,t-j} - (\hat\phi_{1,j} + \hat{\tilde{\phi}}_{1,j})\hat{y}_{1,t-j}\right] \\ 
  &\qquad+ \sum_{j=0}^{q_2-1}\left[(\theta_{1,j+1} + \tilde{\theta}_{1,j+1})'\x_{1,t-j} - (\hat\theta_{1,j+1} + \hat{\tilde{\theta}}_{1,j+1})'\hat\x_{1,t-j}\right] + \varepsilon_{1,t} \Bigg\vert \\
  |y_{i',t} - \hat y_{i',t}| &= \Bigg\vert (\eta_{i'} - \hat\eta_{i'}) + (\alpha_{i'} - \hat{\alpha}_{i'}(\x_{1,T^*_1+1})) + \sum_{j=1}^{q_1}\left[(\phi_{i',j} + \tilde{\phi}_{i',j})y_{i',t-j} - (\hat\phi_{i',j} + \hat{\tilde{\phi}}_{i',j})\hat{y}_{i',t-j}\right] \\ 
  &\qquad+ \sum_{j=0}^{q_2-1}\left[(\theta_{i',j+1} + \tilde{\theta}_{i',j+1})'\x_{1,t-j} - (\hat\theta_{i',j+1} + \hat{\tilde{\theta}}_{i',j+1})'\hat\x_{1,t-j}\right] + \varepsilon_{i',t} \Bigg\vert.
\end{align*}
\textbf{Needs finishing, but the argument is simple: estimation uncertainty declines as error variance decreases. And differences in the random effects decrease as variability decreases.} \vspace*{0.5cm}



%\begin{lem}[Technical Lemma 2] \label{lem1}
%	Let $a_j, b_j \in \R$, $j = 1,2$, and $C > 0$ be such that $C \leq a_j/b_j \leq 2C$, $j = 1,2$. Further suppose that 
%	$$
%	  \frac{|a_1 - a_2|}{a_1 + a_2} < \frac{\varepsilon}{2C},
%	$$
%	for some $\varepsilon > 0$. Then 
%	$$
%	  \frac{|a_1 - a_2|}{b_1 + b_2} < \varepsilon.
%	$$
%\end{lem}

%\begin{proof}
%Observe that 
%$$
%  |a_1 - a_2| < \frac{\varepsilon(a_1 + a_2)}{2C} 
%	  = \frac{\varepsilon(b_1(a_1/b_1) + b_2(a_2/b_2))}{2C} 
%	  \leq \frac{\varepsilon(2b_1C + 2b_2C)}{2C} 
%	  = \varepsilon(b_1 + b_2).
%$$
%The conclusion immediately follows from the above.
%\end{proof}

%\begin{prop} \label{prop:signaltonoisediff}
%	Suppose the conditions of Proposition~\ref{prop:signaltonoise} where $C$ is chosen so that 
%	\begin{equation} \label{signal-to-noise2}
%	  C \leq \frac{\E(y_{i,t})}{\Var(y_{i,t})} \leq 2C,
%	\end{equation}
%	for all $i$ and $1 \leq t \leq T$. Let $2 < i' < n + 1$ be the index of a time series in the donor pool for which CONDITIONS then for some $\varepsilon > 0$ we have 
%	$$
%	  \frac{\left|\E(y_{i,t} - y_{i',t})\right|}{\Var(y_{i,t} - y_{i',t})} < \varepsilon
%	$$
%\end{prop}

%\begin{proof}
%	First suppose that we have 
%	\begin{equation} \label{prop:cond}
%	  \frac{\left|\E(y_{i,t} - y_{i',t})\right|}{\E(y_{i,t} + y_{i',t})} \leq \frac{\varepsilon}{2C}.		
%	\end{equation}
%	Then Lemma~\ref{lem:absdiff} combined with the assumed conditions imply the conclusion of this proposition. We now verify that \eqref{prop:cond} holds.
%Suppose that:
%\begin{align*}
%  |\mu_\delta'\x_{1,T^*+1} - \mu_\delta'\x_{i',T^*+1}| &< \gamma \\
%  |\mu_{\tilde\theta_1}'\x_{1,T^*+1} - \mu_{\tilde\theta_{i'}}'\x_{i',T^*+1}| &< \gamma \\
%  \left\vert\mu_{\tilde\theta_{1}}'\x_{1,t} - \mu_{\tilde\theta_{i'}}'\x_{i',t}\right\vert &< \gamma, \qquad \text{for all} \; t \\
%  |\mu_{\tilde\phi_1}^j - \mu_{\tilde\phi_{i'}}^j| &< \gamma, \qquad \text{for all} \; j \\
%  \mu_\delta'\x_{1,T^*+1} + \mu_\delta'\x_{i',T^*+1} &> D
%\end{align*}
%Similarly, 
%\begin{align*}
%	&\E(y_{i,t} + y_{i',t}) = \sum_{j=0}^{t - (T^*+1)}\left[\left(\mu_{\tilde\phi_{i}}^j(\mu_\delta'\x_{i,T^*+1}) + \mu_{\tilde\phi_{i'}}^j(\mu_\delta'\x_{i',T^*+1})\right) + \mu_{\tilde\phi_{i}}^j(\mu_{\tilde\theta_{i}}'\x_{i,T^*+1}) + \mu_{\tilde\phi_{i'}}^j(\mu_{\tilde\theta_{i'}}'\x_{i',T^*+1})\right] \\
%		&\qquad + \sum_{j=0}^{T^* - 1}\left[\mu_\eta(\mu_{\tilde\phi_{i}}^{t - T^*} + \mu_{\tilde\phi_{i'}}^{t - T^*})\mu_{\phi}^j + \left(\mu_{\tilde\phi_{i}}^{t - T^*}(\mu_{\theta}'\x_{i,t-j}) + \mu_{\tilde\phi_{i'}}^{t - T^*}(\mu_{\theta}'\x_{i,t-j})\right)\mu_{\phi}^j\right].
%\end{align*}
%\end{proof}




%Let 
%$$
%  \rho_\text{min} = \min\left\{
%    \frac{\mu_\eta}{\sigma^2_\eta}, 
%    \frac{\|\mu_{\delta}\|}{\|\Sigma_\delta\|},
%    \frac{\|\mu_{\theta}\|}{\|\Sigma_\theta\|},
%    \frac{\|\mu_{\phi}\|}{\|\Sigma_\phi\|},
%    \frac{\|\mu_{\tilde\theta}\|}{\|\Sigma_{\tilde{\theta}}\|},
%    \frac{\|\mu_{\tilde\phi}\|}{\|\Sigma_{\tilde{\phi}}\|}             
%  \right\},
%$$
%and let
%$$
%  \rho_\text{max} = \max\left\{
%    \frac{\mu_\eta}{\sigma^2_\eta}, 
%    \frac{\|\mu_{\delta}\|}{\|\Sigma_\delta\|},
%    \frac{\|\mu_{\theta}\|}{\|\Sigma_\theta\|},
%    \frac{\|\mu_{\phi}\|}{\|\Sigma_\phi\|},
%    \frac{\|\mu_{\tilde\theta}\|}{\|\Sigma_{\tilde{\theta}}\|},
%    \frac{\|\mu_{\tilde\phi}\|}{\|\Sigma_{\tilde{\phi}}\|}             
%  \right\}.
%$$








\section{Numerical Examples}

In this section, we illustrate shock prevalence testing using two real data examples. Section \ref{sptcop} discusses the prevalence testing of the shock that Conoco Phillips stock experienced on March 9th, 2020. Section \ref{sptuem} discusses the prevalence testing of the shock that seasonally adjusted log nonfarm payrolls experienced in April, 2020.

\subsection{Testing shock prevalence for Conoco Phillips stock price series}

\label{sptcop}

Post-shock  prevalence testing is motivated by post-shock prediction  \citep{lin2021minimizing} and also an extension of it from the perspective of hypothesis testing. Applying our methodology  on one of the datasets demonstrated in \cite{lin2021minimizing} helps connect our work to \cite{lin2021minimizing} and spell out the differences between two methodologies.


As illustrated in Section 5.1 in \cite{lin2021minimizing}, it is found that Conoco Phillips stock price experienced unprecedented shocks due to  oil supply control and COVID-19 pandemic on one single day,  March 9th, 2020. In this data example, we are interested in investigating whether this shock is prevalent, i.e., whether the impact of the shock on the stock price will persist over time. Denote the stock price of Conoco Phillips at time $t$ as $y_{i,t}$ for $i$th time series, where $i = 1, \ldots n+1$, the time series of interest is ordered as the 1st time series, and $n$ is the donor pool size. In the model of $y_{i,t}$, the covariates we consider are S\&P 500 index prices, West Texas Intermediate (WTI) crude oil prices, dollar index, 13-week treasury bill rates, and Chicago Board Options Exchange volatility index (VIX). 

We considered the shock effects that occurred on March 15, 2008, September 2008, and November 27, 2014, where the rationale behind this selection is discussed in Section 5.1(3) in  \cite{lin2021minimizing}. However, we analyzed the multiple shock effects on September (September 8th, 2008; September 16th, 2008; and September 26, 2008) differently. Under the  setting of our methodology, it is possible that the occurrence of  those shocks can be modeled by one persistent shock effect or explained by that the model of $y_{i,t}$ changes after September 5th, 2008. In other words, we analyzed the three shock effects within one time series and considered three donors in our donor pool. 





To analyze the dataset, we propose two models. Denote the covariate vector at time $t$ for $i$th time series by $\bs{x}_{i,t}$.  The first model $\mc{M}_1$, called ``permanent'' model, is 
 \begin{align*}
 	y_{i,t} &= \eta_i + \sum_{j=1}^{q_1} \phi_{i,j} y_{i, t-j}  +\sum_{j=0}^{q_2-1} \bs{x}_{i, t-j}\bs{\theta}_{i, j + 1}  + \alpha_i  D_{i,t}+ \varepsilon_{i,t},\\
 	\alpha_i & = \mu_{\alpha} + \sum_{j=0}^{q_2-1} \bs{x}_{i, T_i^*+1-j}\bs{\delta}_{i, j + 1} + \varepsilon_{\alpha, i},
 \end{align*}
where $q_1 = q_2=2$, $D_{i,t} = I(t > T_i^*+1)$, $i = 1, \ldots, n+1$ and $t = 1, \ldots, T_i$. Note that we allow the shock effect to take effect for multiple time points instead of one time point as in  \cite{lin2021minimizing}. This simple model describes the setting where one additive shock effect is persistent over time. The second model $\mc{M}_2$, called ``dynamic'' model, is
\begin{align*}
 	y_{i,t}
 	&= \left(\eta_i + \sum_{j=1}^{q_1} \phi_{i,j} y_{i, t-j} + \sum_{j=0}^{q_2-1} \bs{x}_{i, t-j}\bs{\theta}_{i, j + 1} \right) (1-D_{i,t}) + f(\mc{F}_{i, t} \alpha_i)D_{i,t} + \varepsilon_{i,t}  \\
 	f(\mc{F}_{i,t},\alpha_i)	 & = \alpha_i +\sum_{j=1}^{q_1} \tilde{\phi}_{i, j} y_{i, t-j} + \sum_{j=0}^{q_2-1} \bs{x}_{i, t-j} \tilde{\bs{\theta}}_{i, j +1} \\
 	\alpha_i & = \mu_{\alpha} + \sum_{j=0}^{q_2-1} \bs{x}_{i, T_i^*+1-j}\bs{\delta}_{i, j + 1} + \varepsilon_{\alpha, i},
 \end{align*}
 where $q_1 = q_2=2$,  $D_{i,t} = I(t > T_i^*+1)$, $i = 1, \ldots, n+1$, and $t = 1, \ldots, T_i$. We estimate $\mc{M}_1$ and $\mc{M}_2$ using ordinary least squares (OLS) estimation. $\mc{M}_2$ adds more flexibility to $\mc{M}_1$ by allowing that the model of $y_{i,t}$ changes after the shock, as seen from the term related to $1-D_{i,t}$,  though the model still preserves the autoregressive model form. In practice, $\mc{M}_2$ should make more sense than $\mc{M}_1$ as it accommodates for more complicated cases. However, $\mc{M}_2$  requires far more degrees of freedom for estimation and this can increase variance in estimation. For this purpose, the synthetic pairwise model selection proposed in the Corollary \ref{coro2} can be used to select between $\mc{M}_1$ and $\mc{M}_2$.
 
 In computing the weights, to avoid the impact of unit differences from different columns of covariates on weighting, we center and scale the covariates. The steps are detailed as below. Consider
\begin{align*}
	\bs{X}= \begin{pmatrix}
		\bs{x}_{1, T_1^*+1} & \bs{x}_{1, T_1^*}\\
		\vdots & \vdots  \\
		\bs{x}_{n+1, T_{n+1}^*+1} & \bs{x}_{1, T_1^*}
	\end{pmatrix},
\end{align*}
where $t = T_i^*+1$ is the time point when the shock effect takes place for $i$th time series. Note that the reason for including $\bs{x}_{i, T_i^*}$ is that we consider $q_2=2$ in $\mc{M}_1$ and $\mc{M}_2$. In other words, the consider the 1-step lagged covariates in the model and its corresponding shock effect time point is $T_i^*$. For each column of $\bs{X}$, we center it by subtracting its column mean and scale the resulting column vector by dividing the standard deviation of the resulting column vector. Denote the resulting covariates matrix by $\bs{X}^*$. We compute the synthetic weights $\mbf{W}^*$  based on $\bs{X}^*$ according to procedures detailed in Section 2.2 of \cite{lin2021minimizing}. However, for the model of $y_{i,t}$, we do not conduct such centering and scaling procedure.


From the definition of $D_{i,t} = I(t > T_i^*+1)$, it is evident that the number of post-shock time points, i.e., time $t$ for which $t > T_i^*+1$, affects the inference results. As our methodology only requires $T_i-T_i^*$ is not small compared to $T_i$, we attempted three post-shock time length for our inference, say $L = 7, 15, 30$. Recall that in  Section \ref{forecast}, we refer to the forecast  that incorporates shock information as adjusted forecast and the one that does not as unadjusted forecast. As specified in Section \ref{forecast}, post-shock prevalence testing requires specification of  the maximum number $H$ of forecast horizons and training sample size $K$, where $h$-step forecasts of adjusted forecast and unadjusted forecast are compared for $h = 1, \ldots, H$. We set $H = 10$, $K = 30$, and $T_i =  H + K + L+15$ for $i = 2, \ldots, 4$. That being said, for $t = K + H + 1, \ldots, T_i$ and $h = 1, \ldots, H$, different $h$-step adjusted and unadjusted forecasts for $y_{i,t}$ under $\mc{M}_1$ or  $\mc{M}_2$ are compared. 


\begin{table}[H]
	\caption{Results for Conoco Philips ($n=3$), $\mbf{W}^* = (0.183, 0, 0.817)$ from scaled covariates} \label{coptable}
\begin{center}
  \begin{tabular}{cccccccccr}
     $L$  & Model & $\sum_{i=2}^4 w_i^* I(a_i^1 > a_i^2)$ & $p_1$ &  $|\hat{p}-p_1|$ & Vote (Correct?) & weighted Vote (Correct?)   \\
     \hline 
     7 & $\mc{M}_1$  & \multirow{2}{*}{$0.817$} & 1  & 0 & 0 (Yes) &0 (Yes)  \\      
     7 & $\mc{M}_2$  &  & 1 & $0$   & 0 (Yes) &0 (Yes)   \\[.2cm]
     15 & $\mc{M}_1$ & \multirow{2}{*}{$0.817$}  & 1   & $0.82$ & 0 (Yes) &1 (No)    \\
     15 &  $\mc{M}_2$ & & 1 & 0 &0 (Yes)  &0 (Yes)  \\[.2cm]
     30 &  $\mc{M}_1$ & \multirow{2}{*}{$1$}  & 1   & $0.82$ & 0 (Yes) &1 (No)   \\
     30 &  $\mc{M}_2$ &  & 1  & 0 &0 (Yes) & 0 (Yes)  
  \end{tabular}
\end{center}	
\end{table}


We implemented our analysis of the Conoco Phillips dataset and summarized the results in Table \ref{coptable}. We discuss our finding in several parts as follows.

Note that the synthetic weight vector $\mbf{W}^*$ for the three donors is $(0.183, 0, 0.817)$. It suggests that the donor associated with the shock on November 27, 2014 is the most similar time series relative to the time series of interest, and the one on March 15, 2008 is the second. As pointed out by \cite{lin2021minimizing} in Section 5.1(3), the shocks experienced by the first and  third donor are related to recessions, and oil supply shock, respectively. The results are intuitive in the sense that from a retrospective perspective of a data analyst, the shock experienced by the series $\{y_{1,t}\colon t = 1, \ldots, T_1\}$ is due to oil supply shock and recessions caused by COVID-19 pandemic.



In term of Table \ref{coptable}, it is important to note that $p_1$ is the $p$-value of the forecast comparison between adjusted forecast and unadjusted forecast if we have the data $\{y_{1,t} \colon 1, \ldots, T_1\}$, where $T_1 > T_1^*$. In other words, it cannot be computed without observing $y_{1, t}$ for $t > T_1^*$ and we present $p_1$ only for retrospective analysis. Also, $\hat{p}$ is the $p$-value from weighting the $p$-values of the forecast comparison  between adjusted and unadjusted forecast for donors using $\mbf{W}^*$. 

Let $a_i^1$ and $a_i^2$ denote the AICs of $\mc{M}_1$ and $\mc{M}_2$ for $i$th time series, respectively. Using the synthetic pairwise model selection procedures specified in Corollary \ref{coro2}, as  $\sum_{i=2}^4 w_i^* I(a_i^1 > a_i^2)>0.5$ for $L = 7, 15, 30$, we conclude that $\mc{M}_2$ is preferred over $\mc{M}_1$.  Notice that $|\hat{p}-p_1|$ is uniformly zero and $p_1$ is uniformly 1 for different $L$ and $\mc{M}_2$. The test  using $\hat{p}$ with $\mc{M}_2$ fails to reject that  the shock, which was to take effect at $t=T_1^*+1$,  is prevalent for $t > T_1^*+1$ under a significance level of 5\%. Nevertheless, the results appear to be a little different for $\mc{M}_1$. For $L=7$, $|\hat{p}-p_1|=0$ for $\mc{M}_1$ whereas $|\hat{p}-p_1|=0.82$ for $L = 15, 30$. Based on the performance of $\mc{M}_2$, it is possible that as the time window $L$ increases, the signal from changing parameters covariates is ``misunderstood'' by $\mc{M}_1$ as shock signal. 



Recall from Section \ref{forecast}, voting procedure suggests shock transience (shock prevalence) if the majority of the tests  suggests shock transience (shock prevalence, respectively). We record 0 if the voting procedure suggests shock transience and 1 if shock prevalence. The weighted voting procedure weight those testing indicators using $\mbf{W}^*$; and returns shock prevalence if the output exceeds 0.5 and shock transience if the output is smaller than 0.5. As we observe $\{y_{1,t} \colon 1, \ldots, T_1\}$, we are able to check whether the decision of voting and weighted voting is correct. From the sixth column of Table \ref{coptable}, we can see that voting procedure suggests shock transience for $\mc{M}_1$ and $\mc{M}_2$ for different $L$. However, weighted voting suggests shock transience for $\mc{M}_2$ for different $L$ but shock prevalence for $\mc{M}_1$ and $L=15, 30$ and shock transience for $\mc{M}_1$ and $L = 7$. As $p_1$ is uniformly 1, it is reasonable to expect $\P(p_1\leq .05)$ be small. Note that Proposition \ref{votprop} and Corollary \ref{coro1} show that the decisions made by voting and weighted voting procedures should be correct with very high probability if we assume the asymptotic approximation of the expected absolute error works well for  $n = 3$. Note that the interpretation and meaning of $p_1$ is closely connected with models. For example, $\mc{M}_1$ would imply a persistent shock whereas $\mc{M}_2$ is more general in suggesting changing dynamics. Thus, even if  this theoretical statement  may be correct, it makes practical sense only when model is identified well. This comment corresponds to the possibility that $\mc{M}_1$ may ``misunderstand'' the wrong signal of changing dynamics that are instead captured by $\mc{M}_2$.

To verify our results, we plot $\{y_{1,t} \colon 1, \ldots, T_1\}$ in Figure \ref{copshocktransience}. Two dashed vertical lines are plotted in Figure \ref{copshocktransience}, where the left one corresponds to $t = T_1^*$ and the right one  corresponds to $t = T_1^*+1$. As shown by \cite{lin2021minimizing}, $y_{1,t}$ experienced a substantial shock at  $t = T_1^*+1$ and it is evident from $y_{1, T_1^*}-y_{1, T_1^*+1}$ shown in Figure 1. We also graphed a benchmark horizontal dashed line for comparison, which corresponds to the stock price $y_{1, T_1^*}$. However, notice that even if $y_{1,t}$ first experienced a considerable drop at $t = T_1^*+1$,   it started to revert back and became closer to the benchmark horizontal line as $t$ increases for $t > T_1^*$. This observation confirms our results as the reverting behavior implies that the impact of the shock starts to diminish as $t$ increases and that the shock is very likely to be transient.


\begin{figure}[H]
	\begin{center}
		\includegraphics[height = 8cm]{COPtransience.pdf}
		\caption{The shock transience of $\{y_{1,t} \colon 1, \ldots, T_1\}$} \label{copshocktransience}
	\end{center}	
\end{figure}


%\begin{table}[H]
	%\caption{Results for Conoco Philips ($n=5$), $\mbf{W}^* = (0, 0, 0, 0.363, 0.637)$ from scaled covariates}
%\begin{center}
  %\begin{tabular}{cccccccccr}
     %$L$ ($T_i-T^*_i$) & Model & $p_1$  & $|\hat{p}-p_1|$ & Vote & weighted Vote  & weighted  AIC\\
     %\hline 
     %7 & Permanent & 1  & 0 & \textcolor{red}{0}  & \textcolor{red}{0} & $236.21$ \\
     % 7 & Dynamic & 1 & $0$   &  \textcolor{red}{0} & \textcolor{red}{0}  & $324.91$ \\[.2cm]
     %15 & Permanent & 1   & 0 & \textcolor{red}{0} &  \textcolor{red}{0}    & 273.95 \\
     %15 & Dynamic & 1  & 0 & \textcolor{red}{0}  & \textcolor{red}{0}  & $370.83$\\[.2cm]
     %30 & Permanent & 1   & $0$ &  \textcolor{red}{0} & \textcolor{red}{0}  &  346.17 \\
     %30 & Dynamic & 1 & 0 & \textcolor{red}{0} &\textcolor{red}{0}  & 451.16
  %\end{tabular}
%\end{center}	
%\end{table}


\subsection{Testing shock prevalence for Month-on-Month Change in Log Nonfarm Payrolls (Seasonally Adjusted)}

\label{sptuem}

Arguably the most watched macroeconomic indicator for the US economy is the monthly release of the total nonfarm payrolls, which reflects the gains or losses of persons employed. The pandemic-induced recession of 2020 coincided with a one-month drop in US nonfarm payrolls in excess of 20 million people, or $13.57\%$, between March and April 2020.  This decline exceeds all other monthly declines by many orders of magnitude, and hence then distance-based weighting employed in \citep{lin2021minimizing} could not hope to recover the full shock effect.  However, the test developed herein does not rely as heavily upon the assumption of a common shock distribution for donors or the assumption that the donors represent a rich and diverse sampling from that distribution (\textcolor{red}{Think about this}).  Instead, what is needed is that the donor pool serve as an information source about the persistence of a shock effect in the time series under study.  

To that end, we assemble a donor pool of US recessions induced by large shocks in the last three decades: the recessions of 1981-82, 2001-2003, and 2007-2009 (\textcolor{red}{Justification needed for these shocks; the March 2020 lockdown was an unprecedented halting of economic activity}).  We employ a suite of monthly macroeconomic indicators as covariates for the linear model as for arriving at an optimal weighting of the donor pool.  These indicators are all transformed to their month-on-month change in log values, so as to be consistent with the time series under study and to capture the signal inherent in changes, not raw levels, of macroeconomic variables.  These indicators are the US unemployment insurance transfers, real personal income, personal consumption expenditures, industrial production, consumer price index, Federal Reserve funds rate, and the count of black Americans age 20 and over on US nonfarm payrolls.


The variable selection steps are outlined as follows. At first, we have the same set of covariates $\{\mc{X}_1, \ldots, \mc{X}_p\}$ for each donor, where $\mc{X}_1$ corresponds to the name of the covariate. Next, for each donor, we conduct stepwise variable selection based on AIC so that for each donor, we end up with a model with covariates that may differ across  donors, say $\{\mc{X}_j \colon j\in \mc{I}_i\}$ for $i = 2, \ldots, n+1$, where $\mc{I}_j$ is the collection of the indices corresponding to which covariate is selected. Next, we  take the union $\cup_{i=2}^{n+1}\{\mc{X}_j \colon j\in \mc{I}_i\}$ to obtain the final set of covariates that are present in the model of every donor and the time series of interest.

In Table 2, using the synthetic pairwise model selection procedures specified in Corollary \ref{coro2}, because $\sum_{i=2}^4 w_i^* I(a_i^1 > a_i^2)>0.5$,  we conclude that  $\mc{M}_2$ is preferred. Thus, we turn our interest to that model.  Since $p_{1}$ is far above the rejection threshold, we lack the evidence to reject the null of a transient shock in the time series under study, the month-on-month change in log employment in the span 2019-2022.  The weighted p-value, $\hat p $, misses the ground truth p-value by .146 in absolute value, yet renders the correct decision.




\textbf{Questions to answer and points to hit}
\begin{enumerate}
\item Why did I choose the donors I chose? X 
\item Why did I use the columns I did in OLS? X
\item Why did I use the columns I did in the estimation of the points on the simplex? X
\item Discuss the two models, M1, M2 X
\item Weighted AIC just as in previous example?  X
\item By weighted AIC, M2 is the preferred model Yes
\item We can see that $p_{1}$ is far larger than the rejection threshold, indicating that we lack the evidence to reject the null of a transiet shock
\item The weighted pvalue, $\hat p $, misses the ground truth p-value by .146 in absolute value
\item The method of voting renders the correct decision
\item The method of weighted voting renders the correct decision
\item Ask Jilei to correct y-axis X
\item Ask Jilei whether AIC was used in the same way as first example X
\item Ask Jilei whether covariate set $\textbf{X}$ is same as simplex optimization $\textbf{X}$ 
\item Ask Jilei to drop RPI as a covariate in OLS (but not necessarily for optimization) (needs checking)
\item Ask Jilei to include chart with data stopping at March 2020 to better visualize how payrolls change by month X

\end{enumerate}

\begin{table}[H]
	\caption{Results for Month-on-Month Change in Log Nonfarm Payrolls (Seasonally Adjusted) ($n=3$), $\mbf{W}^* = (0, 0.278, 0.722)$ from scaled covariates} \label{uemtable}
\begin{center}
  \begin{tabular}{ccccccccr}
      Model & $\sum_{i=2}^4 w_i^* I(a_i^1 > a_i^2)$  & $p_1$ &  $|\hat{p}-p_1|$ & Vote (Correct?) & weighted Vote (Correct?)   \\
     \hline 
     $\mc{M}_1$  & \multirow{2}{*}{0.722}& 0  & 1 & 1 (No) &1 (No)  \\      
      $\mc{M}_2$ ($q_1=q_2=1$)  & & 0.470 & $0.146$   & 0 (Yes) &0 (Yes)   \\[.2cm]
  \end{tabular}
\end{center}	
\end{table}


\begin{figure}[H]
	\begin{center}
		\includegraphics[height = 8cm]{UEMtransience.pdf}
		\caption{The shock transience of $\{y_{1,t} \colon 1, \ldots, T_1\}$} \label{uemshocktransience}
	\end{center}	
\end{figure}



\subsection{Simulations}
\label{simulation}



\section{Discussion}

Talk about post-shock forecasting as parameterizing exogeniety and using hindsight in the donor pool to explicitly estimate exogeneity apriori.




\bibliographystyle{plainnat}
\bibliography{synthetic-prediction-notes}
%\bibliography{../synthetic-prediction-notes}

	
\end{document}


