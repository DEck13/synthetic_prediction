#df$shock5 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 5))
m <- lm(t ~ ., data = df)
#m <- lm(t ~ ., data = df)
summary(m)
#shock <- coef(m)[lag + 2:6]
shock <- coef(m)[lag + 2]
shift_hat <- rep(0, length(y))
#shift_hat[(T+n)/2 + 1:5] <- shock
shift_hat[1:(T+n+H) > (T+n)/2] <- shock
# compute losses
for (h in 1:H) {
for (t in 1:T) {
# AR(lag) with shift removed
m1.t.h <- arima(y[(t + H - h + 1):(t + n + H - h)] -
shift_hat[(t + H - h + 1):(t + n + H - h)],
order = c(lag, 0, 0))
# AR(lag)
m2.t.h <- arima(y[(t + H - h + 1):(t + n + H - h)],
order = c(lag, 0, 0))
m1.L[t, h] <- sel(y = y[t + n + H], yhat = predict(m1.t.h, h)$pred[h])
m2.L[t, h] <- sel(y = y[t + n + H], yhat = predict(m2.t.h, h)$pred[h])
}
}
d <- m1.L - m2.L
# taSPA test for comparison of two models
taSPA.mhfc <- function(ell, d, B, bw = 4) {
# ell is the block length
# d is the matrix of loss differential
# B is the bootstrap size
# bw is the bandwidth parameter for computing HAC estimator
# compute dbar
d.bar <- mean(matrix(apply(d, 1, mean)))
# moving-block bootstrap
T <- nrow(d); H <- ncol(d)
# T = ell * K
K <- T / ell
# Bootstrap replication
t.aSPA.B <- c()
for (b in 1:B) {
# uniform draw from 1,...,T - ell + 1
Iks <- sample(1:(T - ell + 1), K, replace = TRUE)
tau <- matrix(sapply(Iks, function(x) x:(x + ell - 1)))
tau <- as.numeric(tau)
# moving-block bootstrap
d.b <- d[tau, ]
d.b.bar <- mean(matrix(apply(d.b, 1, mean)))
# compute moving block bootstrap standard error
d.b.w <- apply(d.b, 1, mean)
xi.b <- 0
for (k in 1:K) {
xi.b <- xi.b + (sum(d.b.w[(k - 1) * ell + 1:ell]) - mean(d.b.w)) ^ 2 / ell
}
xi.b <- xi.b / K
# compute t.aSPA statistic
t.aSPA.B[b] <- sqrt(T) * (d.b.bar - d.bar) / xi.b
}
# compute t.aSPA statistic
## compute Heteroskedasticity and autocorrelation Consistent (HAC) estimator
require('tsapp')
Omega <- HAC(d, method = 'Quadratic Spectral', bw = bw)
w <- matrix(1 / H, nrow = H)
xi.hat <- sqrt(t(w) %*% Omega %*% w)
t.aSPA <- as.numeric(sqrt(T) * d.bar / xi.hat)
p <- mean(t.aSPA < t.aSPA.B)
# return output
return(p)
}
# fail to reject
taSPA.mhfc(ell = 20, d = d, B = 3000)
sqrt(T)
# fail to reject
taSPA.mhfc(ell = ceiling(sqrt(T)), d = d, B = 3000)
T^(1/2+0.01)
T^(1/2-0.01)
# fail to reject
taSPA.mhfc(ell = ceiling(T^(1/2-0.01)), d = d, B = 3000)
sqrt(T) / log(T)
sqrt(T / log(T))
sqrt(T/log(T))
log(T)
100000^(0.01)
100000000^(0.01)
log(1000000000000)
1000000000000^(0.01)
1000000000000^(0.1)
# squared error loss
sel <- function(yhat, y) {
(y - yhat) ^ 2
}
# h = 1 to 24
# T = 120 length of series to evaluate
# y[145:246] is of interest
# n: training sample
T <- 240
n <- 120
H <- 24
m1.L <- matrix(NA, nrow = T, ncol = H)
m2.L <- matrix(NA, nrow = T, ncol = H)
# simulate a time series
set.seed(13)
lag <- 5
y <- arima.sim(n = n + H + T,
model = list(order = c(lag, 0, 0),
ar = rep(0.15, lag)))
plot(y)
shift <- rep(0, length(y))
#shift[(T+n)/2 + 1:5] <- 5:1
shift[1:(T+n+H) > (T+n)/2] <- 5
y <- y + shift
plot(y)
library(data.table)
df <- data.frame(t=y)
setDT(df)[, paste("t", 1:lag, sep = "") := shift(t, 1:lag)]
df$shock <- as.factor(as.numeric(1:nrow(df) > (T+n)/2))
head(df)
y
# h = 1 to 24
# T = 120 length of series to evaluate
# y[145:246] is of interest
# n: training sample
T <- 240
n <- 120
H <- 24
m1.L <- matrix(NA, nrow = T, ncol = H)
m2.L <- matrix(NA, nrow = T, ncol = H)
# simulate a time series
set.seed(13)
lag <- 5
y <- arima.sim(n = n + H + T,
model = list(order = c(lag, 0, 0),
ar = rep(0.15, lag)))
plot(y)
shift <- rep(0, length(y))
#shift[(T+n)/2 + 1:5] <- 5:1
shift[1:(T+n+H) > (T+n)/2] <- 5
#shift[(T+n)/2 + 1:5] <- 5:1
shift[1:(T+n+H) > (T+n)/2] <- 5
y <- y + shift
plot(y)
library(data.table)
df <- data.frame(t=y)
setDT(df)[, paste("t", 1:lag, sep = "") := shift(t, 1:lag)]
df$shock <- as.factor(as.numeric(1:nrow(df) > (T+n)/2))
#df$shock1 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 1))
#df$shock2 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 2))
#df$shock3 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 3))
#df$shock4 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 4))
#df$shock5 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 5))
m <- lm(t ~ ., data = df)
#m <- lm(t ~ ., data = df)
summary(m)
#df$shock1 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 1))
#df$shock2 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 2))
#df$shock3 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 3))
#df$shock4 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 4))
#df$shock5 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 5))
m <- lm(t ~ ., data = df)
#m <- lm(t ~ ., data = df)
summary(m)
#shock <- coef(m)[lag + 2:6]
shock <- coef(m)[lag + 2]
shift_hat <- rep(0, length(y))
#shift_hat[(T+n)/2 + 1:5] <- shock
shift_hat[1:(T+n+H) > (T+n)/2] <- shock
# compute losses
for (h in 1:H) {
for (t in 1:T) {
# AR(lag) with shift removed
m1.t.h <- arima(y[(t + H - h + 1):(t + n + H - h)] -
shift_hat[(t + H - h + 1):(t + n + H - h)],
order = c(lag, 0, 0))
# AR(lag)
m2.t.h <- arima(y[(t + H - h + 1):(t + n + H - h)],
order = c(lag, 0, 0))
m1.L[t, h] <- sel(y = y[t + n + H], yhat = predict(m1.t.h, h)$pred[h])
m2.L[t, h] <- sel(y = y[t + n + H], yhat = predict(m2.t.h, h)$pred[h])
}
}
d <- m1.L - m2.L
d
# taSPA test for comparison of two models
taSPA.mhfc <- function(ell, d, B, bw = 4) {
# ell is the block length
# d is the matrix of loss differential
# B is the bootstrap size
# bw is the bandwidth parameter for computing HAC estimator
# compute dbar
d.bar <- mean(matrix(apply(d, 1, mean)))
# moving-block bootstrap
T <- nrow(d); H <- ncol(d)
# T = ell * K
K <- T / ell
# Bootstrap replication
t.aSPA.B <- c()
for (b in 1:B) {
# uniform draw from 1,...,T - ell + 1
Iks <- sample(1:(T - ell + 1), K, replace = TRUE)
tau <- matrix(sapply(Iks, function(x) x:(x + ell - 1)))
tau <- as.numeric(tau)
# moving-block bootstrap
d.b <- d[tau, ]
d.b.bar <- mean(matrix(apply(d.b, 1, mean)))
# compute moving block bootstrap standard error
d.b.w <- apply(d.b, 1, mean)
xi.b <- 0
for (k in 1:K) {
xi.b <- xi.b + (sum(d.b.w[(k - 1) * ell + 1:ell]) - mean(d.b.w)) ^ 2 / ell
}
xi.b <- xi.b / K
# compute t.aSPA statistic
t.aSPA.B[b] <- sqrt(T) * (d.b.bar - d.bar) / xi.b
}
# compute t.aSPA statistic
## compute Heteroskedasticity and autocorrelation Consistent (HAC) estimator
require('tsapp')
Omega <- HAC(d, method = 'Quadratic Spectral', bw = bw)
w <- matrix(1 / H, nrow = H)
xi.hat <- sqrt(t(w) %*% Omega %*% w)
t.aSPA <- as.numeric(sqrt(T) * d.bar / xi.hat)
p <- mean(t.aSPA < t.aSPA.B)
# return output
return(p)
}
# decision
taSPA.mhfc(ell = ceiling(T^(1/2-0.01)), d = d, B = 3000)
# simulate a time series
set.seed(13)
lag <- 5
y <- arima.sim(n = n + H + T,
model = list(order = c(lag, 0, 0),
ar = rep(0.15, lag)))
plot(y)
shift <- rep(0, length(y))
#shift[(T+n)/2 + 1:5] <- 5:1
shift[1:(T+n+H) > (T+n)/2] <- 5
y <- y + shift
plot(y)
library(data.table)
df <- data.frame(t=y)
setDT(df)[, paste("t", 1:lag, sep = "") := shift(t, 1:lag)]
df$shock <- as.factor(as.numeric(1:nrow(df) > (T+n)/2))
#df$shock1 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 1))
#df$shock2 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 2))
#df$shock3 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 3))
#df$shock4 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 4))
#df$shock5 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 5))
m <- lm(t ~ ., data = df)
#m <- lm(t ~ ., data = df)
summary(m)
#shock <- coef(m)[lag + 2:6]
shock <- coef(m)[lag + 2]
shift_hat <- rep(0, length(y))
#shift_hat[(T+n)/2 + 1:5] <- shock
shift_hat[1:(T+n+H) > (T+n)/2] <- shock
# simulate a time series
set.seed(13)
lag <- 5
y <- arima.sim(n = n + H + T,
model = list(order = c(lag, 0, 0),
ar = rep(0.15, lag)))
plot(y)
shift <- rep(0, length(y))
#shift[(T+n)/2 + 1:5] <- 5:1
shift[1:(T+n+H) > (T+n)/2] <- 5
y <- y + shift
plot(y)
library(data.table)
df <- data.frame(t=y)
setDT(df)[, paste("t", 1:lag, sep = "") := shift(t, 1:lag)]
df$shock <- as.factor(as.numeric(1:nrow(df) > (T+n)/2))
#df$shock1 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 1))
#df$shock2 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 2))
#df$shock3 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 3))
#df$shock4 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 4))
#df$shock5 <- as.factor(as.numeric(1:nrow(df) == (T+n)/2 + 5))
m <- lm(t ~ ., data = df)
#m <- lm(t ~ ., data = df)
summary(m)
#shock <- coef(m)[lag + 2:6]
shock <- coef(m)[lag + 2]
shift_hat <- rep(0, length(y))
#shift_hat[(T+n)/2 + 1:5] <- shock
shift_hat[1:(T+n+H) > (T+n)/2] <- shock
# taSPA test for comparison of two models
taSPA.mhfc <- function(ell, d, B, bw = 4) {
# ell is the block length
# d is the matrix of loss differential
# B is the bootstrap size
# bw is the bandwidth parameter for computing HAC estimator
# compute dbar
d.bar <- mean(matrix(apply(d, 1, mean)))
# moving-block bootstrap
T <- nrow(d); H <- ncol(d)
# T = ell * K
K <- T / ell
# Bootstrap replication
t.aSPA.B <- c()
for (b in 1:B) {
# uniform draw from 1,...,T - ell + 1
Iks <- sample(1:(T - ell + 1), K, replace = TRUE)
tau <- matrix(sapply(Iks, function(x) x:(x + ell - 1)))
tau <- as.numeric(tau)
# moving-block bootstrap
d.b <- d[tau, ]
d.b.bar <- mean(matrix(apply(d.b, 1, mean)))
# compute moving block bootstrap standard error
d.b.w <- apply(d.b, 1, mean)
xi.b <- 0
for (k in 1:K) {
xi.b <- xi.b + (sum(d.b.w[(k - 1) * ell + 1:ell]) - mean(d.b.w)) ^ 2 / ell
}
xi.b <- xi.b / K
# compute t.aSPA statistic
t.aSPA.B[b] <- sqrt(T) * (d.b.bar - d.bar) / xi.b
}
# compute t.aSPA statistic
## compute Heteroskedasticity and autocorrelation Consistent (HAC) estimator
require('tsapp')
Omega <- HAC(d, method = 'Quadratic Spectral', bw = bw)
w <- matrix(1 / H, nrow = H)
xi.hat <- sqrt(t(w) %*% Omega %*% w)
t.aSPA <- as.numeric(sqrt(T) * d.bar / xi.hat)
p <- mean(t.aSPA < t.aSPA.B)
# return output
return(p)
}
# decision
taSPA.mhfc(ell = ceiling(T^(1/2-0.01)), d = d, B = 3000)
tail(d)
d
d[130:150, ]
getRversion()
# compute t.aSPA statistic
## compute Heteroskedasticity and autocorrelation Consistent (HAC) estimator
require('tsapp')
?ggtitle
library(ggplot2)
?ggtitle
#
# career_kAB_merged_pitcher <- career_kAB_merged_pitcher %>%
#   mutate(new_WAR = adj_WAR * new_sigma + new_mu)
#
# write.csv(career_kAB_merged_batter, 'year_by_year_batter_median.csv')
# write.csv(career_kAB_merged_pitcher, 'year_by_year_pitcher_median.csv')
#
#
#
#
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv")
View(season)
#
# career_kAB_merged_pitcher <- career_kAB_merged_pitcher %>%
#   mutate(new_WAR = adj_WAR * new_sigma + new_mu)
#
# write.csv(career_kAB_merged_batter, 'year_by_year_batter_median.csv')
# write.csv(career_kAB_merged_pitcher, 'year_by_year_pitcher_median.csv')
#
#
#
#
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv") %>%
select(name, yearID, G, PA, thres, WAR, adj_WAR)
#
# career_kAB_merged_pitcher <- career_kAB_merged_pitcher %>%
#   mutate(new_WAR = adj_WAR * new_sigma + new_mu)
#
# write.csv(career_kAB_merged_batter, 'year_by_year_batter_median.csv')
# write.csv(career_kAB_merged_pitcher, 'year_by_year_pitcher_median.csv')
#
#
#
#
library(tidyverse)
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv") %>%
select(name, yearID, G, PA, thres, WAR, adj_WAR)
View(season)
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR100G.csv") %>%
select(name, yearID, G, PA, thres, WAR, adj_WAR)
View(season)
season <- read.csv("~/research/era_adjustment_resources/season_pit_eWAR.csv") %>%
select(name, yearID, G, PA, thres, WAR, adj_WAR)
season <- read.csv("~/research/era_adjustment_resources/season_pit_eWAR.csv") %>%
select(name, yearID, G, IPouts, thres, WAR, adj_WAR)
View(season)
season <- read.csv("~/research/era_adjustment_resources/season_pit_eWAR400IP.csv") %>%
select(name, yearID, G, IPouts, thres, WAR, adj_WAR)
View(season)
tal <- read.csv("talent_pit_WAR400IP.csv")
tal <- read.csv("~/research/era_adjustment_resources/talent_pit_WAR400IP.csv")
View(tal)
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv") %>%
select(name, yearID, G, IPouts, thres, WAR, adj_WAR)
View(season)
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv") %>%
select(name, yearID, G, PA, thres, WAR, adj_WAR)
View(season)
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv") %>%
select(name, yearID, pops, G, PA, thres, WAR, adj_WAR)
View(season)
View(season %>% filter(yearID == 1954))
View(season %>% filter(yearID == 1966))
View(season %>% filter(yearID == 1950))
View(season %>% filter(yearID == 2005))
season <- read.csv("~/research/era_adjustment_resources/season_bat_eWAR.csv") #%>%
#select(name, yearID, pops, G, PA, thres, WAR, adj_WAR)
View(season)# %>% filter(yearID == 2005))
rm(list = ls())
## load in output
load("output_decay_T_100.RData")
setwd("~/research/synthetic_prediction/synthetic_prediction/prevalence_testing/numerical_studies/")
## load in output
load("output_decay_T_100.RData")
load("output_decay_T_200.RData")
load("output_decay_T_300.RData")
load("output_decay_T_400.RData")
View(output_T_100)
list = ls()
ls()
rm("list")
output <- c(output_T_100, output_T_200, output_T_300,
output_T_400)
## create simulation parameters
shape.K.Ts <- c(100, 200, 300, 400)
ns <- c(10, 25, 50, 100)
sim_params <- expand.grid(list(shape.K.Ts = shape.K.Ts, ns = ns))
nsim <- 200
# store results
# load packages
require('readxl')
require('writexl')
write_xlsx(lapply(output, as.data.frame), 'trial.xlsx')
result <- c()
for (i in 1:nrow(sim_params)) {
table <- output[[i]]
# means and sds
means <- apply(table, 2, function(x) mean(x))
sds <- apply(table, 2, function(x) sd(x))
result.i <- c()
for (j in 1:2) {
result.i <- cbind(result.i, paste0(round(means[j], digits = 3),
' (', round(sds[j] / sqrt(nsim),
digits = 3), ')'))
}
result <- rbind(result, result.i)
}
result <- cbind(sim_params[, c(2,1)], result)
require('xtable')
xtable(result)
write_xlsx(lapply(output, as.data.frame), 'ntrial.xlsx')
result <- c()
for (i in 1:nrow(sim_params)) {
table <- output[[i]]
# means and sds
means <- apply(table, 2, function(x) mean(x))
sds <- apply(table, 2, function(x) sd(x))
result.i <- c()
for (j in 1:7) {
result.i <- cbind(result.i, paste0(round(means[j], digits = 3),
' (', round(sds[j] / sqrt(100),
digits = 3), ')'))
}
result <- rbind(result, result.i)
}
result <- c()
for (i in 1:nrow(sim_params)) {
table <- output[[i]]
# means and sds
means <- apply(table, 2, function(x) mean(x))
sds <- apply(table, 2, function(x) sd(x))
result.i <- c()
for (j in 1:7) {
result.i <- cbind(result.i, paste0(round(means[j], digits = 3),
' (', round(sds[j] / sqrt(nsim),
digits = 3), ')'))
}
result <- rbind(result, result.i)
}
result <- cbind(sim_params[, c(1,2)], result)
require('xtable')
xtable(result)
sim_params <- expand.grid(list(ns = ns, shape.K.Ts = shape.K.Ts))
sim_params
nsim <- 200
# store results
# load packages
require('readxl')
require('writexl')
write_xlsx(lapply(output, as.data.frame), 'ntrial.xlsx')
result <- c()
for (i in 1:nrow(sim_params)) {
table <- output[[i]]
# means and sds
means <- apply(table, 2, function(x) mean(x))
sds <- apply(table, 2, function(x) sd(x))
result.i <- c()
for (j in 1:7) {
result.i <- cbind(result.i, paste0(round(means[j], digits = 3),
' (', round(sds[j] / sqrt(nsim),
digits = 3), ')'))
}
result <- rbind(result, result.i)
}
result <- cbind(sim_params[, c(1,2)], result)
require('xtable')
xtable(result)
ncol(result)
result <- cbind(sim_params[, c(1,2)], result, digits = c(0,0,3,3,3,3,3,3,3))
?xtable
xtable(result, digits = c(0,0,3,3,3,3,3,3,3))
xtable(result, digits = c(0,0,0,3,3,3,3,3,3,3))
output_T_100
output_T_400
