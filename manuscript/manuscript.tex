\documentclass[11pt]{article}

% use packages
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
%\usepackage{amscd}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{url}
\usepackage{authblk}
\usepackage{bm}
\usepackage[usenames]{color}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{caption}
\usepackage{float}
\usepackage{tikz}
\usepackage{multirow}
\usepackage[linesnumbered, ruled,vlined]{algorithm2e}
% margin setup
\geometry{margin=0.8in}


% function definition
\newcommand{\R}{\mathbb{R}}
\newcommand{\w}{\textbf{w}}
\newcommand{\x}{\textbf{x}}
\newcommand{\X}{\textbf{X}}
\newcommand{\Y}{\textbf{Y}}
\newcommand{\Hist}{\mathcal{H}}
\def\mbf#1{\mathbf{#1}} % bold but not italic
\def\ind#1{\mathrm{1}(#1)} % indicator function
\newcommand{\simiid}{\stackrel{iid}{\sim}} %[] IID 
\def\where{\text{ where }} % where
\newcommand{\indep}{\perp \!\!\! \perp } % independent symbols
\def\cov#1#2{\mathrm{Cov}(#1, #2)} % covariance 
\def\mrm#1{\mathrm{#1}} % remove math
\newcommand{\reals}{\mathbb{R}} % Real number symbol
\def\t#1{\tilde{#1}} % tilde
\def\normal#1#2{\mathcal{N}(#1,#2)} % normal
\def\mbi#1{\boldsymbol{#1}} % Bold and italic (math bold italic)
\def\v#1{\mbi{#1}} % Vector notation
\def\mc#1{\mathcal{#1}} % mathical
\DeclareMathOperator*{\argmax}{arg\,max} % arg max
\DeclareMathOperator*{\argmin}{arg\,min} % arg min
\def\E#1{\mathrm{E}(#1)} % Expectation symbol
\def\var#1{\mathrm{Var}(#1)} % Variance symbol
\def\checkmark{\tikz\fill[scale=0.4](0,.35) -- (.25,0) -- (1,.7) -- (.25,.15) -- cycle;} % checkmark
\newcommand\red[1]{{\color{red}#1}}


\newcommand{\norm}[1]{\left\lVert#1\right\rVert} % A norm with 1 argument
\DeclareMathOperator{\Var}{Var} % Variance symbol

\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{remark}{Remark}
\hypersetup{
  linkcolor  = blue,
  citecolor  = blue,
  urlcolor   = blue,
  colorlinks = true,
} % color setup

% proof to proposition 
\newenvironment{proof-of-proposition}[1][{}]{\noindent{\bf
    Proof of Proposition {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}
% general proof of corollary
  \newenvironment{proof-of-corollary}[1][{}]{\noindent{\bf
    Proof of Corollary {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}
% general proof of lemma
  \newenvironment{proof-of-lemma}[1][{}]{\noindent{\bf
    Proof of Lemma {#1}}
  \hspace*{.5em}}{\qed\bigskip\\}

\allowdisplaybreaks






% title
\title{Minimizing post-shock forecasting error using disparate information}
\author{Jilei Lin\thanks{jileil2@ilinois.edu}}
\author{Ziyu Liu\thanks{ziyuliu3@illinois.edu}}
\author{Daniel J. Eck\thanks{dje13@illinois.edu}}
\affil{Department of Statistics, University of Illinois at Urbana-Champaign}

%%% New version of \caption puts things in smaller type, single-spaced 
%%% and indents them to set them off more from the text.
\makeatletter
\long\def\@makecaption#1#2{
  \vskip 0.8ex
  \setbox\@tempboxa\hbox{\small {\bf #1:} #2}
  \parindent 1.5em  %% How can we use the global value of this???
  \dimen0=\hsize
  \advance\dimen0 by -3em
  \ifdim \wd\@tempboxa >\dimen0
  \hbox to \hsize{
    \parindent 0em
    \hfil 
    \parbox{\dimen0}{\def\baselinestretch{0.96}\small
      {\bf #1.} #2
      %%\unhbox\@tempboxa
    } 
    \hfil}
  \else \hbox to \hsize{\hfil \box\@tempboxa \hfil}
  \fi
}
\makeatother

\begin{document}

\begin{center}
  \textbf{Note}
\end{center}




\begin{center}
  \textbf{Updates in this version}
\end{center}

\begin{enumerate}
  \item Revisions to abstract, introduction, simulation, data analysis, and discussions.
  \item Addition of Section 3.4
  \item A major addition of content to simulation.
\end{enumerate}


% table of contents
%\tableofcontents


\newpage 

\maketitle
\begin{abstract}
    We developed a forecasting methodology for providing credible forecasts for time series data that has undergone a shock  by borrowing knowledge from disparate time series that have undergone similar shocks for which post-shock outcome is recorded. Three shock-effects estimators were constructed for minimizing average forecast risk. We proposed risk-reduction propositions providing conditions when our methodology works and estimators for risk-reduction quantities. Bootstrap procedures are provided to estimate the variability of our shock effect estimators; and these procedures can be used to assess the potential success of post-shock prediction before the post-shock response is observed. The risk-reduction propositions and risk-reduction quantities are powerful tools for users because we can empirically aid a prospective evaluation about whether the three aggregation techniques  will work well and which one is the best. Leave-one-out cross validation with $k$ random  draws is proposed to estimate consistency of risk-reduction propositions and best consistency of voting the best shock-estimators, prospectively informing users the probabilities that this prospective evaluation is consistent with the reality. Several simulated data examples, and a real data example of forecasting Conoco Phillips stock price are provided for verification and illustration.
\end{abstract}




\section{Introduction}

In this article we provide forecasting adjustment techniques with the goal of lowering overall forecast error when the time series under study has undergone a structural shock. It is unlikely that any forecast that previously gave successful predictions for the time series of interest will be able to accommodate the structural shock.  However, all is not lost in this setting, one can integrate information from disparate time series that have previously undergone similar structural shocks to estimate the shock effect of the time series under study. One can then combine these past similar shock effects and add them to the present forecast to reduce the overall forecast error. Then, we discuss some frequently used methodologies that may be applied in this situation.
%When such past similar shock effects exist, one may be able to aggregate the post-shock information from these disparate time series to aid the post-shock forecast for the time series under investigation.  


Improving forecasts through forecast combination has a rich history \citep{bates1969combination, mundlak1978pooling, timmermann2006forecast, granger2014forecasting}.  
%\citet{timmermann2006forecast} provided several reasons for combining forecasts.  In particular, combining forecasts may be beneficial when: 1) the information set underlying individual forecasts is often unobserved to the forecast user; 2) different individual forecasts may be very differently affected by non-stationarity and model misspecifications; 3) different individual forecasts may be motivated by different loss functions \citep[and references therein]{timmermann2006forecast}. 
The classical setting for the forecast combination problem is when there are competing forecasts for a single time series.  %In this setting, one may desire combining forecasts as a method for lowering overall forecast error. %\textbf{Integrate this paragraph into the next paragraph.}
In this setting there are a plethora of methods for combining forecasts, e.g., model averaging \citep{newbold2002forecast, hendry2004pooling, koop2004forecasting, timmermann2006forecast, eklund2007forecast, hansen2008least}, model selection \citep{swanson1997model, swanson2001choosing, lee2015model}, and other various methods \citep{pesaran2011forecast,li2014forecasting}. Forecast encompassing methods are developed to test whether competing forecasts are suitable for combinations \citep{newbold2002forecast, fang2003forecasting}.

%Time-series pooling is a broad topic for borrowing information from multiple time series for prediction improvement and is the focus of this paper. \textbf{Need to mention that these methods may fail or be irrelevant in the presence of shock effects.}

% In our post-shock setting we combine estimated quantities from different time series with the aim of lowering forecast error for a single time series under study. Our techniques are similar to those in time-series pooling and data integration.  The literature of time-series pooling is mainly related to pooling cross-sectional panel data \citep{mundlak1978pooling, zellner1991forecasting, fosten2019panel}. The issues about whether to assume homogeneity or heterogeneity of slope coefficients across individual units are confounded. \citet{baltagi2008forecasting} showed that homogeneity approach often outperform  heterogeneity one in mean squared forecast error; while heterogeneity approach is more general to accommodate differences among units. \textbf{How does this relevant to what we are doing?}

As alternatives to forecast combination, data integration and time series pooling for forecasting is a broad domain of research including ideas from many areas. Time-series pooling methods are frequently used for panel data \citep{zellner1991forecasting, hoogstrate2000pooling, baltagi2008forecasting, fosten2019panel, liu2020forecasting}.  Recent researches in data integration have investigated versatile methods to combine knowledge from multiple datasets for better prediction, whose ideas are similar to ours. For instance, \citet{lee2020estimation} constructed a Bayesian hierarchical model embracing data integration %estimate posterior parameters for Richards model 
to improve predictive precision of COVID-19 infection trajectories for different countries. A similar setup may be beneficial for post-shock prediction but may be too dependent upon model specification for the shock distribution.
%, and conditions for predictive improvement are generally hard to know. 
  \citet{plessen2020integrated} employed a data-mining approach to combine COVID-19 data from different countries as input to predict global net daily infections and deaths of COVID-19 using clustering. However, there is a tremendous amount of volatility in this form of COVID-19 data, and the fit of this prediction method may be improved with modeling structure or preprocessing of the donor pool. %From a machine learning perspective, 
\citet{agarwal2020two} proposed a model-free synthetic intervention method to predict unobserved potential outcomes after different interventions given a donor pool of observed outcomes with given interventions. They also provide useful guidelines for how to estimate the effects of potential interventions by giving recommendations for choosing the metric of interest, the intervention of interest, time horizons, and the donor pool.


%Therefore the traditional forecast combination framework may not be of any help.  
%The Bayesian hierarchical approach of \citet{lee2020estimation} may be too sensitive to prior and the hierarchical parametric model setup.

Up to the scale of the shock, it is very unlikely that the above mentioned methods will work ideally since they are trained on the time series data that do not experience such a shock. To combat this problem, we develop and compare aggregation techniques in this post-shock setting based on the idea of data integration. We assume a simple auto regressive data generating process similar to that in \citet{blundell1998initial} with a general random effects structure. The main idea is to provide a scalar adjustment, based on estimated shock effects from the disparate time series, to the original forecast at the known shock time point. 

We consider three aggregation techniques: simple averaging, inverse-variance weighted averaging, and similarity weighting. The latter technique is similar to the weighting in synthetic control methodology \citep{abadie2010synthetic}. We provide risk-reduction propositions that detail the conditions  when the adjusted forecasts will work better than the original one, and estimate risk-reduction quantities to find the best technique out of the three. The involved parameters in the risk-reduction propositions and risk-reduction quantities are estimated by parametric bootstrap. These propositions and risk-reduction quantities are powerful tools for users in the sense that we can empirically aid a prospective evaluation about whether the three aggregation techniques  will work well and which one is the best, using estimates from bootstrap procedures. Furthermore, to inform the credibility of this prospective evaluation, we propose a leave-one-out cross validation with $k$ random draws to estimate the consistency of the risk-reduction propositions and best consistency of voting the best shock-effect estimators prospectively (see Definition \ref{consistency} and \ref{best consistency}), which tell the probabilities that the prospective evaluation is consistent with the reality. Our Monte Carlo simulation results show that  the risk-reduction conditions are highly consistent with the truth when the model for the shock effects is identified well with appropriate covariates under a fixed design; and gain more precision when the donor pool size increases. Our simulations further show that the procedure of voting the best technique by risk-reduction quantities is consistent with the truth when donor pool size is large. In the real data example of forecasting stock price of Conoco Phillips that experienced a shock on 2020 March 9th, the proposed three aggregation techniques work decently well. We now motivate our aggregation techniques.


% Note that this methodology is not motivated with the goal of unbiased, asymptotically unbiased, or consistent estimation for the shock-effect of the time series under study.


%The question is then how to estimate the shock effects and improve the 
%prediction? This paper proposes three estimators, the adjustment estimator, weighted adjustment estimator, and inverse-variance weighted estimators. Section (to be updated) discusses properties of those estimators. Section (to be updated) compares the  interplay between prediction risk and different shock effects estimators. Section (to be updated) conducts simulation to justify our claims and certain properties that cannot be found analytically.



\section{Setting}
\label{setting}

We will suppose that an analyst has time series data ($y_{i,t}$,$\x_{i,t}$), $t = 1$, $\ldots$, $T_i$, $i = 1$, $\ldots$, $n+1$, where $y_{i,t}$ is a scalar response and $\x_{i,t}$ is a vector of covariates that are revealed to the analyst prior to the observation of $y_{1,t}$.  Suppose that the analyst is interested in forecasting $y_{1,t}$, the first time series in the collection. 
%$y_1, y_2, \ldots$.  
%Given each time point $t \geq 1$, let $\x_{1,t}$ be a vector of covariates 
%revealed prior to the observation of $y_{1,t}$.  

We will suppose that specific interest is in forecasts to made after the occurrence of a structural shock. To gauge the performance of forecasts, we consider forecast risk in the form of mean squared error (MSE),
$$
  R_T = \frac{1}{T}\sum_{t=1}^T\E{\hat y_{1,t} - y_{1,t}}^2,
$$
and root mean squared error (RMSE), given by $\sqrt{R_T}$, in our analyses. 


%The forecast consists of three steps: (1) pick a model, (2) selection of covariates, (3) choices of donor pool.
Our post-shock prediction methodology will consist of selecting covariates $x_{i,t}$, constructing a suitable donor pool of candidate time series that have undergone similar structural shocks to the time series under study, and specifying a model for the time series ($y_{i,t}$,$\x_{i,t}$), $t = 1$, $\ldots$, $T_i$, $i = 1$, $\ldots$, $n+1$. In this article, we consider a dynamic panel data model with autoregressive structure similar to that in \citet{blundell1998initial}. Our dynamic panel model includes an additional shock effect whose presence or absence is given by the binary variable $D_{i,t}$, and we will assume that the donor pool time series are independent of the time series under study. The details of this model are in the next section.


\begin{figure}
  \begin{center}
    \includegraphics[height = 8cm]{comp.pdf}
    \caption{The time series experience a shock at $T_1^*+1=126$ with true shock effect $\alpha = 4.88$. The figure is a comparison between forecast without considering shock effects and the one uses simple averaging given $n=40$ disparate time series, and that the shock time is at $T_1^* +1=126$. The magenta dots represent least square estimate $\hat{\alpha}_i$ from disparate time series. The prediction of $\hat{y}^{2}_{T_1^*+1}$ and $\hat{y}^{1}_{T_1^*+1}$ differs only by an adjustment $\hat{\alpha}=5.22$. It is clear that $\hat{y}^{2}_{T_1^*+1}$ performs better than $\hat{y}^{1}_{T_1^*+1}$.}\label{figure1}
  \end{center}  
  \vspace{-.6cm}
\end{figure}

Figure \ref{figure1} provides simple intuition of the practical usefulness of our proposed methodology. This figure depicts a time-series that experienced a ``shock'' at time point $T_1^*+1 = 126$. It is supposed that the researcher does not have any information beyond $T_1^*+1$, but does have observations of forty disparate time series that have previously undergone a similar shock for which post-shock responses are recorded. Similarity in this context means that the shock effects are random variables that from a common distribution.
%As we can see, the purple line provides a better prediction than the red line in this particular example. But we will justify later that consideration of estimated shock-effects will improve the prediction under certain conditions.
In this example, the mean of the estimated shock effects is taken as a shock-effect estimator for the time series under study. Forecasts are then made by adding this shock-effect estimator to the estimated response values obtained from the process that ignores the shock. It is apparent from Figure \ref{figure1} that adjusting forecasts in this manner 1) leads to a reduction in forecasting risk; 2) does not fully recover the true shock-effect. We evaluate the performance of this post-shock prediction methodology throughout this article; we outline situations for when it is expected to work and when it is not.




%Section \ref{properties} will provide a more detailed treatment about when our proposed estimators in Section \ref{constructionofestimators} will improve the prediction under various model setups in Section \ref{modelsetup}. More examples will be provided using Monte Carlo simulations in Section \ref{simulation}.



\subsection{Model Setup}

\label{modelsetup}

In this section, we will describe the assumed dynamic panel models for which 
post-shock aggregated estimators are provided. The basic structure of these models 
are the same, the differences between them lie in the setup of the shock effect 
distribution.

The model $\mc{M}_1$ is defined as
\begin{align}
\mc{M}_1 \colon y_{i,t} =\eta_i +\alpha_i D_{i,t} + \phi_i y_{i, t-1} + \theta_i'\mbf{x}_{i,t}+ \beta_i'\mbf{x}_{i, t-1} + \varepsilon_{i,t}\label{equation1}
\end{align}
for $t = 1,\ldots,T_i$ and $i = 1,\ldots, n+1$, where $D_{i,t} = I(t = T_i^* + 1)$, 
$T_i^* < T_i$ and $\x_{i,t} \in \R^{p}$, $p \geq 1$.  We assume that the 
$\mbf{x}_{i,t}$'s are fixed and $T_i^*$s are known. The random effects structure for $\mc{M}_1$ is:
\begin{align*}
  \eta_i &\simiid  \eta ,\where \E{\eta} = 0, \var{\eta} = \sigma^2_{\eta}, \qquad i = 1, \ldots, n+1,\\
  \phi_i &\simiid \phi, \where |\phi|<1, \qquad i = 1, \ldots, n+1, \\
   \theta_i &\simiid \theta, \where \E{\theta}=\mu_{\theta}, \var{\theta}=\Sigma_{\theta}^2, \qquad i = 1, \ldots, n + 1, \\
\beta_i &\simiid \beta, \where \E{\beta}=\mu_{\beta}, \var{\beta}=\Sigma_{\beta}^2, \qquad i = 1, \ldots, n+1,\\
\alpha_i &\simiid \alpha, \where \E{\alpha}=\mu_{\alpha}, \var{\alpha}=\sigma_{\alpha}^2, \qquad  i = 1, \ldots, n+1; \\
\varepsilon_{i,t} &\simiid \normal{0}{\sigma^2}, \qquad t=1, \ldots, T_i, \; i = 1, \ldots, n+1,\\
\eta &\indep  \alpha_i \indep \phi \indep \theta \indep \varepsilon_{i,t}.
\end{align*}
Notice that $\mc{M}_1$ assumes that $\alpha_i$ are iid with $\E{\alpha_i}=\mu_{\alpha}$ 
for $i = 1, \ldots, n+1$.  %Although synthesizing information from disparate time series assumes similarities of shock effects, shock effects may differ in terms of their means in practice. 
We also consider a model where the shock effects are linear functions of covariates and 
lagged covariates with an additional additive mean-zero error.
%Additionally, $\mc{M}_1$ can be further improved in the sense that the shock effects may depend on the covariates around the shock time points, i.e., $T_i^*$ for $i = 1, \ldots, n+1$. 
%$\mc{M}_2$ can be constructed to accommodate those issues by imposing additional structures on the means of $\alpha_i$ as in (\ref{model2}) for $i = 1, \ldots, n+1$.
The random effects structure for this model (model $\mc{M}_2$) is:
\begin{align}
\mc{M}_2 \colon \begin{array}{l}
  y_{i,t} =\eta_i +\alpha_i D_{i,t} + \phi_i y_{i, t-1} + \theta_i'\mbf{x}_{i,t}+ \beta_i'\mbf{x}_{i, t-1} + \varepsilon_{i,t}\\[.2cm]
  \; \alpha_i = \mu_{\alpha}+\delta_{i}'\mbf{x}_{i, T_i^*+1}+\gamma_i'\mbf{x}_{i, T^*_i}+\t{\varepsilon}_{i},
\end{array}\label{model2}
\end{align}
for $i = 1, \ldots, n+1$, where the added random effects are
\begin{align*}
\t{\varepsilon}_{i} &\simiid  \E{\t{\varepsilon}}=0, \var{\t{\varepsilon}}=\sigma^2_{\alpha}, \qquad i = 1, \ldots, n+1,\\
\eta &\indep  \alpha_i \indep \phi \indep \theta \indep \varepsilon_{i,t} \indep \t{\varepsilon}_{i}.
\end{align*}
%and $\delta_i$ and $\gamma_i$ will be discussed shortly. 
We further define 
$\tilde{\alpha}_i=\mu_{\alpha}+\delta_i'\mbf{x}_{i, T_i^*+1}+\gamma_i'\mbf{x}_{i, T_i^*}$. 
We will investigate post-shock aggregated estimators in $\mc{M}_2$ 
in settings where $\delta_i$ and $\gamma_i$ are either fixed or random. 
We let $\mc{M}_{21}$ denote model $\mc{M}_{2}$ with $\gamma_i = \gamma$ 
and $\delta_i = \delta$ for $i= 1, \ldots, n+1$, 
where $\gamma$ and $\delta$ are fixed unknown parameters.
We let $\mc{M}_{22}$ denote model $\mc{M}_{2}$ with the following random effects 
structure for $\gamma$ and $\delta$:
\begin{align*}
\begin{array}{c}
  \gamma_i \overset{iid}{\sim} \mrm{E}(\gamma) = \mu_\gamma, \var{\gamma} = \Sigma_\gamma \\
  \delta_i \overset{iid}{\sim} \mrm{E}(\delta) = \mu_\delta, \var{\delta} = \Sigma_\delta
\end{array}
   \quad \text{ with } \quad  \delta_i  \indep \t{\varepsilon}_{i} \quad  \text{ and } \quad \gamma_i  \indep \t{\varepsilon}_{i}.
\end{align*}
Note that $\delta_i$ and $\gamma_i$ may be dependent. We further define the parameter sets
\begin{align}
  \begin{array}{lll}
     \Theta &= &\{(\eta_i, \phi_i, \theta_i, \beta_i, \alpha_i, \mbf{x}_{i,t}, y_{i,t-1}, \delta_i, \gamma_i)\colon t= 1, \ldots, T_i, i = 2, \ldots, n +1\}.\\
    \Theta_1 &= &\{(\eta_i, \phi_i, \theta_i, \beta_i, \alpha_i, \mbf{x}_{i,t}, y_{i,t-1}, \delta_i, \gamma_i)\colon t= 1, \ldots, T_i, i = 1\}.\label{parameter},
  \end{array}
\end{align}
where $\Theta$ and $\Theta_1$ can adapt to $\mc{M}_1$ by dropping $\delta_i$ and 
$\gamma_i$. We assume this for notational simplicity.



\subsection{Forecast}
\label{forecast}
In this section we show how post-shock aggregate estimators improve upon standard 
forecasts that do not account for the shock effect.
%The interest of this study lies in comparing how consideration of shock effects 
%improves the prediction. 
More formally, we will consider the following candidate forecasts: 
\begin{align*}
  &\text{Forecast 1}: \hat y_{1,T_1^*+1}^1 = \hat\eta_1 
    + \hat\phi_1 y_{1,T_1^*} + \hat\theta_1'\x_{1,T_1^*+1} 
    + \hat\beta_1'\x_{1,T_1^*}, \\
  &\text{Forecast 2}: \hat y_{1,T_1^*+1}^2 = \hat\eta_1 
    + \hat\phi_1 y_{1,T_1^*} + \hat\theta_1'\x_{1,T_1^*+1} 
    + \hat\beta_1'\x_{1,T_1^*} + \hat{\alpha},
\end{align*}
where $\hat\eta_1$, $\hat\phi_1$, $\hat\theta_1$, and $\hat\beta_1$ are all 
OLS estimators of $\eta_1$, $\phi_1$, $\theta_1$, and $\beta_1$ respectively, 
and $\hat{\alpha}$ is some form of estimator for the shock effect of time series of interest, i.e., $\alpha_1$. 
The first forecast ignores the presence of $\alpha_1$ while the second forecast 
incorporates an estimate of $\alpha_1$ that is obtained from the other independent forecasts under study. 
%Under $\mc{M}_1$ (Section \ref{modelsetup}), $\E{\alpha_1} = \mu_{\alpha}$.

Note that the two forecasts do not differ in their predictions for 
$y_{1,t}$, $t = 1,\ldots T_1^*$, they only differ in predicting 
$y_{1,T_1^*+1}$. Throughout the rest of this article we show that the collection of 
disparate time series $\{y_{i,t}, t = 2,\ldots,T_i, i = 1,\ldots,n\}$ has 
the potential to improve the forecasts for $y_{1, t}$ when $t > T_1^*$ under different 
circumstances for the dynamic panel model $\mc{M}_1$, $\mc{M}_{21}$, and $\mc{M}_{22}$. 
It is important to note that in general $\hat{\alpha}$ 
is not a consistent estimator of the unobserved $\alpha_1$ nor does it converge 
to $\alpha_1$.  Despite these inferential shortcomings, adjustment of the forecast 
for $y_{1,T_1^*+1}$ through the addition of $\hat{\alpha}$ has 
the potential to lower forecast risk under several conditions corresponding to 
different estimators of $\alpha_1$. % which will be discussed shortly.


\subsection{Construction of shock effects estimators}
\label{constructionofestimators}

We now construct the aggregate estimators of the shock effects that appear in 
Forecast 2. We use these to forecast response values $y_{1, t}$ when 
$t > T_1^*$, i.e., the time series of interest after the shock time where we 
assume that $T_1^*$ is known.  %Notice that the shock effect 
%for time series $1$ is typically unobserved. But we can gather knowledge from 
%disparate time series to construct some sort of estimators that may be helpful for 
%estimating $\E{\alpha_1}$. 
First, we introduce the procedures of parameter estimation for 
$\mc{M}_1$, $\mc{M}_{21}$, and $\mc{M}_{22}$ (see Section \ref{modelsetup}). 
Conditional on all regression parameters, previous responses, and covariates, 
the response variable $y_{i,t}$ in $\mc{M}_1$, $\mc{M}_{21}$, and $\mc{M}_{22}$ 
has distribution 
$$
  y_{i,t} 
    \sim N(\eta_i + \alpha_iD_{i,t} + \phi_i y_{i,t-1} + \theta_i'\x_{i,t} 
      + \beta_i'\x_{i,t-1}, \sigma^2).
$$
For $i = 2, \ldots, n$, all parameters in this model will be estimated with ordinary least squares 
(OLS) using historical data of $t = 1, \ldots, n_i$. For $i = 1$, we estimate all the parameters but $\alpha_1$ using OLS procedures for $t=1, \ldots, T_1^*$. In particular, let $\hat{\alpha}_i$, $i = 2, \ldots, n+1$ be the OLS estimate 
of $\alpha_i$.  Note that parameter estimation 
for $\mc{M}_1$ is identically the same as $\mc{M}_{21}$ and $\mc{M}_{22}$.  


Second, we introduce the candidate estimators for $\alpha_1$. Define the 
\emph{adjustment estimator} for time series $i=1$ by,
\begin{equation} \label{adjusted}
  \hat{\alpha}_{\text{adj}} = \frac{1}{n}\sum_{i=2}^{n+1}\hat{\alpha}_i,
\end{equation}
where the $\hat{\alpha}_i$s in \eqref{adjusted} are OLS estimators of all of 
the $\alpha_i$s.  We can use $\hat{\alpha}_{\text{adj}}$ as an estimator for 
the unknown $\alpha_1$ term for which no meaningful estimation information 
otherwise exists. It is intuitive that $\hat{\alpha}_{\rm adj}$ should perform 
well under $\mc{M}_1$ where we assume that $\alpha_i$'s share the same mean 
for $i= 1, \ldots, n+1$. However, it can also be shown that 
$\hat{\alpha}_{\rm adj}$ may be less favorable in $\mc{M}_{21}$ 
and $\mc{M}_{22}$, which will be discussed in detail in Section \ref{properties}. 

We also consider the \emph{inverse-variance weighted estimator} 
in practical settings where the $T_i$'s and $T_i^*$'s vary greatly across $i$. 
The inverse-variance weighted estimator is defined as 
\begin{align*}
  \hat{\alpha}_{\rm IVW} = \frac{\sum_{i=2}^{n+1} \hat{\alpha}_i / \hat{\sigma}_{i\alpha}^2}{\sum_{i=2}^{n+1} 1/\hat{\sigma}_{i\alpha}^2},
  \quad \text{ where } \quad  \hat{\sigma}_{i\alpha}^2 = \hat{\sigma}^2_i( \mathbf{U}_i'\mbf{U}_i)_{22}^{-1},
\end{align*}
 $\hat{\alpha}_i$ is the OLS estimator of $\alpha_i$, 
$\hat{\sigma}_i$ is the residual standard error from OLS estimation, 
and $\mbf{U}_i$ is the design matrix for OLS with respect to time series 
for $i = 2, \ldots, n+1$. Note that since $\sigma$ is unknown, estimation 
is required and the numerator and denominator terms are dependent in general. 
%It is hard to evaluate its expectation and variance. 
%But it is clear 
%that $\hat{\alpha}_{\rm IVW}$ will generally not be unbiased. 
However, $\hat{\alpha}_{\rm IVW}$ can be a reasonable estimator in 
practical settings. %when the change 
%point is not symmetric and/or there are some series with small or large number 
%of time points recorded. We will then investigate the performance of this 
%estimator in simulation (Section \ref{simulation}).
We do not provide closed form expressions for $\E{\hat{\alpha}_{\rm IVW}}$ 
and $\var{\hat{\alpha}_{\rm IVW}}$, empirical performance of 
$\hat{\alpha}_{\rm IVW}$ is assessed via Monte Carlo simulation 
(see Section \ref{simulation}).

%More importantly, we introduce the backgrounds and procedures of constructing \emph{weighted-adjustment estimator} as follows. 
We now motivate a \emph{weighted-adjustment estimator} for model $\mc{M}_{21}$ 
and $\mc{M}_{22}$. Our weighted-adjustment estimator is inspired by the 
weighting techniques in synthetic control methodology (SCM) developed 
in \cite{abadie2010synthetic}. 
%we intend to construct a weighted adjustment estimator by using similar but 
%different methods. 
However, our weighted-adjustment estimator is not a causal estimator and 
our estimation premise is a reversal of that in SCM. 
%The case study of \citet{abadie2010synthetic} in essence observes the data and 
%estimates the effect of policy. 
Our objective is in predicting a post-shock response $y_{1,T_1^*+1}$ that is not yet 
observed using disparate time series whose post-shock responses are observed.
%However, the idea of merging information from  similar events (i.e., time series in our 
%study) to improve the solutions for the problem of interest should be the same. 

We use similar notation as that in \cite{abadie2010synthetic} to motivate 
our weighted-adjustment estimator. Consider a $n\times 1$ weight vector 
$\mbf{W}=(w_2, \ldots, w_{n+1})$, where $w_i\in [0,1]$ for all 
$i = 2, \ldots, n+1$. Construct
\begin{align*}
 \mbf{X}_1 = \begin{pmatrix}
    \mbf{x}_{1, T_1^*} \\ \mbf{x}_{1, T_1^*+1}
  \end{pmatrix}
  \quad \text{ and }
  \quad  \hat{\mbf{X}}_1(\mbf{W})
  = w_2\begin{pmatrix}
     \mbf{x}_{2, T_2^*} \\
      \mbf{x}_{2, T_2^*+1}
   \end{pmatrix} + \cdots + w_{n+1} \begin{pmatrix}
     \mbf{x}_{n+1, T_{n+1}^*} \\
      \mbf{x}_{n+1, T_{n+1}^*+1}
   \end{pmatrix} .
\end{align*}
where $\mbf{X}_1$ and $\hat{\mbf{X}}_1(\mbf{W})$ are $2 \times p$. Define $\mc{W}=\{\mbf{W}\in [0,1]^n \colon w_2+ \cdots + w_{n+1}=1 \}$. 
Suppose there exists $\mbf{W}^*\in \mc{W}$ with 
$\mbf{W}^*=(w_2^*, \ldots, w_{n+1}^*)$ such that
\begin{align}
 \mbf{X}_1=\hat{\mbf{X}}_1(\mbf{W}^*)  \quad i.e., \quad \mbf{x}_{1, T_1^*} = \sum_{i=2}^{n+1} w_i^*\mbf{x}_{i, T_i^*} \text{ and } \mbf{x}_{1, T_1^*+1} = \sum_{i=2}^{n+1} w_i^*\mbf{x}_{i, T_i^*+1}.\label{SCM}
\end{align}
 Notice that $\mbf{W}^*$ exists as long as $\mbf{X}_1$ falls in the convex hull of 
 \begin{align*}
   \left\{ \begin{pmatrix}
     \mbf{x}_{2, T_2^*} \\
      \mbf{x}_{2, T_2^*+1}
   \end{pmatrix}, \ldots, \begin{pmatrix}
     \mbf{x}_{n+1, T_{n+1}^*} \\
      \mbf{x}_{n+1, T_{n+1}^*+1}
   \end{pmatrix} \right\}.
 \end{align*}
%It is a reasonable assumption that the pool of time series that we are considering 
%should be similar to the time series of interest. 
Our weighted-adjustment estimator will therefore perform well when the pool of 
disparate time series posses similar covariates to the time series for which 
no post-shock responses are observed. We compute $\mbf{W}^*$ as
\begin{align}
  \mbf{W}^* = \argmin_{\mbf{W}\in \mc{W}} \norm{\mrm{vec}\big(\mbf{X}_1-\hat{\mbf{X}}_1(\mbf{W})\big)}_{2p}. 
  \label{W}
\end{align}
%which is a more general form of $\mbf{W}^*$ for the case when $\mbf{X}_1$ does not fall in the convex hull of (\ref{convexhull}). 
\cite{abadie2010synthetic} commented that we can select $\mbf{W}^*$ 
so that (\ref{SCM}) holds approximately %so that the property of $\mbf{W}^*$ should 
%still hold approximately 
and that weighted-adjustment estimation techniques of this form are not 
appropriate when the fit is poor. 
Note that $\mbf{W}^*$ is not random since the covariates are assumed to be fixed. Since $\mc{W}$ is a closed and bounded subset of $\reals^n$,  $\mc{W}$ is compact. Because the objective function 
is continuous in $\mbf{W}$, $\mbf{W}^*$ will always exist. %Relying on this weight, 
%we are mainly interested in constructing the following adjustment estimator:
Our weighted-adjustment estimator for the shock effect $\alpha_1$ is
  \begin{align*}
    \hat{\alpha}_{\rm wadj} = \sum_{i=2}^{n+1} w_i^*\hat{\alpha}_i
    \quad \text{ for } \quad \mbf{W}^* = \begin{pmatrix}
      w^*_2 & \cdots & w^*_{n+1}
    \end{pmatrix}.
  \end{align*}
 %Different from \citet{abadie2010synthetic}, our construction customizes synthetic control methods to the setting of AR(1) model. To be more specific, notice that (\ref{SCM}) implicitly uses the covariates at $T_*$ the shock-time and $T_*-1$ the time point just before the shock. This model somehow accounts for the impact of past information on the shock-effects. 
 We further define
\begin{align*}
  \mathbf{V} = \Big(\mrm{vec}\big((\mathbf{x}_{2, T_2^*}, \mathbf{x}_{2, T_2^*+1})\big), \ldots, \mrm{vec}\big((\mathbf{x}_{n+1, T_2^*}, \mathbf{x}_{n+1, T_2^*+1})\big)\Big).
\end{align*}
\begin{prop}
  \label{uniqueness} If $\mathbf{V}$ has full rank and it exists some $\mathbf{W}$ satisfies (\ref{SCM}), the solution to  (\ref{W}) is unique.
\end{prop} 
Proposition \ref{uniqueness} details some conditions when $\mathbf{W}^*$ is unique.  Note that $\mathbf{V}$ is $2p \times n$. Therefore, if the covariates are of full rank and the true solution lies in the convex and compact $\mathcal{W}$, a sufficient condition for $\mathbf{W}^*$ to be unique is $2p \geq n$. However, when $2p < n$, $\mathbf{W}^*$ may not be unique. If it exists some $\mathbf{W}^*$ satisfies (\ref{SCM}) and $2p < n$, there are infinitely many solutions to (\ref{SCM}).  The issue of non-uniqueness is further discussed in Section \ref{discussion}.

\begin{remark}
% Though in Section \ref{modelsetup} we assume $\mbf{x}_{i,t}\in \reals^p$ and bases construction for $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$  on this fact, we will shortly show that the covariates need not be of the same dimension across disparate time series. 
In Section \ref{modelsetup} we specify that $\mbf{x}_{i,t}, \theta, \beta \in \reals^p$. 
 % We can insert $\mathbf{0}$ into the covariates of disparate time series for the parts that they do not share. For example, suppose $\mathbf{x}_{2, t}, \mathbf{x}_{2, t-1}\in \reals^{p-1}$ and $\mathbf{x}_{3, t}, \mathbf{x}_{3, t-1}\in \reals^p$ are the covariates for time series 2 and 3. Assume that the $p$th column of $\mathbf{x}_{3, t}, \mathbf{x}_{3, t-1}$ is the part they do not share. In this case, we can let $\mathbf{x}^{adj}_{2, t}=(\mathbf{x}_{2, t}, \mathbf{0})$, $\mathbf{x}^{adj}_{2, t-1}=(\mathbf{x}_{2, t-1}, \mathbf{0})$ to satisfy the $p$-dimension requirement. 
However, it is not necessary that the all $p$ covariates are important for every time series under study. The regression coefficients $\theta$ and $\beta$ are nuisance parameters that are not of primary importance. 
% In this regard, OLS estimations in software can reparameterize the design matrix.  Of course, the column space of the design matrix does not change compared to the one in original dimension. Thus, the least squares estimators do not change. In other words, OLS estimation need no adjustment. The generalization may only apply to the construction of weighted-adjustment  estimator $\hat{\alpha}_{\rm wadj}$.
It will be understood that structural 0s in $\mbf{x}_{i,t}$ correspond to variables that are unimportant. 
\end{remark}



\section{Forecast risk and properties of shock-effects estimators}
\label{properties}

In this section, we discuss the properties that are related to forecast-risk reduction. In discussion of risk, it is useful to derive expressions for expectation and variance of the adjustment estimator $\hat{\alpha}_{\rm adj}$ and weighted-adjustment estimator.  The expression for the expectations are as follow,

 \begin{enumerate}[label = (\roman*)]
    \item Under $\mc{M}_{1}$, $\E{\hat{\alpha}_{\rm adj}}=\E{\hat{\alpha}_{\rm wadj}} = \mu_{\alpha}$.
    \item Under $\mc{M}_{21}$, 
    \begin{align*}
      \E{\hat{\alpha}_{\rm adj}} = \mu_{\alpha} + \frac{1}{2} \sum_{i=2}^{n+1} \delta' \mbf{x}_{i, T_i^*+1} + \frac{1}{n} \sum_{i=2}^{n+2} \gamma' \mbf{x}_{i, T_i^*} 
      \quad \text{ and } \quad 
       \E{\hat{\alpha}_{\rm wadj}} = \mu_{\alpha} + \delta'\mbf{x}_{1, T_1^*+1} + \gamma' \mbf{x}_{1, T_1^*}.
    \end{align*}
    \item Under $\mc{M}_{22}$,
    \begin{align*}
      \E{\hat{\alpha}_{\rm adj}} = \mu_{\alpha} + \frac{1}{2} \sum_{i=2}^{n+1} \mu_{\delta}' \mbf{x}_{i, T_i^*+1} + \frac{1}{n} \sum_{i=2}^{n+2}  \mu_{\gamma}'\mbf{x}_{i, T_i^*} 
      \quad \text{ and } \quad 
       \E{\hat{\alpha}_{\rm wadj}} = \mu_{\alpha} + \mu_{\delta}'\mbf{x}_{1, T_1^*+1} + \mu_{\gamma}' \mbf{x}_{1, T_1^*}.
    \end{align*}
  \end{enumerate}
Formal justification for these results can be found in Appendix. Note that $\hat{\alpha}_{\rm adj}$, $\hat{\alpha}_{\rm wadj}$, and $\hat{\alpha}_{\rm IVW}$ are not unbiased estimators for $\alpha_1$. Notice that under $\mc{M}_{1}$, $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm adj}$ are unbiased estimators for $\E{\alpha_1}=\mu_{\alpha}$ (see distributional details of $\alpha_1$ in Section \ref{modelsetup}). Nevertheless, $\hat{\alpha}_{\rm adj}$ is a biased estimator for $\E{\alpha_1}$ but $\hat{\alpha}_{\rm wadj}$ is an unbiased estimator for $\E{\alpha_1}$ under both $\mc{M}_{21}$ and $\mc{M}_{22}$. Thus, we collect these results  as the following proposition. 

\begin{prop}
\label{unbiased} 
\quad 
\begin{enumerate}[label = (\roman*)]
  \item Under $\mc{M}_1$, $\hat{\alpha}_{\rm adj}$ is an unbiased estimator of $\E{\alpha_1}$. Under $\mc{M}_{21}$ and $\mc{M}_{22}$, $\hat{\alpha}_{\rm adj}$ is a biased estimator of $\E{\alpha_1}$ in general.
  \item Suppose that $\mbf{W}^*$ satisfies (\ref{SCM}). Under $\mc{M}_{1}$, $\mc{M}_{21}$ and $\mc{M}_{22}$, $\hat{\alpha}_{\rm wadj}$ is an unbiased estimator of $\E{\alpha_1}$.
\end{enumerate}
\end{prop}


Unbiasedness properties for $\E{\alpha_1}$ of $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ allow for simple risk-reduction conditions and invoke a method of comparison, although our primary interest is in reducing forecast risk. % for $\hat{\alpha}_{\rm adj}$ and  $\hat{\alpha}_{\rm wadj}$ to reduce risk, and make it more clear with respect to when one is better than the other. 
These conditions will be discussed in Section \ref{conditions} and Section \ref{comparisons}. Next, we present the variance expressions for $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ as below

\begin{enumerate}[label = (\roman*)]
  \item Under $\mc{M}_1$ and $\mc{M}_{21}$,  
\begin{align*}
  \var{\hat{\alpha}_{\rm adj}} 
  &=\frac{\sigma^2}{n^2}\sum_{i=2}^{n+1}\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\frac{\sigma^2_{\alpha}}{n^2}\\
\var{\hat{\alpha}_{\rm wadj}}  &= \sigma^2\sum_{i=2}^{n+1}(w_i^*)^2\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\sigma^2_{\alpha}\sum_{i=2}^{n+1}(w_i^*)^2
\end{align*}
\item Under $\mc{M}_{22}$, 
\begin{align*}
\var{\hat{\alpha}_{\rm adj}} 
  &=\frac{\sigma^2}{n^2}\sum_{i=2}^{n+1}\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\frac{1}{n^2}\var{\alpha_i}\\
  \var{\hat{\alpha}_{\rm wadj}} 
  &= \sigma^2\sum_{i=2}^{n+1}(w_i^*)^2\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}
  + \sum_{i=2}^{n+1} (w_i^*)^2 \var{\alpha_i}.
\end{align*}
\end{enumerate}
Formal justification for these results can be found in Appendix. Note that the variances are not comparable in closed-form %we shall see that it is difficult to compare variances between $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ 
because of the term $\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}$.  This term exists because of the inclusion of the random lagged response in our auto regressive model formulation.  Under $\mc{M}_{22}$, the expression for $\var{\alpha_i}$ is not of closed form because $\gamma_i$ and $\delta_i$ may be dependent when they are placed in a random-effects model. We investigate comparisons between the variability of these estimators in Section \ref{comparisons}.%, we will show under some circumstances, it is possible to compare the variances for $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$. 



As Section \ref{conditions} and \ref{comparisons} detailed the conditions for risk-reduction and comparisons, they usually involve fixed quantities related to variance and expectation. To make use of those properties in practice, estimation is required. Section \ref{varbootstrap} will introduce a general procedure of parametric bootstrap under the context of the problem to attain this purpose.


\subsection{Conditions for risk-reduction for shock-effects estimators}
\label{conditions}

In this section we will discuss the conditions for risk reduction for individual shock-effects estimators under $\mc{M}_1$, $\mc{M}_{21}$, and $\mc{M}_{22}$. %The reason is that if the estimator of $\alpha_1$ turns out to be an unbiased estimator of $\E{\alpha_1}$, the conditions get  simplified. 

\subsubsection{Conditions under $\mc{M}_1$}
 \label{conditionsmodel1}
 
Recall that Proposition \ref{unbiased} implies that the adjustment estimator $\hat{\alpha}_{\rm adj}$ and weighted-adjustment estimator $\hat{\alpha}_{\rm wadj}$ are unbiased for $\E{\alpha_1}$ under $\mc{M}_1$. With this result, we will have  the following propositions that specify the conditions that are necessary for risk reduction. 

\begin{prop}
\label{proprisk}Under $\mc{M}_1$,
\begin{enumerate}[label = (\roman*)]
  \item  $R_{T_1^*+1, 2} < R_{T_1^*+1, 1}$ when 
$\Var(\hat{\alpha}_{\rm adj}) < \mu_{\alpha}^2$.
  \item if $\mbf{W}^*$ satisfies (\ref{SCM}), $R_{T_1^*+1,2}<R_{T_1^*+1,1}$ when $\var{\hat{\alpha}_{\rm wadj}}<\mu_{\alpha}^2$. 
\end{enumerate}
\end{prop}

Proposition \ref{proprisk} tells that under $\mc{M}_1$ if the variance of the estimator is smaller than the squared mean of $\alpha_1$, those estimators will enjoy the risk reduction properties. Recalling from variance expression at the beginning of Section \ref{properties}, Proposition \ref{proprisk} shows that the risk-reduction condition is
\begin{align}
  \var{\hat{\alpha}_{\rm adj}} 
  &=\frac{\sigma^2}{n^2}\sum_{i=2}^{n+1}\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\frac{\sigma^2_{\alpha}}{n^2} < \mu_{\alpha}^2 \label{riskconditionadj}
\end{align}

In terms of the adjustment estimator, $\hat{\alpha}_{\rm adj}$, (\ref{riskconditionadj}) implies two facts: (1) Forecast 2 is preferable to Forecast 1 asymptotically in $n$ 
whenever $\mu_{\alpha} \neq 0$; (2) In finite pool of time series, Forecast 2 is preferable to 
Forecast 1 when the $\mu_{\alpha}$ is large relative to its variability and 
overall regression variability.   %This result is also intuitive. For example, if  $\mu_{\alpha}$ is large, Forecast 1 will definitely work poor because large  $\mu_{\alpha}$ implies shock effects that cannot be ignored.  See Section \ref{forecast} for forecast formulation for details.

For the weighted-adjustment estimator $\hat{\alpha}_{\rm wadj}$, if  $\mathbf{W}^*$ does not satisfy (\ref{SCM}), its unbiased properties for $\E{\alpha_1}$ should hold approximately when the fit in (\ref{W}) is appropriate as commented in Section \ref{constructionofestimators}. From Proposition \ref{proprisk} and variance expression of $\hat{\alpha}_{\rm wadj}$, the following is the risk-reduction condition for $\hat{\alpha}_{\rm wadj}$.
\begin{align*}
\var{\hat{\alpha}_{\rm wadj}}
 = \sigma^2\sum_{i=2}^{n+1}(w_i^*)^2\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\sigma^2_{\alpha}\sum_{i=2}^{n+1}(w_i^*)^2 < \mu_{\alpha}^2.
\end{align*}
In this case, Forecast 2 is preferable to Forecast 1 when $\mu_{\alpha}$ is large relative to the \emph{weighted sum of variances for shock effects for other time series} and overall regression variability.  However, the above criteria are generally difficult to evaluate in practice due to the term $\hat{\alpha}_{\rm wadj}$. Section  \ref{varbootstrap} will provide a detailed treatment about how to deal with these technical inequalities in practice.


\subsubsection{Conditions under $\mc{M}_{21}$ and $\mc{M}_{22}$}
\label{conditionsm2122}
The $\alpha_i$s have different means under $\mc{M}_{21}$ and $\mc{M}_{22}$ unlike under $\mc{M}_1$. %It is a more reasonable and general model since it is often the case that shock-effects differ by means among disparate time series in practice. $\mc{M}_{22}$  further adds the random effect structure of $\delta$ and $\gamma$ to  $\mc{M}_{21}$ for generalization.
However, Proposition \ref{unbiased} implies that $\hat{\alpha}_{\rm wadj}$ is an unbiased estimator of $\E{\alpha_1}$. %From Proposition \ref{varprop}, the risk-reduction condition for $\hat{\alpha}_{\rm wadj}$  will be as in Proposition \ref{propriskwadj2}. 
We now state conditions for risk reduction.

\begin{prop}
\label{propriskwadj2} If $\mbf{W}^*$ satisfies (\ref{SCM}), under $\mc{M}_{21}$ and $\mc{M}_{22}$, $R_{T_1^*+1,2}<R_{T_1^*+1,1}$ when $\var{\hat{\alpha}_{\rm wadj}}<(\E{\alpha_1})^2$. 
\end{prop}

Based on Proposition \ref{propriskwadj2}, we can obtain a similar inequality as in Section \ref{conditionsmodel1} as below
\begin{align*}
\var{\hat{\alpha}_{\rm wadj}}
 = \sigma^2\sum_{i=2}^{n+1}(w_i^*)^2\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\} + \sum_{i=2}^{n+1} (w_i^*)^2 \var{\alpha_i} < (\E{\alpha_1})^2,
\end{align*}
where $\var{\alpha_i}$ may be replaced with $\sigma^2_{\alpha}$ in $\mc{M}_{21}$. The conclusions and intuitions will be identically the same as what we have in Section \ref{conditionsmodel1}. 


Proposition \ref{unbiased} shows that $\hat{\alpha}_{\rm adj}$ is a biased estimator of $\E{\alpha_1}$ under $\mc{M}_{21}$ and $\mc{M}_{22}$ generally. Hence, Proposition \ref{proprisk} no longer  holds for $\hat{\alpha}_{\rm adj}$ under $\mc{M}_{21}$ and $\mc{M}_{22}$. But, as an alternative, we can derive similar conditions as below. By Lemma \ref{risklemma} (see Section \ref{proofs}) and risk decomposition, we will achieve risk-reduction as long as
\begin{align*}
 \E{\alpha_1^2}= \var{\alpha_1}+(\E{\alpha_1})^2
 &>\E{\hat{\alpha}_{\rm adj}-\alpha_1}^2\\
  &=\var{\hat{\alpha}_{\rm adj}} +  (\E{\hat{\alpha}_{\rm adj}}-\alpha_1)^2 \\
  &=\var{\hat{\alpha}_{\rm adj}} +  \var{\alpha_1} + (\E{\hat{\alpha}_{\rm adj}}-\E{\alpha_1})^2
\end{align*}
Therefore, the above inequality will simply to 
\begin{align*}
 (\E{\alpha_1})^2
 &>\var{\hat{\alpha}_{\rm adj}}+ (\E{\hat{\alpha}_{\rm adj}}-\E{\alpha_1})^2.
\end{align*}
Note that since $\hat{\alpha}_{\rm adj}$ is biased for $\E{\alpha_1}$, the bias term $(\E{\hat{\alpha}_{\rm adj}}-\E{\alpha_1})^2$ will become complicated and  simplification yields no insightful results. 

As mentioned in Section \ref{constructionofestimators}, it is difficult to evaluate the expectation and variance of $\hat{\alpha}_{\rm IVW}$. In other words, $\hat{\alpha}_{\rm IVW}$ is generally biased for $\E{\alpha_1}$. That is to say we can adapt the above proof to derive the risk-reduction conditions for $\hat{\alpha}_{\rm IVW}$: under $\mc{M}_{1}$, $\mc{M}_{21}$, and $\mc{M}_{22}$, $R_{T_1^*+1,2}<R_{T_1^*+1,1}$ when $\var{\hat{\alpha}_{\rm IVW}} +(\E{\hat{\alpha}_{\rm IVW}}-\E{\alpha_1})^2<(\E{\alpha_1})^2$.

Topics of  evaluation of these inequalities in practice can be found in Section \ref{varbootstrap}. 
We will discuss comparisons of adjustment estimators in the the next Section.
%As conditions of risk-reduction for individual estimators are obtained, a natural question arises --- which one is better? It will be discussed in Section \ref{comparisons}.



 

\subsection{Comparisons among estimators}

\label{comparisons}

In comparing  shock-effects estimators, we would assume that the risk-reduction conditions are satisfied as in Section \ref{conditions}.  

Denote the risk-reduction quantity for the adjustment estimator as $\Delta_{\rm adj}$, the one for inverse-weighted estimator as $\Delta_{\rm IVW}$, and the one for weighted-adjustment estimator as $\Delta_{\rm wadj}$. As long as the risk-reduction of one estimator is greater than those of others, we will vote it as the best estimator among our pool of estimators for consideration. For example, if we find that $\Delta_{\rm wadj}>\Delta_{\rm adj}$ and $\Delta_{\rm wadj}>\Delta_{\rm IVW}$, the weighted-adjustment estimator $\hat{\alpha}_{\rm wadj}$ is the most favorable.


According to  discussion in Section \ref{conditionsm2122}, we know that under $\mc{M}_{1}$, $\mc{M}_{21}$, and $\mc{M}_{22}$, the risk-reduction quantity for $\hat{\alpha}_{\rm IVW}$ is
\begin{align*}
  \Delta_{\rm IVW} = (\E{\alpha_1})^2- \var{\hat{\alpha}_{\rm IVW}} -(\E{\hat{\alpha}_{\rm IVW}}-\E{\alpha_1})^2.
\end{align*}
From discussions in Section \ref{conditions}, we know that the risk-reduction quantities for $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ differ across models, we will discuss in different cases accordingly.


\subsubsection{Under $\mc{M}_{1}$}

From Proposition \ref{proprisk}, we know that the risk-reduction quantities for $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ are
\begin{align*}
  \Delta_{\rm adj} 
  = \mu_{\alpha}^2 -\var{\hat{\alpha}_{\rm adj}}
\quad   \text{ and } \quad 
  \Delta_{\rm wadj} 
  = \mu_{\alpha}^2-\var{\hat{\alpha}_{\rm wadj}}.
\end{align*}
Under the framework of $\mc{M}_1$, the risk-reduction quantity  for $\hat{\alpha}_{\rm IVW}$ is
\begin{align*}
  \Delta_{\rm IVW} = \mu_{\alpha}^2- \var{\hat{\alpha}_{\rm IVW}} -(\E{\hat{\alpha}_{\rm IVW}}-\mu_{\alpha})^2.
\end{align*}
In other words, when $\var{\hat{\alpha}_{\rm wadj}}<\var{\hat{\alpha}_{\rm adj}}$ and $\hat{\alpha}_{\rm wadj} < \var{\hat{\alpha}_{\rm IVW}} +(\E{\hat{\alpha}_{\rm IVW}}-\mu_{\alpha})^2$, we would prefer $\hat{\alpha}_{\rm wadj}$ as the best estimator. Other conditions for voting the other estimators as the best one follow similarly. 

\subsubsection{Under $\mc{M}_{21}$ and $\mc{M}_{22}$}
\label{section322}
According to Proposition \ref{propriskwadj2} and the discussion in Section \ref{conditionsm2122}, the risk-reduction quantities $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ are
\begin{align*}
  \Delta(\hat{\alpha}_{\rm adj})
  = (\E{\alpha_1})^2 -\var{\hat{\alpha}_{\rm adj}} -(\E{\hat{\alpha}_{\rm adj}}-\E{\alpha_1})^2
\quad   \text{ and } \quad 
  \Delta(\hat{\alpha}_{\rm wadj})
  = (\E{\alpha_1})^2-\var{\hat{\alpha}_{\rm wadj}}.
\end{align*}
In this case, the risk-reduction quantity for $\hat{\alpha}_{\rm adj}$ is similar to that of $\hat{\alpha}_{\rm IVW}$ since they are both biased for $\E{\alpha_1}$.  Thus,
\begin{align*}
 \Delta(\hat{\alpha}_{\rm IVW})
  = (\E{\alpha_1})^2 -\var{\hat{\alpha}_{\rm IVW}} -(\E{\hat{\alpha}_{\rm IVW}}-\E{\alpha_1})^2
\end{align*}



For the case of $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ , we can derive the following inequality for $\hat{\alpha}_{\rm wadj}$ to be favored over $\hat{\alpha}_{\rm adj}$.
 \begin{align*}
  \var{\hat{\alpha}_{\rm adj}} 
  -\var{\hat{\alpha}_{\rm wadj}} + \big(\E{\hat{\alpha}_{\rm adj}}-\mrm{E}(\alpha_1)\big)^2> 0.
\end{align*}
 
%What insights can we gain from this inequality? 
We analyze this inequality from two perspectives. 
 \begin{enumerate}
   \item If it turns out to be fact that the variance of the weighted-adjustment estimator is greater than that of adjustment estimator, we should be aware that  the compromise for variance because of using $\hat{\alpha}_{\rm wadj}$ shouldn't exceed the squared bias, i.e., $\big(\E{\hat{\alpha}_{\rm adj}}-\mrm{E}(\alpha_1)\big)^2$.
   \item If instead the variance of $\hat{\alpha}_{\rm wadj}$ is smaller than that of $\hat{\alpha}_{\rm adj}$, the above inequality should always hold because $\big(\E{\hat{\alpha}_{\rm adj}}-\mrm{E}(\alpha_1)\big)^2>0$ under $\mc{M}_{21}$ and $\mc{M}_{22}$. 
 \end{enumerate}
  
 These are some analytical results for comparison studies among estimators of $\alpha_1$. Next, we will detail a framework for estimation of risk-reduction  quantities using a parametric bootstrap routine. Therefore, the above inequalities can be analyzed numerically and risk-reduction quantities can be estimated using plug-in estimators in practice.

\begin{remark}
  In Section \ref{constructionofestimators}, we noted that $\mathbf{W}^*$ may not be unique if $2p < n$. However, Proposition \ref{proprisk} and \ref{propriskwadj2} will hold for  every $\hat{\alpha}_{\rm wadj}$ using  $\mathbf{W}^*$ that satisfies (\ref{SCM}).
\end{remark}


\subsection{Bootstrap for risk-reduction evaluation problems}
\label{varbootstrap}


In this section, we present a bootstrap procedure that approximates the distribution of our shock-effect estimators, checks the underlying conditions of our risk reduction propositions, and estimate risk-reduction using plug-in approach in practice. Our procedure involves the resampling of residuals in the separate OLS fits. This procedure has its origins in Section 6 of \citet{efron1986bootstrap}, and it involves the resampling the residuals which are assumed to be the realizations of an iid processes.
%NO NEED TO OUTLINE IN THIS WAY: In the first step, we will introduce the pros and cons of this algorithm and refer the users to other alternatives. In the second step, we detail the procedure of boostrapping AR(1).
%%\citet{efron} introduces the bootstrapping method to approximate the distribution of a statistic using a \emph{random sample}. However, this method will fail for time-series data since serial correlation or time-dependent features exist in typical time-series model. 
%In the AR($p$) model, the unobserved errors are assumed to be identically and independently distributed. In this setting, standard bootstrap methodology can be applied to resampling the residuals \citep{efron1986bootstrap}. 
\citet{bose1988edgeworth} showed that the asymptotic accuracy for OLS parameter estimation can be further improved from $O(T^{-1/2})$ to $o(T^{-1/2})$ almost surely under some regularity conditions.

To specify, there are two possible bootstrap procedures for our methods, one based on resampling the donor pool with replacement (\emph{unconditional} on donor pool) and the other without any resampling of the donor pool (\emph{conditional} on donor pool). We denote them by $\mc{B}_u$ and  $\mc{B}_c$, respectively. We first illustrate the simple $\mc{B}_c$. The formal steps of it are outlined in the Supplementary Materials, the intuition for this procedure is as follows: let $B$ be the bootstrap sample size and let $I=\{2, \ldots, n +1\}$ be the indexes for donor pool. Initialize $y_{i,0}$ for all $i \in I$. At iteration $b$, resample the residuals and then obtain shock-effect estimators for each of the disparate time series for all $i \in I$. Then construct and store any of the adjustment estimators $\hat{\alpha}^{(b)}_{\mrm{adj}}$, $\hat{\alpha}^{(b)}_{\mrm{wadj}}$, and $\hat{\alpha}^{(b)}_{\mrm{IVW}}$. We can then estimate distributional quantities of our shock-effect estimators with the bootstrap sample of $\hat{\alpha}^{(b)}_{\mrm{adj}}$, $\hat{\alpha}^{(b)}_{\mrm{wadj}}$, and $\hat{\alpha}^{(b)}_{\mrm{IVW}}$, for $b = 1,\ldots,B$.
%The errors $\varepsilon_{i,t}$ and $\varepsilon_{i,t'}$ for $t \in \{1, \ldots, T_i^*\}$ and $t' \in \{T_i^*+1, \ldots, T_i\}$ are iid. 
%since $y_{i,t}$ differs from $y_{i,t'}$ only in the shock-effect $\alpha_i$ conditioned on $\Theta$, and it is not absorbed into $\varepsilon_{i,t}$ and $\varepsilon_{i,t'}$. Therefore, this parametric bootstrap framework will work for us.
One can then use the bootstrapped shock-effects estimated by our residual bootstrap to provide an approximation for parameters involved in the risk-reduction conditions in Propositions \ref{proprisk} and \ref{propriskwadj2}. $\mc{B}_u$ is the same as above with the addition of first round of sampling $n$ values from $I=\{2, \ldots, n+1\}$ without replacement to generate  $I^{(b)}$. Then, it follows the same step with the one conditioned on the donor pool by replacing $I$ by $I^{(b)}$. 

%When the rank of the design matrix corresponding to the mean function of $\alpha_i$ under $\mc{M}_2$ is greater than $n$ then our bootstrap procedure will work fine, however, 

Now, we discuss some consistency results for $\mc{B}_c$ conditioned on $\Theta_1$. The bootstrap is aimed for estimating $\mrm{Var}(\hat{\alpha}_{\rm adj})$, $\mrm{Var}(\hat{\alpha}_{\rm wadj})$, and $\mrm{Var}(\hat{\alpha}_{\rm IVW})$. The consistency results will be similar to the counterparts in sample mean. Therefore, we will briefly discuss the proof for the sample mean case and adapt it to the variance case. Let 
$
  \overline{\hat{\alpha}_{\mrm{adj}}} 
    = \frac{1}{B} \sum_{b=1}^B \sum_{i=2}^{n+1}\hat{\alpha}_i^{(b)}
$, 
$
  \overline{\hat{\alpha}_{\mrm{wadj}}} 
    = \frac{1}{B} \sum_{b=1}^B \sum_{i=2}^{n+1}w_i^* \hat{\alpha}_i^{(b)}
$, 
and 
$
  \overline{\hat{\alpha}_{\mrm{IVW}}} 
    = \frac{1}{B} \sum_{b=1}^B \sum_{i=2}^{n+1}\hat{\alpha}_{{\rm IVW}, i}^{(b)}.
$
The almost-sure convergence results for $\hat{\alpha}_{\mrm{adj}}$ and $\hat{\alpha}_{\mrm{wadj}}$ follow naturally, provided that the regularity conditions outlined in \citet{bose1988edgeworth} hold. The proof for the case of $\overline{\hat{\alpha}_{\mrm{wadj}}}$, the sample mean of the bootstrapped weighted adjustment estimator, can be presented as below. As $B \to \infty$,
\begin{align*}
  \overline{\hat{\alpha}_{\mrm{wadj}}}
  = \frac{1}{B} \sum_{b=1}^B \sum_{i=2}^{n+1}w_i^* \hat{\alpha}_i^{(b)}
  = \sum_{i=2}^{n+1}w_i^* \frac{1}{B} \sum_{b=1}^B\hat{\alpha}_i^{(b)}
  \overset{a.s.}{\rightarrow} \sum_{i=2}^{n+1}w_i^* \alpha_i 
  =\E{\hat{\alpha}_{\rm wadj}|\Theta}.
\end{align*}
The same rationale holds for the adjustment estimator as well since it is a linear combination of OLS estimates, and the weights are not random conditioned on $\Theta$. However, the case for $\hat{\alpha}_{\rm IVW}$ is slightly different because its weights are random. It is not clear with respect to whether similar consistency results holds for $\hat{\alpha}_{\rm IVW}$. We would only claim that our bootstrap procedure provides an approximation for the case of $\hat{\alpha}_{\rm IVW}$.





We stress that the above approximation is conditioned on $\Theta$ and for $\mc{B}_c$, and that bootstrapping cannot alleviate the inherent bias of using our adjustment estimators as estimates for $\alpha_1$. %Under a random-effect structure, every true shock effect of a time series in reality isan observed value from a population. For example, $\overline{\hat{\alpha}_{\rm wadj}}\overset{a.s.}{\rightarrow}\E{\hat{\alpha}_{\rm wadj}|\Theta}$ and $\overline{\hat{\alpha}_{\rm wadj}} \not\overset{a.s.}{\rightarrow}\E{\alpha_1}$ but $\E{\E{\hat{\alpha}_{\rm wadj}|\Theta}}=\E{\alpha_1}$. 
For $\mc{B}_u$ but conditioned on $\Theta$, we emphasize that the $\mathbf{W}^*$ from SCM method will become random, and that the consistency results may not hold. Simulation for justification of the parametric bootstrap is provided in Section \ref{parametricbootstrapsimulation}.

Recall that  $\hat{\alpha}_{\rm wadj}$ and $\hat{\alpha}_{\rm IVW}$ are unbiased estimators of their expectations, and $\hat{\alpha}_{\rm wadj}$ is an unbiased estimator of $\E{\alpha_1}$ from Proposition \ref{unbiased}. Given the bootstrapped estimates of the variance, we can estimate the risk-reduction quantities in Section \ref{comparisons} using the estimates from the parametric bootstrap. For example, the $\Delta_{\rm adj}$ and its estimator $\hat{\Delta}_{\rm adj}$ are
\begin{align*}
  \Delta(\hat{\alpha}_{\rm adj})
  &= (\E{\alpha_1})^2 -\var{\hat{\alpha}_{\rm adj}} -(\E{\hat{\alpha}_{\rm adj}}-\E{\alpha_1})^2 \\
  \hat{\Delta}(\hat{\alpha}_{\rm adj})
 & = (\hat{\alpha}_{\rm wadj})^2 -S^2_{\hat{\alpha}_{\rm adj}} -(\hat{\alpha}_{\rm adj}-\hat{\alpha}_{\rm wadj})^2,
\end{align*}
where $S^2_{\hat{\alpha}_{\rm adj}}$ is the bootstrap sample variance for $\hat{\Delta}(\hat{\alpha}_{\rm adj})$. 

Suppose we have a pool of shock-effect estimators, $\mc{A}$, e.g., $\mc{A}= \{\hat{\alpha}_{\rm adj}, \hat{\alpha}_{\rm wadj}, \hat{\alpha}_{\rm IVW}\}$ in our study. We can determine which one is the best shock-effect estimator. Suppose the $\hat{\Delta}(\hat{\alpha}_{\rm wadj})$ and $\hat{\Delta}(\hat{\alpha}_{\rm IVW})$ are estimators for $\Delta(\hat{\alpha}_{\rm wadj}) $ and $\Delta(\hat{\alpha}_{\rm IVW})$, respectively. Then, we can evaluate the risk-reduction propositions by judging whether $\hat{\Delta}(\hat{\alpha})>0$ for $\hat{\alpha}\in \mc{A}$. Besides, we can also select the best shock-effect estimator accordingly. Define
\begin{align}
	\hat{\alpha}_{\rm best} = \argmax_{\hat{\alpha}\in \mc{A}} \hat{\Delta}(\hat{\alpha}) \quad \text{ and } \quad  \alpha_{\rm best} = \argmax_{\hat{\alpha}\in \mc{A}} \Delta(\hat{\alpha}). \label{best}
\end{align}
That is, the best estimator $\hat{\alpha}_{\rm best}$ is the one with the maximum estimated risk-reduction quantities whereas $\alpha_{\rm best}$ is the true best shock-effect estimator given observed post-shock response. See more discussions in Section \ref{loocv}.



In terms of the use of our bootstrap procedure, we caution that the bootstrapping residuals in OLS estimation may not provide valid inference in moderate or high dimension where $p < T_i$ but $p / T_i$ is not close to zero for $i\in \{2, \ldots, n+1\}$ \citep{el2018can}; see alternatives for residual bootstrapping in linear models in \citet{el2018can}.

Recall that $\mathbf{W}^*$  may not be unique if the conditions in Proposition \ref{uniqueness} are not satisfied. Under the setup of our model, non-uniqueness of $\mathbf{W}^*$ would not be a problem for inferential purposes. It is because all risk-reduction propositions and other properties established will still hold. However, non-uniqueness may not be desirable in other model setup. For example, consider the model where $\alpha_i$ is assumed to be not identically distributed, the size of  donor pool to be 2, and there are two solutions to (\ref{SCM}), say, $\mathbf{W}^*_1=(1,0)$ and  $\mathbf{W}^*_2=(0,1)$ with $\var{\alpha_2}\neq \var{\alpha_3}$. In this scenario, the procedure of trying to recover $\alpha_1$ will fail since the resulting $\alpha_1$ will be different with different variances for $\mathbf{W}^*_1=(1,0)$ and  $\mathbf{W}^*_2=(0,1)$. If the non-uniqueness is of concern, users may select the weight that optimizes some objective function. For instance, in the example just discussed, users may select the weight that minimizes the estimated variance of $\hat{\alpha}_{\rm wadj}$.

There are some issues in using $\mc{B}_u$ when $\mathbf{W}^*$ falls in the boundary of the parameter space. See detailed discussions in Section \ref{discussion}.

\subsection{Leave-one-out cross validation with $k$ random draws}
\label{loocv}

In this section, we introduce a powerful procedure for rendering prospective evaluations of the applicability of the proposed methods. First, we introduce the concept of consistency. Second, we detail the leave-one-out cross-validation with $k$ random draws to realize the estimation prospectively.

Our risk-reduction propositions methodologies can be evaluated by \emph{consistency}. Risk-reduction propositions are decision-making procedures that might commit a type of error, which is similar to Type I or Type II error in a hypothesis test. Let $\hat{\alpha}$ be an estimator of $\alpha_1$ and $\delta_{\hat{\alpha}}$ is the corresponding risk-reduction proposition. If $\Delta(\hat{\alpha})>0$ ($\Delta(\hat{\alpha})<0$, respectively) but $\delta_{\hat{\alpha}}$ incorrectly reported $\Delta(\hat{\alpha})<0$ ($\Delta(\hat{\alpha})>0$, respectively) so that it make the decision not to use $\hat{\alpha}$ (to use $\hat{\alpha}$, respectively), $\delta_{\hat{\alpha}}$ is said to be \emph{inconsistent}. If $\Delta(\hat{\alpha})<0$ ($\Delta(\hat{\alpha})>0$, respectively) and  $\delta_{\hat{\alpha}}$ correctly reported $\Delta(\hat{\alpha})<0$ ($\Delta(\hat{\alpha})>0$, respectively) so that it make the decision to use $\hat{\alpha}$, $\delta_{\hat{\alpha}}$ is said to be \emph{consistent}. These situations are depicted in the following table. 
\begin{center}
  \begin{center}
      \begin{tabular}{cc|c|c}
        \hline
        & & \multicolumn{2}{c}{Decision} \\
        & & Use $\hat{\alpha}$ & Do not use $\hat{\alpha}$ \\ 
                \hline
     \multirow{2}{*}{Truth}  & $\Delta(\hat{\alpha})>0$ & Consistent & Inconsistent\\
      \cline{3-4}
      & $\Delta(\hat{\alpha})<0$  & Inconsistent & Consistent \\
      \hline
      \end{tabular}
  \end{center}
\end{center}
Similar definitions can be adapted to the best shock-effect estimators. 

In this setting, judging whether $\delta_{\alpha}$ is consistent can be treated as a Bernoulli trial that reports 1 if $\delta_{\alpha}$ is consistent, and reports 0 if $\delta_{\alpha}$ is inconsistent. It is also true for selecting the best shock-effect estimator. It motivates us to define
\begin{align*}
	\mc{C}(\delta_{\alpha}) 
	= I(\delta_{\hat{\alpha}} \text{ is consistent})
 \quad \text{ and } \quad  	
 \mc{C}(\mc{A}) 
	= I(\hat{\alpha}_{\rm best} = \alpha_{\rm best})
\end{align*}
where $I(\cdot)$ is an indicator function, $\mc{A}$ is the pool of shock-effect estimators, $\alpha_{\rm best}$ is the true best shock-effect estimator, and $\hat{\alpha}_{\rm best}$ is the best shock-effect estimator selected by the maximum estimated risk-reduction,   see Section \ref{varbootstrap}.  Then, we can define consistency and best consistency as in Definition \ref{consistency} and \ref{best consistency}.

\begin{defn}
\normalfont 	\label{consistency}The \emph{consistency} of 	the risk-reduction proposition $\delta_{\hat{\alpha}}$ is $\E{\mc{C}(\delta_{\hat{\alpha}})}$.
\end{defn}

\begin{defn}
	\normalfont  \label{best consistency}Given $\mc{A}$ the pool of shock-effect estimators, the \emph{best consistency} is $\E{\mc{C}(\mc{A})}$.
\end{defn}
Consistency and best consistency are important parameters for users to evaluate the effectiveness of our risk-reduction propositions and estimated risk-reduction. If $\E{\mc{C}(\delta_{\hat{\alpha}})} > 0.5$, we  claim that $\delta_{\hat{\alpha}}$ is better than a random guess. If $\E{\mc{C}(\mc{A})} > 1/ |\mc{A}|$, we claim that the procedure of selecting the best shock-effect estimators is better than a random guess. Note that $\mc{C}(\delta_{\hat{\alpha}})$
 and $\mc{C}(\mc{A})$ generally can be known only if the post-shock response is observed. But it is only feasible in a retrospective finding. It is even more difficult to construct a random sample to estimate $\E{\mc{C}(\delta_{\hat{\alpha}})}$ or $\E{\mc{C}(\mc{A})}$. Nevertheless, it is possible to estimate them using the procedure \emph{leave-one-out cross validation (LOOCV)}.

Our proposed LOOCV tailors the standard LOOCV to our application. See details of standard LOOCV in Section 7.10 of \citet{hastie2009elements}.  Recall in Section \ref{modelsetup} that we are given the data $\{(\mbf{x}_{i,t}, y_{i,t}) \colon i = 1, \ldots, n+1, t = 1, \ldots, T_i\}$, where $\{(\mbf{x}_{1,t}, y_{1,t})\colon t = 1, \ldots, T_1\}$ is the data of the time series of interest and the remaining is those of the donor pool. For $m^{\rm th}$ iteration of LOOCV, where $m = \{1, \ldots, n\}$, we set aside $\{(\mbf{x}_{m + 1, t}, y_{m + 1, t}) \colon t = 1, \ldots, T_{m+1}\}$ as the time series of interest, and construct a new donor pool $\{(\mbf{x}_{i, t}, y_{i, t}) \colon i \in \mc{I}, t = 1, \ldots, T_{i}\}$, where $\mc{I}=\{2, \ldots, n+1\} \setminus \{m+1\}$. Since the post-shock response $y_{m+1, T_{m+1}^*+1}$ is observed, for $\hat{\alpha}\in \mc{A}$, we can compute the consistency and best consistency as $\mc{C} ^{(-m)}(\delta_{\hat{\alpha}})$ and $\mc{C} ^{(-m)}(\mc{A})$  using the estimation procedures in Section \ref{varbootstrap}. The the LOOCV estimates for $\E{\mc{C}(\delta_{\hat{\alpha}})}$  and $\E{\mc{C}(\mc{A})}$ are
\begin{align*}
	 \bar{\mc{C}}(\delta_{\hat{\alpha}})= \frac{1}{n} \sum_{m = 1}^n \mc{C} ^{(-m)}(\delta_{\hat{\alpha}})
	 \quad \text{ and } \quad \bar{\mc{C}}(\mc{A})= \frac{1}{n} \sum_{m = 1}^n \mc{C} ^{(-m)}(\mc{A}).
\end{align*}
Note that to satisfy $\mc{M}_{2}$ in Section \ref{modelsetup}, the candidates from donor pool has to be assumed to be mutually independent. Based on this assumption, $\bar{\mc{C}}(\delta_{\hat{\alpha}})$ and $\bar{\mc{C}}(\mc{A})$ should be almost unbiased estimates of $\E{\mc{C}(\delta_{\hat{\alpha}})}$ and $\E{\mc{C}(\mc{A})}$, respectively \citep[Page 222]{msos}.

However, LOOCV can be very computationally intensive when $n$ is large. In an analog to $k$-fold cross-validation as remedies for ordinary LOOCV, we propose a similar procedure \emph{LOOCV with $k$ random draws} to alleviate the computation burden. LOOCV with $k$ random draws differs in the selection of indices. The algorithms are outlined as follows. If $n \leq  k$, we set $k$ to be $n$. If $n > k$, we randomly sample without replacement $k$ elements from $\{1, \ldots, n\}$ to gather them into $\mc{J}$. For $m \in \mc{J}$, we set aside $\{(\mbf{x}_{m + 1, t}, y_{m + 1, t}) \colon t = 1, \ldots, T_{m+1}\}$ as the time series of interest, and construct a new donor pool $\{(\mbf{x}_{i, t}, y_{i, t}) \colon i \in \mc{I}, t = 1, \ldots, T_{i}\}$, where $\mc{I}=\{2, \ldots, n+1\} \setminus \{m+1\}$. The LOOCV with $k$ random draws estimates for $\E{\mc{C}(\delta_{\hat{\alpha}})}$  and $\E{\mc{C}(\mc{A})}$ are
\begin{align*}
	 \bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}})= \frac{1}{k} \sum_{m \in \mc{J}} \mc{C} ^{(-m)}(\delta_{\hat{\alpha}})
	 \quad \text{ and } \quad \bar{\mc{C}}^{(k)}(\mc{A})= \frac{1}{k} \sum_{m \in \mc{J}} \mc{C} ^{(-m)}(\mc{A}).
\end{align*}
Similarly, assuming the candidates in the donor pool are  mutually independent and the data  satisfy $\mc{M}_{2}$, $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}})$ and $\bar{\mc{C}}^{(k)}(\mc{A})$ should be almost unbiased estimates of $\E{\mc{C}(\delta_{\hat{\alpha}})}$ and $\E{\mc{C}(\mc{A})}$, respectively. The estimation bias should decrease as $k \to n$.  In practice, $k=5$ or $k=10$ can be typical choices for LOOCV with $k$ random draws. 
 


\section{Simulation}

\label{simulation}


In this section, we provide justification for our methods based on Monte Carlo simulation examples. We implemented our simulation based on $\mc{M}_{22}$ with negligibly small $\Sigma_{\gamma}$ and $\Sigma_{\delta}$ approximating the design of $\mathcal{M}_{21}$.  We consider $p=13$ and $\mu_{\alpha}=2$, where $p = 13$ is set to satisfy conditions in Proposition \ref{uniqueness}. Parameter setup of our simulations is detailed as below. $\phi_i$'s are sampled independently from $\mrm{Uniform}(0,1)$. We  sampled $T_i$'s  independently from  $\Gamma(15, 10)$ that are further rounded to integers. If it exists some  $i$ such that  $T_i < 90$, we let $T_i=90$ instead.  Moreover, $T_i^*$ is randomly selected from $2p + 4$ to $T_i-1$. Those are set up to satisfy a necessary condition for the design matrix of OLS estimation to have full rank. Moreover, it is also designed to illustrate the performance of $\hat{\alpha}_{\rm IVW}$ that may perform well in time series with varying lengths. Additionally, we generated the covariates from $\Gamma(1,2)$ to set up a setting when the $\hat{\alpha}_{\rm wadj}$ may perform well. Last, we set $\gamma_i, \delta_i\simiid  \normal{1}{0.5}$ and $\theta_i, \beta_i \sim \normal{0}{1}$.



In this experiment, we consider parameter setup by varying $\sigma$ in the model of $y_{i,t}$, $n$, the donor pool size, and $\sigma_{\alpha}$ in the model of $\alpha_i$. We used $30$ replications of Monte Carlo simulations and $B = 200$ for computation; and report means and standard errors for estimated quantities accordingly. $k=5$ is set up for LOOCV with $k$ random draws.


\subsection{Bootstrap simulation for risk-reduction propositions}
\label{parametricbootstrapsimulation}

In this section, we discuss simulation results for the bootstrap procedures used in estimating parameters for risk-reduction propositions (see Section \ref{properties}) and risk-reduction quantities (see Section \ref{comparisons}). We will compare $\mc{B}_c$ with $\mc{B}_u$.

In the first experiment, we consider the parameter combination of  $n \in \{5, 10, 15, 25\}$ and $\sigma_{\alpha} \in  \{5, 10, 25, 50, 100\}$ where we fix $\sigma$ to $10$. Note that $\E{\E{\alpha_1}}=54$, where the last expectation is operated under the density of the covariates. In other words, data with $\sigma_{\alpha} \in  \{5, 10, 25, 50, 100\}$  should well represent the situations when the signal of the covariates is strong and when it is nearly lost.

\begin{table}[t]
\caption{30 Monte Carlo simulations for $\mc{B}_c$ with varying $n$ and $\sigma_{\alpha}$ (risk-reduction propositions $\delta_{\hat{\alpha}}$)} \vspace{.2cm} \label{table1}
\begin{center}\begin{tabular}{cc|ccc|cccc} $n$   & $\sigma_{\alpha}$ &  $\hat{\alpha}_{\rm adj}$  & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm adj}})$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm wadj}})$ & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm IVW}})$ &  $\bar{\mc{C}}^{(k)}(\mc{A})$ \\[.15cm]    \hline  \multirow{5}{*}{5} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.89 (0.03) & 0.90 (0.02) & 0.89 (0.03) & 0.35 (0.04) \\   & 10  & 1 (0) & 1 (0) & 1 (0) & 0.89 (0.03) & 0.89 (0.03) & 0.89 (0.03) & 0.37 (0.05) \\   & 25  & 1 (0) & 1 (0) & 1 (0) & 0.78 (0.03) & 0.83 (0.03) & 0.78 (0.03) & 0.44 (0.05) \\   & 50  & 0.90 (0.06) & 0.97 (0.03) & 0.93 (0.05) & 0.66 (0.04) & 0.65 (0.04) & 0.65 (0.04) & 0.45 (0.06) \\   & 100  & 0.77 (0.08) & 0.97 (0.03) & 0.80 (0.07) & 0.57 (0.04) & 0.53 (0.04) & 0.57 (0.04) & 0.49 (0.05) \\[.3cm]    \multirow{5}{*}{10} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.91 (0.02) & 0.92 (0.02) & 0.91 (0.02) & 0.25 (0.03) \\   & 10  & 1 (0) & 1 (0) & 1 (0) & 0.89 (0.02) & 0.89 (0.03) & 0.89 (0.02) & 0.29 (0.03) \\   & 25  & 1 (0) & 1 (0) & 1 (0) & 0.75 (0.03) & 0.78 (0.03) & 0.77 (0.04) & 0.3 (0.04) \\   & 50  & 0.83 (0.07) & 1 (0) & 0.80 (0.07) & 0.59 (0.04) & 0.63 (0.04) & 0.59 (0.04) & 0.37 (0.05) \\   & 100  & 0.80 (0.07) & 0.93 (0.05) & 0.80 (0.07) & 0.47 (0.04) & 0.51 (0.04) & 0.46 (0.04) & 0.41 (0.05) \\[.3cm]    \multirow{5}{*}{15} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.91 (0.02) & 0.93 (0.02) & 0.91 (0.02) & 0.31 (0.05) \\   & 10  & 1 (0) & 1 (0) & 1 (0) & 0.87 (0.02) & 0.89 (0.03) & 0.87 (0.02) & 0.31 (0.04) \\  & 25  & 1 (0) & 1 (0) & 1 (0) & 0.75 (0.03) & 0.78 (0.03) & 0.76 (0.03) & 0.37 (0.04) \\   & 50  & 0.933 (0.05) & 1 (0) & 0.9 (0.06) & 0.61 (0.04) & 0.69 (0.03) & 0.64 (0.04) & 0.41 (0.04) \\   & 100  & 0.67 (0.09) & 1 (0) & 0.63 (0.09) & 0.55 (0.04) & 0.51 (0.04) & 0.55 (0.04) & 0.47 (0.04) \\[.3cm]    \multirow{5}{*}{25} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.95 (0.02) & 0.94 (0.02) & 0.95 (0.02) & 0.29 (0.04) \\   & 10  & 1 (0) & 1 (0) & 1 (0) & 0.93 (0.02) & 0.91 (0.02) & 0.93 (0.02) & 0.30 (0.04) \\   & 25  & 1 (0) & 1 (0) & 1 (0) & 0.78 (0.04) & 0.79 (0.04) & 0.77 (0.04) & 0.31 (0.04) \\   & 50  & 0.90 (0.06) & 1 (0) & 0.9 (0.06) & 0.57 (0.04) & 0.60 (0.04) & 0.58 (0.04) & 0.34 (0.04) \\     & 100  & 0.83 (0.07) & 1 (0) & 0.80 (0.07) & 0.49 (0.04) & 0.48 (0.04) & 0.50 (0.04) & 0.39 (0.03) \\ \end{tabular}  
\end{center}
 \vspace{-.5cm}
\end{table}

 Table \ref{table1} shows the results, with the 3rd to 5th column representing the averaged $I\big(\hat{\Delta}(\hat{\alpha})>0\big)$ (i.e., guess of $\delta_{\hat{\alpha}}$), and the 6th to 9th columns representing the LOOCV consistency and best consistency following notations in Section \ref{loocv}. First, assuming that $\bar{C}^{(k)}(\delta_{\hat{\alpha}})$ well estimates $\E{\mc{C}(\delta_{\hat{\alpha}}})$ and fixing $n$, we  observe that $\delta_{\hat{\alpha}}$ is highly consistent for $\hat{\alpha}\in \mc{A}$ when $\sigma_{\alpha}$ is small  from Table \ref{table1}. The reasons can be explained as follows. When $\sigma_{\alpha}$ is small, the signal of the covariates is strong so that $\hat{\alpha}_{\rm wadj}$ will be expected to capture the signal according to construction of $\hat{\alpha}_{\rm wadj}$ in Section \ref{constructionofestimators}. Moreover, when $\sigma_{\alpha}$ is small, $\mc{M}_{22}$ approximates $\mc{M}_{21}$ such that estimation of $\E{\alpha_1}$ should be nearly unbiased according to Proposition \ref{unbiased}. However, when the signal of the covariates is poor ($\sigma_{\alpha}$ is big), the $\delta_{\hat{\alpha}}$  become more inconsistent for $\hat{\alpha}\in \mc{A}$. It is to be expected since the bootstrap estimates become more biased. However, users can be warned by $\bar{C}^{(k)}(\delta_{\hat{\alpha}})$ to have an idea of the effectiveness of $\delta_{\hat{\alpha}}$. Second, fixing $\sigma_{\alpha}$, we can observe that $\delta_{\hat{\alpha}}$ becomes more consistent when $n$ increases. It is due to the robustness gain in estimation when $n$ increases.  


\begin{table}[t]
    \caption{30 Monte Carlo simulations for $\mc{B}_c$ with varying $\sigma$ and $\sigma_{\alpha}$} \label{table2} \vspace{-.2cm}
  \begin{center}\begin{tabular}{cc|ccc|cccc} $\sigma$   & $\sigma_{\alpha}$ &  $\hat{\alpha}_{\rm adj}$  & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm adj}})$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm wadj}})$ & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm IVW}})$ &  $\bar{\mc{C}}^{(k)}(\mc{A})$ \\[.15cm]    \hline \multirow{5}{*}{5} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.99 (0.01) & 0.99 (0.01) & 0.99 (0.01) & 0.4 (0.04) \\ 
  & 10  & 1 (0) & 1 (0) & 1 (0) & 0.91 (0.02) & 0.95 (0.02) & 0.92 (0.02) & 0.46 (0.04) \\ 
    & 25  & 0.97 (0.03) & 1 (0) & 0.97 (0.03) & 0.83 (0.03) & 0.89 (0.02) & 0.83 (0.03) & 0.43 (0.04) \\ 
   & 50  & 0.87 (0.06) & 1 (0) & 0.87 (0.06) & 0.67 (0.04) & 0.69 (0.03) & 0.67 (0.04) & 0.38 (0.04) \\ 
    & 100  & 0.7 (0.09) & 0.93 (0.05) & 0.77 (0.08) & 0.52 (0.04) & 0.51 (0.04) & 0.51 (0.05) & 0.51 (0.05) \\[.3cm] 
    \multirow{5}{*}{10} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.93 (0.02) & 0.93 (0.02) & 0.93 (0.02) & 0.27 (0.03) \\ 
    & 10  & 1 (0) & 1 (0) & 1 (0) & 0.86 (0.03) & 0.89 (0.03) & 0.87 (0.03) & 0.37 (0.04) \\ 
    & 25  & 0.97 (0.03) & 1 (0) & 0.97 (0.03) & 0.78 (0.03) & 0.82 (0.03) & 0.79 (0.03) & 0.29 (0.04) \\ 
    & 50  & 0.93 (0.05) & 1 (0) & 0.9 (0.06) & 0.66 (0.04) & 0.67 (0.04) & 0.66 (0.04) & 0.32 (0.04) \\ 
    & 100  & 0.77 (0.08) & 0.97 (0.03) & 0.7 (0.09) & 0.54 (0.05) & 0.54 (0.04) & 0.55 (0.04) & 0.43 (0.05) \\[.3cm] 
    \multirow{5}{*}{25} & 5  & 0.97 (0.03) & 1 (0) & 1 (0) & 0.78 (0.03) & 0.77 (0.03) & 0.79 (0.03) & 0.22 (0.03) \\ 
    & 10  & 1 (0) & 1 (0) & 1 (0) & 0.81 (0.03) & 0.79 (0.03) & 0.81 (0.03) & 0.21 (0.03) \\ 
    & 25  & 1 (0) & 1 (0) & 1 (0) & 0.72 (0.03) & 0.77 (0.03) & 0.71 (0.03) & 0.25 (0.04) \\ 
    & 50  & 0.87 (0.06) & 0.97 (0.03) & 0.87 (0.06) & 0.55 (0.04) & 0.57 (0.04) & 0.52 (0.04) & 0.35 (0.04) \\ 
    & 100  & 0.9 (0.06) & 1 (0) & 0.87 (0.06) & 0.54 (0.04) & 0.54 (0.04) & 0.56 (0.04) & 0.39 (0.05) \\[.3cm] 
    \multirow{5}{*}{50} & 5  & 0.83 (0.07) & 0.73 (0.08) & 0.8 (0.07) & 0.54 (0.04) & 0.57 (0.04) & 0.55 (0.04) & 0.19 (0.03) \\ 
    & 10  & 0.77 (0.08) & 0.8 (0.07) & 0.77 (0.08) & 0.53 (0.04) & 0.55 (0.05) & 0.53 (0.04) & 0.23 (0.03) \\ 
    & 25  & 0.73 (0.08) & 0.8 (0.07) & 0.73 (0.08) & 0.58 (0.04) & 0.58 (0.04) & 0.57 (0.04) & 0.21 (0.03) \\ 
    & 50  & 0.8 (0.07) & 0.83 (0.07) & 0.8 (0.07) & 0.59 (0.04) & 0.53 (0.04) & 0.6 (0.04) & 0.29 (0.05) \\ 
    & 100  & 0.5 (0.09) & 0.73 (0.08) & 0.53 (0.09) & 0.51 (0.05) & 0.48 (0.05) & 0.51 (0.05) & 0.31 (0.04) \\[.3cm] 
    \multirow{5}{*}{100} & 5  & 0.43 (0.09) & 0.37 (0.09) & 0.43 (0.09) & 0.47 (0.04) & 0.47 (0.04) & 0.47 (0.04) & 0.19 (0.03) \\ 
    & 10  & 0.63 (0.09) & 0.6 (0.09) & 0.67 (0.09) & 0.51 (0.04) & 0.49 (0.04) & 0.51 (0.04) & 0.23 (0.03) \\ 
    & 25  & 0.57 (0.09) & 0.6 (0.09) & 0.57 (0.09) & 0.51 (0.04) & 0.53 (0.04) & 0.49 (0.04) & 0.21 (0.04) \\ 
    & 50  & 0.53 (0.09) & 0.43 (0.09) & 0.5 (0.09) & 0.47 (0.04) & 0.49 (0.05) & 0.49 (0.05) & 0.19 (0.03) \\ 
    & 100  & 0.63 (0.09) & 0.63 (0.09) & 0.63 (0.09) & 0.43 (0.04) & 0.41 (0.04) & 0.45 (0.04) & 0.17 (0.03) \\ \end{tabular}
  \end{center}  
   \vspace{-.5cm}
\end{table}


Interestingly, we observe different patterns of $\bar{\mc{C}}^{(k)}(\mc{A})$ as opposed to $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}})$: (1) as $n$ increases, $\bar{\mc{C}}^{(k)}(\mc{A})$ decreases; and (2) as $\sigma_{\alpha}$ increases, $\bar{\mc{C}}^{(k)}(\mc{A})$ increases. It is possible  when $n$ is small or $\sigma_{\alpha}$ is large, the differences among $\hat{\Delta}(\hat{\alpha})$ for  $\hat{\alpha}\in \mc{A}$ are large so that the ranking gains more precision. It is because in these situations, the estimation of bootstrap estimates is less precise so that the variance of $\hat{\Delta}(\hat{\alpha})$ is larger.

Additionally, we  observe that in most cases $\delta_{\hat{\alpha}_{\rm wadj}}$ reports $\hat{\alpha}_{\rm wadj}$ reduces the risk even when $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm wadj}})$ starts to break down.  Recall from Section \ref{varbootstrap} that $\hat{\Delta}(\hat{\alpha})$ contains the squared bias for estimating $\E{\alpha_1}$. But it is not present for $\hat{\Delta}(\hat{\alpha}_{\rm wadj})$ since we applied the fact  $\hat{\alpha}_{\rm wadj}$ is unbiased for $\E{\alpha_1}$ from Proposition \ref{unbiased}. Therefore, when the signal from covariates is poorer, $\delta_{\hat{\alpha}_{\rm wadj}}$ becomes less conservative. Besides, the averaged $I\big(\hat{\Delta}(\hat{\alpha})>0\big)$ times $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}})$ can provide an approximation for the probability that $\hat{\alpha}$ actually reduces the risk assuming an symmetry of consistencies between the cases when $\hat{\Delta}(\hat{\alpha})>0$ and when $\hat{\Delta}(\hat{\alpha})<0$. For example, when $n = 5$ and $\sigma_{\alpha}=50$, the probability that $\hat{\alpha}_{\rm adj}$ reduces the risk is approximately $0.90 \times 0.66 = 0.594$ from table \ref{table1}.  In other words,  the probability that $\hat{\alpha}$ reduces the risk has the same pattern as $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}})$ has with $n$ and $\sigma_{\alpha}$ for $\hat{\alpha}\in \mc{A}$.




In the second experiment, we consider the parameter combination of $\sigma, \sigma_{\alpha} \in  \{5, 10, 25, 50, 100\}$ where we fix $n$ to $10$. Likewise, $\sigma, \sigma_{\alpha} \in  \{5, 10, 25, 50, 100\}$ will produce situations when the signal of the covariates is strong and when it is nearly lost in the model of  both $y_{i,t}$ and $\alpha_i$.



From Table \ref{table2}, we observe that as $\sigma_{\alpha}$ increases fixing $\sigma$, $\bar{\mc{C}}(\delta_{\hat{\alpha}})$ and $\bar{\mc{C}}(\mc{A})$ will  decrease, which is a pattern similar to one shown in the first experiment. Furthermore, as $\sigma$ increases fixing $\sigma_{\alpha}$, $\bar{\mc{C}}(\delta_{\hat{\alpha}})$ and $\bar{\mc{C}}(\mc{A})$ decrease as well. Note that the consistency or best consistency  hinges on the estimation of the parameters. Since $\hat{\alpha}_{\rm wadj}$  is a linear combination of OLS estimates, as $\sigma$ increases, $\var{\hat{\alpha}_{\rm wadj}}$ increases as well. Therefore, $\hat{\alpha}_{\rm wadj}$ become more volatile and its estimation of $\E{\alpha_1}$ can be less reliable. Those reasons can explain  why an increase of $\sigma_{\alpha}$ contributes to a decrease of  $\bar{\mc{C}}(\delta_{\hat{\alpha}})$ and $\bar{\mc{C}}(\mc{A})$.



With respect to averaged $I(\hat{\Delta}(\hat{\alpha}>0)$ (i.e., the guess), it starts to decrease as $\sigma$ increases. It is reasonable if we believe the bootstrap estimate $S^2_{\hat{\alpha}}$ provides a good approximation for $\var{\hat{\alpha}}$ for $\hat{\alpha}\in \mc{A}$. The reasons can be outlined as below. Recall in Section \ref{conditionsm2122}, the conditions of risk-reduction propositions involve $(\E{\alpha_1})^2 > \var{\hat{\alpha}} + (\E{\hat{\alpha}}-\E{\alpha_1})^2$ for $\hat{\alpha}\in \mc{A}$, where we note that those parameters are \emph{true} ones but not estimated ones. Notice that $\var{\hat{\alpha}}$ is  a function of $\sigma$ since $\hat{\alpha}$ is estimated by OLS. Therefore, it explains the reason why the increase of $\sigma$ would result in a decrease of averaged $I(\hat{\Delta}(\hat{\alpha}>0)$ since the conditions are not likely to hold when  $\var{\hat{\alpha}}$ increases.

\begin{table}[t]
 \caption{30 Monte Carlo simulation with varying $n$ and $\sigma_{\alpha}$ (distance to $y_{1, T_1^*+1}$)} \label{table3}
 \begin{center}\begin{tabular}{cc|cccc}  $n$   & $\sigma_{\alpha}$  & Original & $\hat{\alpha}_{\rm adj}$ & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$\\[.3cm]    \hline  \multirow{5}{*}{5} & 5  & 51.92 (4.04) & 19.23 (2.55) & 20.64 (2.76) & 19.36 (2.52) \\   & 10  &  52.58 (4.35) & 21.565 (2.58) & 23.38 (2.86) & 21.72 (2.50) \\   & 25  &  55.02 (5.84) & 30.30 (3.59) & 31.99 (4.36) & 30.20 (3.50) \\   & 50  & 64.42 (8.11) & 50.00 (5.78) & 52.56 (6.92) & 49.55 (5.70) \\   & 100  & 91.51 (13.20) & 94.27 (10.41) & 97.26 (12.57) & 93.61 (10.27) \\[.3cm]    \multirow{5}{*}{10} & 5 & 52.34 (4.00) & 17.23 (2.96) & 18.55 (2.84) & 17.39 (2.95) \\   & 10 &  52.59 (4.05) & 19.02 (3.24) & 20.95 (3.14) & 19.23 (3.24) \\   & 25 & 54.07 (4.97) & 27.66 (4.55) & 31.79 (4.47) & 27.85 (4.61) \\   & 50 &  60.32 (7.53) & 47.70 (7.04) & 52.97 (7.54) & 47.78 (7.17) \\   & 100 & 85.60 (12.99) & 89.86 (12.82) & 100.74 (13.61) & 90.40 (12.91) \\[.3cm]    \multirow{5}{*}{15} & 5 & 49.86 (4.01) & 18.07 (2.88) & 18.38 (2.71) & 18.04 (2.88) \\ 
  & 10 & 48.73 (4.30) & 19.45 (2.97) & 19.35 (2.86) & 19.32 (3.00) \\   & 25 &  47.06 (5.13) & 26.16 (3.31) & 26.81 (3.13) & 26.23 (3.33) \\   & 50 &  48.75 (6.86) & 40.27 (4.43) & 42.09 (4.49) & 40.77 (4.38) \\   & 100 &  64.29 (11.11) & 68.85 (8.27) & 74.08 (8.72) & 69.91 (8.14) \\[.3cm]    \multirow{5}{*}{25} & 5 & 57.60 (6.94) & 21.58 (5.90) & 20.00 (5.86) & 21.58 (5.90) \\   & 10 &  56.80 (7.01) & 22.20 (5.97) & 20.47 (5.89) & 22.22 (5.97) \\   & 25 &  56.58 (7.49) & 28.96 (6.49) & 27.06 (6.19) & 29.03 (6.48) \\   & 50 &  64.33 (8.75) & 47.16 (8.28) & 46.30 (7.26) & 47.35 (8.26) \\   & 100 & 95.61 (13.02) & 90.10 (13.29) & 86.81 (11.75) & 90.55 (13.23) \\ \end{tabular}	
 \end{center}
 \vspace{-.5cm}
\end{table}

Simulation for $\mc{B}_u$ with the same parameter setup as that of $\mc{B}_c$ are implemented. See table \ref{table6} and table \ref{table7} for results. Comparing table \ref{table1}
 and table \ref{table6} yields that when $n$ is small ($n = 5$ or $n = 10$) and $\sigma_{\alpha}$ is small ($\sigma_{\alpha}=5$), $\mc{B}_u$ is better than $\mc{B}_c$ with statistical evidence. For other situations, $\mc{B}_u$  and $\mc{B}_c$  are rather similar. It is likely that the extra randomness from sampling with replacement from donor pool compensates for the possible noises from a small donor pool. Concerning  table \ref{table2} and table \ref{table7}, it appears that when $n = 10$ and $\sigma_{\alpha}=5$, $\mc{B}_u$ is better than  $\mc{B}_c$  when $\sigma$ increases. It might be the case that additional layer of bootstrap in the donor pool buffers the negative effects on $\bar{\mc{C}}(\delta_{\hat{\alpha}})$  introduced from increasing variation of $y_{i,t}$. However, when $\sigma_{\alpha}$ increases over 5 and $n = 10$,  $\mc{B}_c$ and $\mc{B}_u$ are quite similar under  situations of different $\sigma$ and $\sigma_{\alpha}$. In conclusion, $\mc{B}_u$ is better than $\mc{B}_c$ when the signal of the covariates is strong and $n$ is small; otherwise, they are similar. See more discussions for differences between $\mc{B}_u$ and $\mc{B}_c$ in Section \ref{discussion}. 



\subsection{Simulation for prediction}
\label{simulationforprediction}


\begin{table}[t]
 \caption{30 Monte Carlo simulation with varying $n$ and $\sigma_{\alpha}$ (distance to $y_{1, T_1^*+1}$)} \label{table4}
 \begin{center}\begin{tabular}{cc|cccc}  $\sigma$   & $\sigma_{\alpha}$  & Original & $\hat{\alpha}_{\rm adj}$ & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$\\[.3cm]    \hline \multirow{5}{*}{5}  & 5  & 53.27 (2.59) & 10.91 (1.70) & 11.77 (1.55) & 10.8 (1.69) \\ 
   & 10  & 50.86 (3.83) & 18.06 (2.18) & 17.26 (2.17) & 18.25 (2.17) \\ 
    & 25  & 60.52 (3.97) & 16.09 (2.40) & 19.83 (3.05) & 15.87 (2.38) \\ 
     & 50  & 54.65 (6.95) & 51.79 (6.87) & 53.83 (7.26) & 52.6 (6.91) \\ 
     & 100  & 104.73 (13.00) & 88.31 (12.16) & 88.88 (12.63) & 86.72 (12.35) \\[.3cm] 
   \multirow{5}{*}{10}  & 5  & 58.17 (4.18) & 18.17 (2.61) & 16.59 (2.39) & 18.09 (2.63) \\ 
    & 10  & 52.81 (4.07) & 19.05 (2.49) & 19.67 (2.66) & 18.95 (2.53) \\ 
     & 25  & 61.53 (5.69) & 28.55 (4.07) & 31.82 (4.27) & 28.76 (4.07) \\ 
     & 50  & 56.31 (8.03) & 47.33 (4.89) & 41.44 (4.39) & 46.86 (4.79) \\ 
     & 100  & 84.38 (11.62) & 82.91 (11.77) & 84.5 (12.39) & 84.3 (11.93) \\[.3cm] 
   \multirow{5}{*}{25}  & 5  & 56.35 (6.1) & 25.54 (4.06) & 30.49 (4.61) & 25.94 (4.02) \\ 
     & 10  & 49.8 (5.24) & 25.04 (4.38) & 26.74 (3.64) & 24.86 (4.38) \\ 
     & 25  & 54.21 (6.51) & 44.17 (6.66) & 43.41 (6.95) & 43.89 (6.63) \\ 
     & 50  & 66.51 (7.86) & 46.19 (8.04) & 44.83 (9.31) & 46.54 (8.27) \\ 
     & 100  & 109.21 (13.29) & 78.47 (12.45) & 83.99 (12.00) & 78.57 (12.75) \\[.3cm] 
   \multirow{5}{*}{50}  & 5  & 75.29 (10.75) & 63.51 (8.30) & 64.25 (8.88) & 63.32 (8.40) \\ 
     & 10  & 57.59 (6.00) & 48.51 (8.32) & 50.58 (8.63) & 48.08 (8.36) \\ 
     & 25  & 77.21 (12.03) & 54.73 (9.47) & 54.76 (10.24) & 54.2 (9.62) \\ 
     & 50  & 90.48 (10.21) & 68.88 (9.20) & 68.28 (10.48) & 68.8 (9.28) \\ 
     & 100  & 111.09 (17.54) & 102.47 (16.68) & 110.15 (15.87) & 101.53 (16.52) \\[.3cm] 
   \multirow{5}{*}{100}  & 5  & 214.07 (67.40) & 195.46 (67.65) & 197.51 (68.45) & 196.58 (67.51) \\ 
     & 10  & 120.85 (15.39) & 114.79 (15.51) & 119.19 (15.75) & 114.02 (15.74) \\ 
     & 25  & 97.84 (13.75) & 95.02 (14.88) & 100.04 (16.94) & 96.23 (14.71) \\ 
     & 50  & 141.22 (24.51) & 136.11 (25.73) & 150.62 (27.62) & 135.26 (25.39) \\ 
     & 100  & 95.53 (12.80) & 103.01 (14.12) & 103.31 (16.27) & 105.79 (14.32) \\\end{tabular}	
 \end{center}
  \vspace{-.5cm}
\end{table}



In this section, we report prediction results for our methods in the simulation. Specifically, we are interested in the distance between the true response $y_{1,T_1^*+1}$ and the adjusted forecasts $\hat{y}_{1,T_1^*+1}$, $\hat{y}_{1,T_1^*+1}+ \hat{\alpha}_{\rm adj}$, $\hat{y}_{1,T_1^*+1}+\hat{\alpha}_{\rm wadj}$, and $\hat{y}_{1,T_1^*+1}+\hat{\alpha}_{\rm IVW}$.

In the first experiment, table \ref{table3} shows that similar to Section \ref{parametricbootstrapsimulation}, as $\sigma_{\alpha}$ increases, the prediction appears to be poorer. When $\sigma_{\alpha}= 5, 10, 25$, forecasts using $\hat{\alpha}_{\rm adj}$, $\hat{\alpha}_{\rm wadj}$, and $\hat{\alpha}_{\rm IVW}$ are always better than the original forecast significantly. But it does not hold generally for the case when $\sigma_{\alpha}=50, 100$. It is reasonable in that when the $\sigma_{\alpha}$ is large, it is difficult to find a reliable estimate of $\alpha_1$.  Nevertheless, unlike what is detected in Section \ref{parametricbootstrapsimulation}, no statistical evidence has been found to support the claim that $n$ matters in prediction. In other words, $n$ matters in producing reliable decision-making of $\delta_{\hat{\alpha}}$  rather than reliable prediction. 

Note that there is no statistical evidence supporting for differences among $\hat{\alpha}_{\rm adj}$, $\hat{\alpha}_{\rm wadj}$, and $\hat{\alpha}_{\rm IVW}$. This observation corresponds to the fact that $\bar{\mc{C}}(\mc{A})$ is small and following analysis in Section \ref{parametricbootstrapsimulation}. When their performances do not differ much, the precision of ranking is small. In this scenario, for a more reasonable statistical inference, one may opt for a double bootstrap instead to approximate the distribution of $\Delta(\hat{\alpha})$ and compare estimators by checking whether their 95\% confidence intervals overlap. See more details in Section \ref{discussion}. 

In the second experiment, table \ref{table4} shows similar results for prediction to those in the first expeirment, for the case of varying $\sigma_{\alpha}$ but fixing $\sigma$, and the case of varying $\sigma$ but fixing $\sigma_{\alpha}$. When $\sigma$ increases with fixing $\sigma_{\alpha}$, it is likely that the degree of variation of $y_{1,t}$ exceeds the extent of adjustment improvement $\hat{\alpha}$ can contribute to for $\hat{\alpha}\in \mc{A}$. It can be confirmed by looking into  the case of $\sigma= 50, 100$ as opposed to the case for $\sigma= 5, 10, 25$ where adjusted forecasts outperform original forecasts. Adjusted forecasts can be worse than original forecast if the variation of $y_{i,t}$ is large and the signal of the covariates is very small. See for the case  of $\sigma=100$ and $\sigma_{\alpha}=100$ in table \ref{table4}. 






%In this section, we present the simulation for the parametric bootstrap procedure for AR(1) outlined in Section \ref{varbootstrap}. The simulation is only provided for the adjustment estimator and weighted adjustment estimator since the weights for inverse-variance estimator are random and the true values of its moments are unknown.



%\begin{center}
%  \begin{tabular}{l|cccccc}
%    & \multicolumn{6}{c}{Time series}\\
%    & 1 & 2 & 3 & 4 & 5 & 6\\
%   Length $T_i$ & 77 & 93 & 72 & 71 & 73 & 50 \\
%   Shock Time  $T_i^*+1$ & 20 & 39 & 45 & 52 & 59 & 11 \\
%   Autoregressive parameter $\phi_i$ & 0.540 & 0.961 & 0.654 & 0.547 & 0.266 & 0.197 
%  \end{tabular}
%\end{center}





\section{Forecasting Conoco Phillips stock in the presence of shocks}
\label{forecasting}
In this example we forecast Conoco Phillips stock prices in the midst of the coronavirus recession. Specific interest is in predictions made after March 6, 2020, the Friday before the stock market crash on March 9, 2020. We will detail how we combine knowledge from disparate time series to improve the forecast of Conoco Phillips stock price that would be made without adjustments for the shock. %Section \ref{background} provides the justification for the first three steps. Section \ref{results} discusses the result of our forecast.

%\subsection{Background}
%\label{background}

Conoco Phillips is chosen for this analysis because it is a large oil and gas resources company \citep{conocowhatwedo}. %and with a relatively recent initial public offering \citep{conocohist}. 
Focus on the oil sector is because oil prices have been shown to exhibit a cointegrating behavior with economic indices \citep{he2010global}, and our chosen time frame represents the onset of a significant economic down turn, coupled with a Russia and OPEC battle for global oil price control the Sunday before trading resumes on Monday, March 9th \citep{sukhankin2020russian}. Furthermore, fear of and action in response to the coronavirus pandemic began to uptick dramatically between Friday, March 6th and Monday, March 9th. Major events include the SXSW festival being cancelled as trading closed on March 6th \citep{wang2020impact}. New York declared a state of emergency on March 7th \citep{nygov}, and by Sunday, March 8th, eight states have declared a state of emergency \citep{alonso2020state} while Italy placed 16 million people in quarantine \citep{sjodin2020only}.


Economic indicators forecasted our recession before the coronavirus pandemic began. The current recession followed an inversion of the yield curve that first happened back in March, 2019 \citep{tokic2019yield}. An inversion of the yield curve is an event that signals that recessions are more likely \citep{andolfatto2018yield, bauer2018economic}. In this analysis we investigate the performance of oil companies in previous recessions that followed an inversion of the yield curve to obtain a suitable Conoco Phillips donor pool for estimating the March 9th shock effect on Conoco Phillips oil stock. We also consider previous OPEC oil supply shocks \citep{mensi2014opec}. We will borrow from the literature on oil price forecasting to establish appropriate time horizons and forecasting models. Recessions that occurred before 1973 are disregarded since oil price forecasts cannot be represented by standard time series models before 1973 \citep{alquist2013forecasting}. In this analysis we make the following considerations:

\begin{enumerate}
\item[(1)] {\bf AR(1) model and time window}. We will use an AR(1) model to forecast Conoco Phillips stock price. This model has been shown to beat no-change forecasts when predicting oil prices over time horizons of one and three months \citep{alquist2013forecasting}. We will consider 30 pre-shock trading days and we will forecast the immediate shock effect and the shock effect over a future five trading day window. All estimates will be adjusted for inflation. The model setup for AR(1) is exactly the same as what is stated in Section \ref{modelsetup} with addition of shock effects. All the parameters are estimated using OLS.
\item[(2)] {\bf Selection of covariates}. We perform our analyses incorporating daily S\&P 500 index prices and West Texas Intermediate (WTI) crude oil prices as covariates. 
\item[(3)] {\bf Construction of donor pool}. Our donor pool will consist of Conoco Phillips shock effects observed on March 14, 2008, several events in September, 2008, and November 27, 2014. The first two shock effects were observed during recessions that were predicated by an inversion of the yield curve \citep{bauer2018economic}, and the third was an OPEC induced supply side shock effect \citep{huppmann2015opec}. The reasons for those three shocks are:
  \begin{enumerate}
  \item On March 14, 2008, Bear Stearns was verging on bankruptcy from what its officials described as a sudden liquidity squeeze related to its large exposure to devalued mortgage-backed securities. On that day, it also received word that it was getting an unprecedented loan from the Federal Reserve System, this decision was unprecedented: never before had the Fed committed to ``bailing out'' a financial entity that was not a commercial bank. The day of the announcement, the stocks of other major Wall Street firms tumbled (including Conoco Phillips). These concerns then spilled over into the broader universe of stocks \citep{shorter2008bear}. %In heavy trading on Tuesday, March 18, the firm’s share price closed at \$5.91.
  \item  In early September 2008, time series of oil prices experienced a sudden increase in volatility simultaneously due to turmoil in financial markets. The political, economic, social or environmental events may coincide with these shocks \citep{ewing2013volatility}. Notable shock effects followed the placement of Fannie May and Freddie Mac in conservatorship on September 7th (shock effect on the 8th), Lehman Brothers filing for bankruptcy on September 15th, and the Office of Thrift Supervision closes Washington Mutual Bank on September 25th \citep{dwyer2009financial, longstaff2010subprime}.
  \item On November 27th, 2014, it is documented that oil prices fall as OPEC opts not to cut production \citep{huppmann2015opec}. During the Great Recession when economic activity clearly declined, both oil and stock prices fell which points to demand factors. During the second half of 2014, oil prices plummeted but equity prices generally increased, suggesting that supply factors were the key driver \citep[Page 19]{baffes2015great}. 
  \end{enumerate}
%\item[(4)] {\bf Forecast}. \textbf{Jilei, please fill this in. I do not know what you have in mind.}
\end{enumerate}

%Since the stock price of a company implies its value provided the number of shares is unchanged, the price of S\&P 500 index in the recession can partially represent the overall status of the economy.  Therefore, using the price of S\&P 500 index as the covariate can possibly improve the precision of the OLS estimates for shock-effects that are unique to Conoco Phillips. The rationale for inclusion of the price of S\&P 500 index is similar to that of market return in capital asset pricing model (CAPM). (\textbf{Need reference for the CAPM}.)


% INTEREST RATES AND OIL RELATIONSHIP: This paper shows that in the short term both gold and crude oil prices positively influence each other. Interest rates have a negative influence on the future gold prices and a positive influence on the future crude oil prices. In the long run, a relationship exists whereby interest rates influence the US dollar, which in turn influences international crude oil prices. When the Federal Reserve Board (Fed) lowers interest rates to boost the economy, market expectations for oil demand change and, as a result, crude oil prices fluctuate \citep{wang2013dynamic}.


%\subsection{Results}
%\label{results}

%\vspace{-.3cm}
%\begin{table}[H]
%  \caption{Some basic summary statistics of the covariates and response} \vspace{.4cm} \label{table1}
%  \begin{center}
%    \begin{tabular}{ccrrrrrr}
%       &    & \multicolumn{2}{c}{S\&P 500} & \multicolumn{2}{c}{WTI}  & \multicolumn{2}{c}{$y_{i,t}$} \\
%     Time series & $T_i^*+1$ & Mean & SD  &  Mean & SD  & Mean & SD  \\
%             \hline 
%2020 Mar.  &  03--09 & 3239.11 & 139.28 & 50.36 & 3.13 & 55.35 & 6.37 \\ %  2008 Mar.  & 03--14 & 1341.03 & 29.50 & 98.82 & 7.01 & 60.59 & 2.28 \\ % \multirow{3}{*}{2008 Sep.}
% & $T_3^* +1 = \text{09--08}$ & \multirow{3}{*}{1504.50}  & \multirow{3}{*}{43.44}  & \multirow{3}{*}{134.46} & \multirow{3}{*}{10.24} & \multirow{3}{*}{71.20}  & \multirow{3}{*}{4.11}  \\ 
% & $T_4^* +1 = \text{09--12}$ \\
%   & $T_5^* +1 = \text{09--26}$ \\
%\\%  2014 Nov.  & 11--27 & 2179.99 & 66.99 & 85.70 & 3.38 & 76.99 & 1.90 \\ 
%    \end{tabular}
%  \end{center}  \vspace{-.3cm}
%\end{table}

%Table \ref{table1} presents some basic summary statistics for the covariates and response for the time series of interest (stock price of Conoco Phillips in 2020 March) and the donor pool. 

We assume that the five shocks are independent of the shock that Conoco Phillips experienced on March 9, 2020. The covariates and response of time series in the donor pool are adjusted for inflation. Note that there are three shock-effects nested in the time series 2008 September, we assume that these three shocks are independent, where the assumption checks using likelihood ratio test are provided in Appendix \ref{smfda}. We computed 3 shock-effect estimates. Using the same model that is described in $\mc{M}_{2}$ (see Section \ref{modelsetup}), we computed 3 shock-effect estimates, namely the adjustment  $\hat{\alpha}_{\rm adj}$, weighted adjustment $\hat{\alpha}_{\rm wadj}$, and the inverse-variance weighted $\hat{\alpha}_{\rm IVW}$. For $\hat{\alpha}_{\rm wadj}$, the fitting result can be described by
\begin{align*}
 \mathbf{W}^*= (0.000,0.000, 0.000, 0.000, 1.000) \quad \text{ and } \quad  \norm{\mrm{vec}\big(\mbf{X}_1-\hat{\mbf{X}}_1(\mbf{W}^*)\big)}_{4} = 1314.04.
\end{align*}
Note that the norm is computed using the $k$-dimensional Euclidean metric. The solution $\mathbf{W}^*$ reports that the $\alpha_1$ is very similar to $\alpha_5$ (November 27, 2014) under the setup of $\mc{M}_2$ and our collected covariates. 

\begin{table}[H]
  \caption{Bootstrap estimates and results yielded by risk-reduction propositions with $B = 1000$}\label{table5}
  \begin{center}
    \begin{tabular}{lrrr}
      & $\hat{\alpha}_{\rm adj}$ & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$ \\
      \hline 
    Bootstrapped variance & 0.531 & 0.479 & 0.970 \\
    \end{tabular}
  \end{center}  
\end{table}
\vspace{-.5cm}

Using $\mc{B}_c$ proposed in Section \ref{varbootstrap}, we estimated parameters for risk-reduction propositions and risk-reduction quantities proposed in  Section \ref{properties}; and the result is presented in Table \ref{table2}. Plugging these estimates into conditions in Section \ref{properties} and risk-reduction formulas in Section \ref{comparisons} yields (1) $\hat{\alpha}_{\text{adj}}$, $\hat{\alpha}_{\text{wadj}}$, and $\hat{\alpha}_{\text{IVW}}$  reduce the risk and (2) the risk-reduction quantities are $34.565$, $34.880$, and $34.139$, respectively.  The estimated risk-reduction quantities  vote $\hat{\alpha}_{\rm wadj}$  for the best estimator. LOOCV with $k$ random draws was not implemented since the effective $n$ is $3$, a number too small to provide a credible estimation. We verify the consistency of the result yielded by risk-reduction propositions with the reality as below.

\begin{figure}
  \begin{center}
    \includegraphics[height = 9cm]{fig2.pdf}
    \caption{March 9th, 2020 post-shock forecasts for Conoco Phillips stock price.}
    \label{Fig:CP}
  \end{center}  
  \vspace{-.6cm}
\end{figure}


We can see from Figure~\ref{Fig:CP} that $\hat{\alpha}_{\text{adj}}$, $\hat{\alpha}_{\text{wadj}}$ and $\hat{\alpha}_{\text{IVW}}$ perform decently well, they do not recover the magnitude of the shock effect but are much better than unadjusted forecasts that do not account for shock effects. 
The unadjusted forecast has an RMSE of 9.870 dollars whereas 
the use of $\hat{\alpha}_{\text{adj}}$, $\hat{\alpha}_{\text{wadj}}$, and $\hat{\alpha}_{\text{IVW}}$ have  RMSE of $4.436$, $3.924$, and $4.423$ dollars, respectively. 
Therefore, the risk-reduction propositions are consistent with the reality  with the reduced risks for forecasts using  $\hat{\alpha}_{\text{adj}}$, $\hat{\alpha}_{\text{wadj}}$, and $\hat{\alpha}_{\text{IVW}}$ than without. Moreover, the risk-reduction quantities are consistent with the reality as well. 


The phenomenon that the true shock effect is not recovered by $\hat{\alpha}_{\text{adj}}$, $\hat{\alpha}_{\text{wadj}}$, and $\hat{\alpha}_{\text{IVW}}$ can be due to that the donor pool is not constructed to be similar enough to the time series of interest. The shock(s) on March 9, 2020 is(are) in the midst of the COVID-19 pandemic and oil production volatility. It is difficult to find available stock market time series data that were generated under a similar setting.  


From another perspective, it is possible that the stock of Conoco Phillips actually experienced multiple shocks on 2020 March 9th. For example, \citet{kilian2009not} studied the effect that different supply and demand shocks have on oil prices through a vector auto regressive model. Their model postulates an additive nature of shock effects, although the additivity parameters requires estimation in their context. Motivated by his study, we also studied additive shock effect estimators where the shock effects corresponding to separate supply and demand shocks are added to estimate the unknown shock effect. The supply shock donor pool consists of the November 27, 2014 shock effect; and the demand shock donor pool consists of the remaining shock effects. The additive adjustment estimator computed by adding the $\hat{\alpha}_{\text{adj}}$, $\hat{\alpha}_{\text{wadj}}$, and $\hat{\alpha}_{\text{IVW}}$ estimators for the demand and supply shock effects have  RMSEs of $1.382$, $2.468$, and $1.145$ dollars, respectively. These additive adjustment estimators do extremely well, nearly perfectly forecasting the realized shock effect. 
%There is apriori justification for the use of these simply additive adjustment estimators, although their nearly perfect performance in this example is a retrospective finding.  %This provides some anecdotal justification for apriori estimation of the March 9th Conoco Phillips shock effect with the addition of the adjustment estimator corresponding to the separate supply and demand aspects of this shock effect.






\section{Discussion}
\label{discussion}



Our proposed model in Section \ref{modelsetup} is a simple model with results in Section \ref{properties} easily generalizing to other more complicated models. Our simple AR(1) model can be extended to AR($p$) settings. %for $y_{i,t}$ with an additional component $\alpha_i 1(D_{i,t}=T_i^* + 1)$. 
Moreover, the mean function for $\alpha_i$ under $\mc{M}_2$ can be extended to include more lagged predictors. Similarly, LOOCV with $k$ random draws can be adapted to AR($p$). Besides, the functional form for the mean function for $y_{i,t}$ can be extended beyond the linear regression model under $\mc{M}_2$. Multiple shock-effects can be nested within a time series; and time series in the donor pool are allowed be dependent. As an example, we could consider a dependency structure for the September 2008 shock effects in our analysis of Conoco Phillips stock. But we note that consistency estimates from LOOCV with $k$ random draws may not work well if donor pool candidates are not mutually independent since the almost unbiased property hinges on the mutual independence among candidates in the donor pool.
%This generalization is possible since our risk-reduction propositions in Section \ref{properties} assumes the shock-effect estimates are independent of $\alpha_1$, the shock effect for the time series of interest; and allows arbitrary dependence of shock-effects in the donor pool. 
%Specific to this generalization, we edit the model of $\alpha_i$ to $\alpha_i = \mu_{\alpha} + \delta_i'\mathbf{x}_{3, T_i^* + 1} + \gamma_i'\mathbf{x}_{3, T_i^*} + \tilde{\varepsilon}_i$ for $i = 3, 4, 5$ and $\alpha_i = \mu_{\alpha} + \delta_i'\mathbf{x}_{4, T_i^* + 1} + \gamma_i'\mathbf{x}_{4, T_i^*} + \tilde{\varepsilon}_i$ for $i = 6$.
%This generalization was adopted in Section \ref{forecasting}. In this case, the variance expressions proposed in Section \ref{properties} will not work. However, the risk-reduction propositions in Section \ref{properties} will work  as long as those shock-effects (not necessarily mutually independent) in the donor pool are independent of the one of interest. 
Although it is reflected in $\mc{M}_2$, we stress that our proposed methods allow $\alpha_i$ to follow arbitrary distributions with existing first and second moments. The covariates in the model for $\alpha_i$ under $\mc{M}_2$ can be different from the covariates in the model of $y_{i,t}$. We also note that our post-shock framework can be extended to settings where the shock effect can be decomposed into separable estimable parts. An example of this is the additive shock effect estimators that we studied in our Conoco Phillips analysis. 




Our bootstrap procedures can be extended to approximate the distribution of shock effect estimators from more general time series. The pseudo time series generated by our proposed parametric bootstrap is not stationary. Note that \citet{politis1994stationary} motivated a stationary bootstrap method for strictly stationary and weakly dependent time series $\{X_n \colon n \in \mathbb{N}\}$. This algorithm generates a sequence of blocks of observations $B_{I_1, L_1}, B_{I_2, L_2}, \ldots$ where $B_{i,b}= \{X_i, X_{i+1}, \ldots, X_{i+b-1}\}$; for $j > N$, $X_{j}$ is defined to be $X_k$, where $k = j \hspace{-.1cm} \mod N$ and $X_0 = X_N$. The sampling stops when $N$ observations are reached. Note that the collection of random positions $\{I_n \colon n \in \mathbb{N}\}$ is a sequence of i.i.d. discrete uniform random variables; and the collection of random lengths $\{L_n \colon n \in \mathbb{N}\}$ is a sequence of  i.i.d. geometric random variables with parameter $p$.  However, the consistency needs to be proved by a case-by-case analysis \citep[Page 66]{politis1999subsampling}. Additionally, the asymptotic accuracy of this algorithm can be sensitive to the selection of $p$. This issue is similar to that of the selection of block size in moving-block bootstrapping \citep{kunsch1989jackknife, liu1992moving}.  More work related to bootstrapping time series  can be referred to Chapters 3 and 4 in \citet{politis1999subsampling}, and \citet{berkowitz2000recent}. It is up to  users in terms of selecting which procedure to choose but under different assumptions on the time series.



Recall that in Section \ref{varbootstrap}, we proposed two possible bootstrap procedures, $\mc{B}_u$  and $\mc{B}_c$. There are some philosophical distinctions between $\mc{B}_u$  and $\mc{B}_c$. $\mc{B}_u$ treats the donor pool as realizations from some infinite super-population of potential donors. In contrast, $\mc{B}_c$ treats the donor pool as being fixed  and known before the analysis is conducted, where the randomness comes from parameters and idiosyncratic error. For other differences,  Section \ref{parametricbootstrapsimulation} shows that $\mc{B}_u$ is better than $\mc{B}_c$ when the signal of the covariates is strong and $n$ is small; otherwise, they are quite similar.

In this study, for the use of $\mc{B}_u$, we assume $\mathbf{W}^*$ is non-degenerate in the population without taking zero weight.  Recall that in Section \ref{constructionofestimators} we note that if there exists some $\mathbf{W}^*$ satisfies (\ref{SCM}) and $2p < n$, there will be infinitely many solutions to $\mathbf{W}^*$. In the simulation setup, we consider $2p >n$ such that the assumption can be reasonably satisfied. However, in the application, it is likely that $\mathbf{W}^*$ may  take values on the boundary of $\mc{W}$, in which case $\mc{B}_u$ may fail for $\hat{\alpha}_{\mrm{wadj}}$ \citep{andrews2000inconsistency}. For example, if $\mathbf{W}^*$ is observed to fall in the boundary, $\mc{B}_u$ will fail.  Moreover, if $2p < n$, $\mc{B}_u$ fails since the existence of infinitely many solutions is certain (if there exists some $\mathbf{W}^*$ satisfies (\ref{SCM})), and will guarantee degeneracy of  $\mathbf{W}^*$. In this regard, the subsampling in Section 4 of \citet{li2019statistical} may adapt to our scenario to solve this problem by resampling $m$ candidates from donor pool with the conditions $m \to \infty$, and $m/n \to 0$ as $n \to \infty$. But we may leave its mathematical justification for future research. Note that this problem will not occur under  $\mc{B}_c$ because  if $w_i^*=0$ for some $i$ and the weights are known, $w_i^*=0$ makes it impossible for bootstrap estimate of $\hat{\alpha}_i$ to contribute to the bootstrap estimate of distribution of $\hat{\alpha}_{\mrm{wadj}}$. 


Double bootstrap can be another general bootstrap alternative to the one proposed in Section \ref{varbootstrap}. For example, for $\mc{B}_u$, double bootstrap adds another layer of residual bootstrap between the donor pool non-parametric bootstrap and the original residual bootstrap. It enables an estimation for distribution of $\Delta(\hat{\alpha})$ for $\hat{\alpha} \in \mc{A}$. In the framework of double bootstrap, instead of checking whether $\Delta(\hat{\alpha})>0$, we can check whether non-parametric 95\% confidence intervals contain 0. Likewise, we can compare two shock-effect estimators by checking whether their 95\% confidence intervals of risk-reduction overlap though an exhaustive ranking may not be feasible. We have attempted Monte Carlo simulations using the same setting in Section \ref{simulation} for the double bootstrap procedure, the results of which are quite similar to those of Section \ref{varbootstrap}.

For the use of our methodology, caution should be dedicated to the construction of donor pool. If the donor pool includes some individuals that are not similar to the time series of interest, the result will possibly not be robust to the introduced noises.   Moreover, according to Section \ref{parametricbootstrapsimulation}, we recommend using estimated risk-reduction quantities for voting best shock-effect estimator only when $n$ is large. It is  due to the fact that plug-in estimation of  risk-reduction quantities in Section \ref{varbootstrap} gains more precision when $n$ becomes large.

\section{Appendix}

\subsection{Proofs}

\label{proofs}

\subsubsection{Justification of Expectation of $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$}
\label{exp}

The building block for the following proof is the fact that least squares is conditionally unbiased conditioned on $\Theta$. 

\noindent \textbf{Case I: under $\mc{M}_{1}$:} It follows that  under $\mc{M}_{1}$ (see Section \ref{modelsetup}),
\begin{align*}
\E{\hat{\alpha}_{\rm adj}} =\frac{1}{n}  \sum_{i=2}^{n+1} \E{\E{\hat{\alpha}_i|\Theta}} = \mu_{\alpha} 
\quad \text{ and } \quad \E{\hat{\alpha}_{\rm wadj}}&= \sum_{i=2}^{n+1} w_i^* \E{\E{\hat{\alpha}_i|\Theta}}= \sum_{i=2}^{n+1} w_i^*\mu_{\alpha}= \mu_{\alpha}.
\end{align*}
where we used the fact that $\sum_{i=2}^{n+1} w_i=1$. 

\noindent \textbf{Case II: under $\mc{M}_{21}$ and $\mc{M}_{22}$:} Since $\E{\tilde{\varepsilon}_{i, T_i}}=0$, $\E{\hat{\tilde{\alpha}}_{i}}=\E{\tilde{\alpha}_{i}}=\E{\alpha_{i}}$, it follows that
  \begin{align*}
   \E{\hat{\alpha}_{\rm wadj}}= \mrm{E}\left\{\mrm{E}\left(\sum_{i=2}^{n+1} w_i^*\hat{\alpha}_i|\Theta\right)\right\}
   &= \mrm{E}\left( \sum_{i=2}^{n+1} w_i^*\alpha_i\right)\\
   &= \mrm{E}\bigg\{ \sum_{i=2}^{n+1} w_i^*\left[\mu_{\alpha}+\delta_{i}'\mbf{x}_{i, T_i^*+1}+\gamma_i'\mbf{x}_{i, T^*_i}\right]\bigg\}\\
   &=\mu_{\alpha}+\mrm{E}\bigg\{ \sum_{i=2}^{n+1} w_i^*\left[\delta_{i}'\mbf{x}_{i, T_i^*+1}+\gamma_i'\mbf{x}_{i, T^*_i}\right]\bigg\}.\tag{$\mbf{W}\in \mc{W}$}
   \end{align*}
Similarly,
  \begin{align*}
   \E{\hat{\alpha}_{\rm adj}}
   &=\mu_{\alpha}+\frac{1}{n}\sum_{i=2}^{n+1} \E{\delta_{i}'\mbf{x}_{i, T_i^*+1}+\gamma_i'\mbf{x}_{i, T^*_i}}.
   \end{align*}


\subsubsection{Justification of Variance of $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$}
\label{var}

Notice that under the setting of OLS, the design matrix for $\mc{M}_2$ is the same as the one for $\mc{M}_1$. Therefore, it follows that
  \begin{align*}
  \var{\hat{\alpha}_{\rm wadj}} 
  &= \E{\var{\hat{\alpha}_{\rm wadj}|\Theta}} + \var{\E{\hat{\alpha}_{\rm wadj}|\Theta}} \\
  &=\mrm{E} \left\{\mrm{Var}\left(\sum_{i=2}^{n+1} w_i^*\hat{\alpha}_i|\Theta\right)\right\} +\mrm{Var}\left(\sum_{i=2}^{n+1} w_i^*\alpha_i\right) 
\end{align*}
Under $\mc{M}_{21}$ where $\delta_i=\delta$ and $\gamma_i=\gamma$ are fixed unknown parameters,  we will have
  \begin{align}
  \var{\hat{\alpha}_{\rm wadj}} 
  &= \mrm{E} \left\{\sum_{i=2}^{n+1}(w_i^*)^2(\sigma^2(\mbf{U}'_i\mbf{U}_i)^{-1}_{22})\right\} +\sigma^2_{\alpha}\sum_{i=2}^{n+1}(w_i^*)^2  \nonumber\\
  &= \sigma^2\sum_{i=2}^{n+1}(w_i^*)^2\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\sigma^2_{\alpha}\sum_{i=2}^{n+1}(w_i^*)^2.\label{equation6}
\end{align}
Similarly, under $\mc{M}_{22}$ where we assume $\delta_i \indep \gamma_i \indep \varepsilon_{i,t}$, we have
 \begin{align*}
  \var{\hat{\alpha}_{\rm wadj}} 
  &= \sigma^2\sum_{i=2}^{n+1}(w_i^*)^2\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}
  + \sum_{i=2}^{n+1} (w_i^*)^2 \var{\alpha_i}
\end{align*}
For the adjustment estimator, we simply replace $\mbf{W}^*$ with $1/n\mbf{1}_n$. Thus, under $\mc{M}_{21}$ we have 
 \begin{align*}
  \var{\hat{\alpha}_{\rm adj}} 
  &=\frac{\sigma^2}{n^2}\sum_{i=2}^{n+1}\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\frac{\sigma^2_{\alpha}}{n^2}
\end{align*}
Under $\mc{M}_{22}$, we shall have
 \begin{align*}
  \var{\hat{\alpha}_{\rm adj}} 
  &=\frac{\sigma^2}{n^2}\sum_{i=2}^{n+1}\mrm{E}\big\{(\mbf{U}'_i\mbf{U}_i)^{-1}_{22}\big\}+\frac{1}{n^2}\var{\alpha_i}.
\end{align*}
Notice that $\mc{M}_{1}$ differs from $\mc{M}_{21}$ only by its mean parameterization of $\alpha$ (see Section \ref{modelsetup}). In other words, the variances of $\hat{\alpha}_{\rm adj}$ and $\hat{\alpha}_{\rm wadj}$ under $\mc{M}_1$ are the same for those under $\mc{M}_{21}$.

\subsubsection{Proofs for lemmas and propositions}

\begin{proof-of-proposition}[\ref{uniqueness}]
  The proof of \citet{li2019statistical} in Appendix A.2 and A.3 adapts easily to Proposition \ref{uniqueness}.
\end{proof-of-proposition}


\begin{proof-of-proposition}[\ref{unbiased}] The proof for unbiasedness follows immediately from discussions related to expectation in Section \ref{properties}. For the biasedness of  $\hat{\alpha}_{\rm adj}$ under $\mc{M}_{21}$ and $\mc{M}_{22}$, we write the bias term for $\hat{\alpha}_{\rm adj}$ as below.
\begin{align*}
  \mrm{Bias}(\hat{\alpha}_{\rm adj}) = 
  \begin{cases}
       \frac{1}{n} \sum_{i=2}^{n+1} \delta'(\mbf{x}_{i, T_i^*+1}-n\mbf{x}_{1, T_1^*+1}) +  \frac{1}{n} \sum_{i=2}^{n+1} \gamma'(\mbf{x}_{i, T_i^*}-n\mbf{x}_{1, T_1^*}) & \text{ for } \mc{M}_{21}\\
    \frac{1}{n} \sum_{i=2}^{n+1} \mu_{\delta}'(\mbf{x}_{i, T_i^*+1}-n\mbf{x}_{1, T_1^*+1}) +  \frac{1}{n} \sum_{i=2}^{n+1} \mu_{\gamma}'(\mbf{x}_{i, T_i^*}-n\mbf{x}_{1, T_1^*}) & \text{ for }\mc{M}_{22}
  \end{cases}.
\end{align*}
But it may be unbiased in some special circumstances when the above bias turns out to be 0. \end{proof-of-proposition}

\begin{lem}
  \label{risklemma} The forecast risk difference is $R_{T_1^*+1,1}-R_{T_1^*+1,2}=\E{\alpha_1^2}-\E{\hat{\alpha}-\alpha_1}^2$ for all estimators of $\alpha_1$ that are independent of $\Theta_1$ (see Section \ref{modelsetup}).
\end{lem}

  
\begin{proof-of-lemma}[\ref{risklemma}]
  Define 
  \begin{align*}
    C(\Theta_1) =\hat{\eta}_1 +\hat{\phi}_1 y_{1, T_1^*}+\hat{\theta}_1'\mbf{x}_{1, T_1^*+1} + \hat{\beta}_1'\mbf{x}_{1, T_1^*}-(\eta_1 +\phi_1y_{1,T_1^*}+\theta_1'\mbf{x}_{1,T_1^*+1}+\beta_1'\mbf{x}_{1,T_1^*}),
  \end{align*}
  where $\Theta_1$ is as defined in (\ref{parameter}). Notice that
  \begin{align*}
    R_{T_1^*+1,1}= \mrm{E}\big\{\big(C(\Theta_1)-\alpha_1\big)^2\big\}
    \qquad \text{ and } 
    \qquad  R_{T_1^*+1,2}= \mrm{E}\big\{\big(C(\Theta_1)+\hat{\alpha}-\alpha_1\big)^2\big\}.
  \end{align*}
  It follows that
  \begin{align*}
    R_{T_1^*+1,1}-R_{T_1^*+1,2}=\E{\alpha_1^2}-2\E{C(\Theta_1)\hat{\alpha}}-\E{\hat{\alpha}-\alpha_1}^2.
  \end{align*}
  Assuming $\mbf{S}=(\mbf{1}_n, \mbf{y}_{1,t-1}, \mbf{x}_{1}, \mbf{x}_{1,t-1})$ has full rank, under OLS setting, $\hat{\eta}_1$, $\hat{\phi}_1$, $\hat{\theta}_1$, and $\hat{\beta}_1$ are unbiased estimators of $\eta_1$, $\phi_1$, $\theta_1$, and $\beta_1$, respectively under conditioning of $\Theta_1$. Since we assume $\hat{\alpha}$ is independent of $\Theta_1$, through the method of iterated expectation,
  \begin{align*}
    \E{C(\Theta_1)\hat{\alpha}}=\mrm{E}\big\{\hat{\alpha}\cdot \E{C(\Theta_1)\mid \Theta_1}\}=0.
  \end{align*}
  It follows that
  \begin{align*}
    R_{T_1^*+1,1}-R_{T_1^*+1,2}=\E{\alpha_1^2}-\E{\hat{\alpha}-\alpha_1}^2,
  \end{align*}
  which finishes the proof.
\end{proof-of-lemma}





\begin{proof-of-proposition}[\ref{proprisk}] The proofs are arranged into two separate parts as below.

 \textbf{Proof for statement (i):} Under $\mc{M}_1$, $\hat{\alpha}_{\rm adj}$ is an unbiased estimator of $\E{\alpha_1}$ because
  \begin{align*}
   E\left( \frac{1}{n}\sum_{i=2}^{n+1} \hat{\alpha}_i\right)
   = \frac{1}{n}\sum_{i=2}^{n+1}\E{\hat{\alpha}_i}
   &= \frac{1}{n}\sum_{i=2}^{n+1}\E{\E{\hat{\alpha}_i\mid \Theta}}\\
   &=  \frac{1}{n}\sum_{i=2}^{n+1}\E{\alpha_i}
   = \mu_{\alpha}=\E{\alpha_1},
  \end{align*}
  where we used the fact that OLS estimator is unbiased when the design matrix $\mbf{U}_i$ is of full rank for all $i = 2, \ldots, n+1$. Because $\alpha_1\indep \varepsilon_{i,t}$, $\E{\hat{\alpha}_{\rm adj}\alpha_1}=\E{\hat{\alpha}_{\rm adj}}\E{\alpha_1}=(\E{\hat{\alpha}_{\rm adj}})^2$. By Lemma \ref{risklemma}, 
    \begin{align*}
    R_{T_1^*+1,1}-R_{T_1^*+1,2}
    &=\E{\alpha_1^2}-\E{\hat{\alpha}_{\rm adj}-\alpha_1}^2\\
   & =\E{\alpha_1^2}-\E{\alpha_1^2}- \E{\hat{\alpha}_{\rm adj}^2}+2\E{\hat{\alpha}_{\rm adj}\alpha_1} \\
   &= \mu_{\alpha}^2 - \var{\hat{\alpha}_{\rm adj}} 
  \end{align*}
  Therefore, as long as we have $\var{\hat{\alpha}_{\rm adj}}<\mu_{\alpha}^2$, we will achieve the risk reduction. 

 \textbf{Proof for statement (ii):} By Proposition \ref{unbiased}, the property that $\hat{\alpha}_{\rm wadj}$ is an unbiased estimator of $\mu_{\alpha}$ holds for $\mc{M}_{1}$. The remainder of the proof follows a similar argument to the proof of statement (i).
\end{proof-of-proposition}



\begin{proof-of-proposition}[\ref{propriskwadj2}]
  By Proposition \ref{unbiased}, the property that $\hat{\alpha}_{\rm wadj}$ is an unbiased estimator of $\E{\alpha_1}$ holds for $\mc{M}_{21}$ and $\mc{M}_{22}$. The remainder of the proof follows a similar argument to the proof of Proposition \ref{proprisk}.
\end{proof-of-proposition}


\subsection{Supplementary materials for data analysis}
\label{smfda}
The independence of estimated shock-effects are further tested using likelihood ratio test (LRT) based on their estimated covariance matrix. The estimated covariance matrix is
\begin{align*}
  \hat{\mathbf{\Sigma}}=\left(\begin{array}{rrr}
    4.012 & 0.362 & -0.062 \\
    0.362 & 3.894 & -0.029 \\
    -0.062 & -0.029 & 3.927
  \end{array}\right).
\end{align*}
with degrees of freedoms 35.
Using LRT for independence between blocks of random variables \citep[Section 10.2]{msos}, the LRT test statistic is $0.304$ with $p$-value of 0.581. Therefore, we do not reject  the null hypothesis that the three estimated shock-effects are independent.

\subsection{Simulations tables for $\mc{B}_u$}
\label{simulationappendix}


\begin{table}[H]\caption{30 Monte Carlo simulations  for $\mc{B}_u$ with varying $n$ and $\sigma_{\alpha}$} \vspace{.3cm} \label{table6}
\begin{center}\begin{tabular}{cc|ccc|cccc}
   &   & \multicolumn{3}{|c|}{Guess} & \multicolumn{4}{|c}{LOOCV with $k$ random draws}  \\  $n$   & $\sigma_{\alpha}$ &  $\hat{\alpha}_{\rm adj}$  & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm adj}})$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm wadj}})$ & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm IVW}})$ &  $\bar{\mc{C}}^{(k)}(\mc{A})$ \\[.15cm]    \hline  \multirow{5}{*}{5} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.92 (0.02) & 0.96 (0.02) & 0.92 (0.02) & 0.36 (0.05) \\     & 10  & 1 (0) & 1 (0) & 1 (0) & 0.90 (0.02) & 0.92 (0.02) & 0.90 (0.02) & 0.40 (0.04) \\     & 25  & 0.97 (0.03) & 1 (0) & 0.97 (0.03) & 0.80 (0.02) & 0.81 (0.02) & 0.80 (0.03) & 0.43 (0.04) \\     & 50  & 0.83 (0.07) & 0.87 (0.06) & 0.87 (0.06) & 0.55 (0.05) & 0.57 (0.05) & 0.55 (0.05) & 0.38 (0.05) \\     & 100  & 0.47 (0.09) & 0.73 (0.08) & 0.47 (0.09) & 0.48 (0.05) & 0.48 (0.04) & 0.46 (0.04) & 0.37 (0.05) \\[.3cm]     \multirow{5}{*}{10} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.95 (0.02) & 0.95 (0.02) & 0.95 (0.02) & 0.33 (0.04) \\     & 10  & 1 (0) & 1 (0) & 1 (0) & 0.92 (0.03) & 0.91 (0.03) & 0.92 (0.03) & 0.33 (0.04) \\     & 25  & 0.90 (0.06) & 0.97 (0.03) & 0.93 (0.05) & 0.77 (0.04) & 0.79 (0.04) & 0.75 (0.05) & 0.31 (0.04) \\     & 50  & 0.77 (0.08) & 0.80 (0.07) & 0.77 (0.08) & 0.55 (0.04) & 0.64 (0.04) & 0.55 (0.05) & 0.31 (0.04) \\     & 100  & 0.63 (0.09) & 0.70 (0.09) & 0.63 (0.09) & 0.53 (0.04) & 0.53 (0.05) & 0.51 (0.04) & 0.33 (0.04) \\[.3cm]     \multirow{5}{*}{15} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.92 (0.02) & 0.93 (0.02) & 0.92 (0.02) & 0.31 (0.05) \\     & 10  & 1 (0) & 1 (0) & 1 (0) & 0.92 (0.02) & 0.91 (0.02) & 0.92 (0.02) & 0.30 (0.03) \\    & 25  & 1 (0) & 1 (0) & 1 (0) & 0.83 (0.03) & 0.83 (0.03) & 0.83 (0.03) & 0.35 (0.04) \\   & 50  & 0.90 (0.06) & 0.93 (0.05) & 0.90 (0.06) & 0.71 (0.03) & 0.67 (0.04) & 0.69 (0.04) & 0.33 (0.04) \\     & 100  & 0.70 (0.09) & 0.73 (0.08) & 0.70 (0.09) & 0.54 (0.05) & 0.61 (0.04) & 0.54 (0.05) & 0.31 (0.05) \\[.3cm]    \multirow{5}{*}{25} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.90 (0.03) & 0.91 (0.03) & 0.90 (0.03) & 0.27 (0.04) \\     & 10  & 1 (0) & 1 (0) & 1 (0) & 0.87 (0.03) & 0.89 (0.03) & 0.87 (0.03) & 0.30 (0.03) \\     & 25  & 1 (0) & 1 (0) & 1 (0) & 0.73 (0.03) & 0.74 (0.04) & 0.73 (0.03) & 0.29 (0.03) \\     & 50  & 0.83 (0.07) & 0.87 (0.06) & 0.83 (0.07) & 0.55 (0.05) & 0.56 (0.05) & 0.55 (0.05) & 0.29 (0.04) \\     & 100  & 0.77 (0.08) & 0.73 (0.08) & 0.77 (0.08) & 0.48 (0.05) & 0.49 (0.04) & 0.49 (0.05) & 0.31 (0.05) \\ \end{tabular}
   \end{center}
      \vspace{-.5cm}
\end{table}



\begin{table}[H]\caption{30 Monte Carlo simulations  for $\mc{B}_u$ with varying $\sigma$ and $\sigma_{\alpha}$} \label{table7}
\begin{center}\begin{tabular}{cc|ccc|cccc}
   &   & \multicolumn{3}{|c|}{Guess} & \multicolumn{4}{|c}{LOOCV with $k$ random draws}  \\  $\sigma$   & $\sigma_{\alpha}$ &  $\hat{\alpha}_{\rm adj}$  & $\hat{\alpha}_{\rm wadj}$ & $\hat{\alpha}_{\rm IVW}$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm adj}})$  & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm wadj}})$ & $\bar{\mc{C}}^{(k)}(\delta_{\hat{\alpha}_{\rm IVW}})$ &  $\bar{\mc{C}}^{(k)}(\mc{A})$ \\[.3cm]    \hline \multirow{5}{*}{5} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.99 (0.01) & 0.99 (0.01) & 0.99 (0.01) & 0.39 (0.05) \\     & 10  & 1 (0) & 1 (0) & 1 (0) & 0.97 (0.01) & 0.97 (0.01) & 0.97 (0.01) & 0.34 (0.04) \\     & 25  & 0.93 (0.05) & 0.97 (0.03) & 0.93 (0.05) & 0.81 (0.04) & 0.82 (0.04) & 0.81 (0.04) & 0.29 (0.04) \\     & 50  & 0.77 (0.08) & 0.80 (0.07) & 0.77 (0.08) & 0.57 (0.05) & 0.67 (0.04) & 0.57 (0.05) & 0.34 (0.04) \\     & 100  & 0.63 (0.09) & 0.70 (0.09) & 0.63 (0.09) & 0.51 (0.04) & 0.53 (0.05) & 0.50 (0.04) & 0.35 (0.05) \\[.3cm]    \multirow{5}{*}{10} & 5  & 1 (0) & 1 (0) & 1 (0) & 0.95 (0.02) & 0.95 (0.02) & 0.95 (0.02) & 0.33 (0.04) \\     & 10  & 1 (0) & 1 (0) & 1 (0) & 0.92 (0.03) & 0.91 (0.03) & 0.92 (0.03) & 0.33 (0.04) \\     & 25  & 0.90 (0.06) & 0.97 (0.03) & 0.93 (0.05) & 0.77 (0.04) & 0.79 (0.04) & 0.75 (0.05) & 0.31 (0.04) \\     & 50  & 0.77 (0.08) & 0.80 (0.07) & 0.77 (0.08) & 0.55 (0.04) & 0.64 (0.04) & 0.55 (0.05) & 0.31 (0.04) \\     & 100  & 0.63 (0.09) & 0.70 (0.09) & 0.63 (0.09) & 0.53 (0.04) & 0.53 (0.05) & 0.51 (0.04) & 0.33 (0.04) \\[.3cm]    \multirow{5}{*}{25}  & 5  & 1 (0) & 1 (0) & 1 (0) & 0.79 (0.04) & 0.78 (0.04) & 0.77 (0.04) & 0.19 (0.04) \\     & 10  & 0.97 (0.03) & 0.97 (0.03) & 0.97 (0.03) & 0.75 (0.04) & 0.75 (0.04) & 0.73 (0.05) & 0.20 (0.04) \\    & 25  & 0.90 (0.06) & 0.93 (0.05) & 0.90 (0.06) & 0.66 (0.05) & 0.69 (0.05) & 0.66 (0.05) & 0.30 (0.05) \\     & 50  & 0.77 (0.08) & 0.80 (0.07) & 0.80 (0.07) & 0.55 (0.05) & 0.64 (0.05) & 0.55 (0.05) & 0.34 (0.05) \\     & 100  & 0.60 (0.09) & 0.70 (0.09) & 0.63 (0.09) & 0.48 (0.04) & 0.52 (0.04) & 0.47 (0.04) & 0.31 (0.04) \\[.3cm]    \multirow{5}{*}{50}  & 5  & 0.73 (0.08) & 0.73 (0.08) & 0.77 (0.08) & 0.57 (0.05) & 0.57 (0.05) & 0.58 (0.05) & 0.24 (0.04) \\     & 10  & 0.73 (0.08) & 0.73 (0.08) & 0.73 (0.08) & 0.55 (0.05) & 0.55 (0.05) & 0.55 (0.05) & 0.23 (0.03) \\     & 25  & 0.77 (0.08) & 0.70 (0.09) & 0.73 (0.08) & 0.48 (0.05) & 0.51 (0.05) & 0.47 (0.05) & 0.24 (0.04) \\     & 50  & 0.77 (0.08) & 0.73 (0.08) & 0.73 (0.08) & 0.46 (0.05) & 0.53 (0.04) & 0.47 (0.05) & 0.25 (0.04) \\     & 100  & 0.57 (0.09) & 0.70 (0.09) & 0.57 (0.09) & 0.46 (0.05) & 0.49 (0.03) & 0.45 (0.05) & 0.33 (0.05) \\[.3cm]   \multirow{5}{*}{100}  & 5  & 0.63 (0.09) & 0.60 (0.09) & 0.63 (0.09) & 0.46 (0.05) & 0.52 (0.04) & 0.48 (0.05) & 0.23 (0.04) \\     & 10  & 0.60 (0.09) & 0.60 (0.09) & 0.63 (0.09) & 0.46 (0.05) & 0.53 (0.04) & 0.46 (0.05) & 0.24 (0.04) \\     & 25  & 0.63 (0.09) & 0.57 (0.09) & 0.63 (0.09) & 0.45 (0.05) & 0.54 (0.04) & 0.47 (0.05) & 0.21 (0.04) \\     & 50  & 0.57 (0.09) & 0.53 (0.09) & 0.57 (0.09) & 0.49 (0.04) & 0.54 (0.04) & 0.47 (0.05) & 0.23 (0.03) \\     & 100  & 0.43 (0.09) & 0.53 (0.09) & 0.43 (0.09) & 0.52 (0.05) & 0.55 (0.03) & 0.51 (0.05) & 0.27 (0.04) \\\end{tabular}
   \end{center}
   \vspace{-.5cm}
\end{table}

\newpage


\bibliographystyle{plainnat}
\bibliography{synthetic-prediction-notes}




\end{document}

